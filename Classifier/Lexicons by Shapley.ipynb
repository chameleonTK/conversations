{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "DNesLV3APJ5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1945,
     "status": "ok",
     "timestamp": 1710872533192,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "DNesLV3APJ5e",
    "outputId": "f67cd8c8-24d8-4219-fbcb-4430954fead9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "TSKW-5YdPLLp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710872533192,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "TSKW-5YdPLLp",
    "outputId": "611e69b9-edc6-4bff-e269-f27b967ed6f1"
   },
   "outputs": [],
   "source": [
    "# cd /content/drive/MyDrive/TalkLikeMom/src/Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iGtcVVbnPLOn",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710872533762,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "iGtcVVbnPLOn"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Ta8MgmcKPLR8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1710872534275,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "Ta8MgmcKPLR8"
   },
   "outputs": [],
   "source": [
    "# !pip install -q emoji pythainlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a2543c-adc6-4ccb-a36a-1cf4a92575e0",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1710872534621,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "12a2543c-adc6-4ccb-a36a-1cf4a92575e0"
   },
   "outputs": [],
   "source": [
    "from data_loader import get_task1_conver, get_task2_conver, preprocess\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "# import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "# import shap\n",
    "\n",
    "from utils import dump_jsonl, load_jsonl, set_random_seed\n",
    "# import torch\n",
    "# from transformers import AutoModelForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0aab9f-76a7-4133-ad38-64586852c0fb",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710872536151,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "df0aab9f-76a7-4133-ad38-64586852c0fb"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_obj_values(filepath):\n",
    "    with open(filepath, 'rb') as fin:\n",
    "        obj = pickle.load(fin)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hxDForvdPeNk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14062,
     "status": "ok",
     "timestamp": 1710862721296,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "hxDForvdPeNk",
    "outputId": "07b6276c-d588-41af-b968-e4ff6fc9b6f6"
   },
   "outputs": [],
   "source": [
    "# !pip install -q pythainlp emoji==1.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SuihU-NePwRM",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1710862721296,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "SuihU-NePwRM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d320908-70d0-40bd-a9e9-fd12fe85658a",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1710872539408,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "0d320908-70d0-40bd-a9e9-fd12fe85658a"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import pythainlp\n",
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "def is_emoji(s):\n",
    "    for char in s:\n",
    "        if char in UNICODE_EMOJI[\"en\"]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "labelmap = {\n",
    "    '1. Close': \"1. Close\",\n",
    "    '2. Know each other': \"2. Acquainted\",\n",
    "    \"3. Don't know each other\": \"3. Unfamiliar\",\n",
    "\n",
    "\n",
    "    '0. Very respect': \"0. Highly Respectful\",\n",
    "    '1. Respect': \"1. Respectful\",\n",
    "    '2. Normal': \"2. Normal\",\n",
    "    '3. Not respect': \"3. Disrespectful\"\n",
    "}\n",
    "\n",
    "def label2newlabel(label):\n",
    "    if type(label) is list or type(label) is np.ndarray:\n",
    "        return [labelmap[l] if l in labelmap else l for l in label ]\n",
    "\n",
    "    if label in labelmap:\n",
    "        return labelmap[label]\n",
    "    return label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VOa9G2ZdQF3C",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1710862726513,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "VOa9G2ZdQF3C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8302ffe5-5ff9-419b-8aeb-34e92bf966dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1710872826440,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "8302ffe5-5ff9-419b-8aeb-34e92bf966dd",
    "outputId": "01033646-5a74-4d38-ddef-893bb082b9a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 records from Lexicons/v3/lexicon_task1_clse.jsonl\n",
      "Loaded 2 records from Lexicons/v3/lexicon_task2_clse.jsonl\n",
      "Loaded 2 records from Lexicons/v3/lexicon_task3_clse.jsonl\n",
      "Loaded 2 records from Lexicons/v3/lexicon_task1_auth.jsonl\n",
      "Loaded 2 records from Lexicons/v3/lexicon_task2_auth.jsonl\n",
      "Loaded 2 records from Lexicons/v3/lexicon_task3_auth.jsonl\n"
     ]
    }
   ],
   "source": [
    "fn_abs = False\n",
    "suffix = \"abs\" if fn_abs else ''\n",
    "\n",
    "lexicon_paths = [\n",
    "  f\"Lexicons/v3/lexicon_task1_clse{suffix}.jsonl\",\n",
    "  f\"Lexicons/v3/lexicon_task2_clse{suffix}.jsonl\",\n",
    "  f\"Lexicons/v3/lexicon_task3_clse{suffix}.jsonl\",\n",
    "\n",
    "  f\"Lexicons/v3/lexicon_task1_auth{suffix}.jsonl\",\n",
    "  f\"Lexicons/v3/lexicon_task2_auth{suffix}.jsonl\",\n",
    "  f\"Lexicons/v3/lexicon_task3_auth{suffix}.jsonl\"\n",
    "]\n",
    "\n",
    "def print_lexicon(lexicons, lexicons_by_feat):\n",
    "    s = \"\"\n",
    "    for label in sorted(lexicons.keys()):\n",
    "        if label==\"overall\":\n",
    "          continue\n",
    "\n",
    "        for lextype in [\"all\", \"pronoun\", \"particles\", \"misspelling\"]:\n",
    "\n",
    "          lexlabel = {\n",
    "              \"all\": \"All\",\n",
    "              \"pronoun\": \"Pronoun\",\n",
    "              \"particles\": \"Sentence-final Particle\",\n",
    "              \"misspelling\": \"Spelling Variation\",\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          if lextype==\"all\":\n",
    "            s += \"            \"+label2newlabel(label) +f\" & {lexlabel[lextype]} & \"\n",
    "            words = lexicons[label]\n",
    "          else:\n",
    "            s += \"            \" +f\" & {lexlabel[lextype]} & \"\n",
    "            words = lexicons_by_feat[label][lextype]\n",
    "\n",
    "          sorted_words = dict(sorted(words.items(), key=lambda item: -item[1]))\n",
    "          for k in list(sorted_words.keys())[0:10]:\n",
    "              unique = False\n",
    "\n",
    "              if pythainlp.util.isthai(k):\n",
    "                  s += \"\\\\thaitext{\"+k+\"}, \"\n",
    "              elif is_emoji(k):\n",
    "                  kk = \"\"\n",
    "                  for ch in k:\n",
    "                      if ch in UNICODE_EMOJI[\"en\"]:\n",
    "                        ch = emoji.demojize(ch)\n",
    "                        ch = ch.replace(\"_\", \"-\").replace(\":\", \"\")\n",
    "                        ch = \"\\emoji{\"+ch+\"}\"\n",
    "                        kk += ch\n",
    "                      else:\n",
    "                        kk += ch\n",
    "\n",
    "                  s += f\"{kk}, \"\n",
    "              else:\n",
    "                  s += f\"{k}, \"\n",
    "\n",
    "          s += \"\\\\\\\\\\n\"\n",
    "\n",
    "        s += \"            \\hline\\n\"\n",
    "    # s += \"            \\hline\"\n",
    "    # print(s)\n",
    "    return s\n",
    "\n",
    "texts = []\n",
    "for path in lexicon_paths:\n",
    "  lexicons, lexicons_by_feat = load_jsonl(path)\n",
    "  s = print_lexicon(lexicons, lexicons_by_feat)\n",
    "  texts.append(s)\n",
    "  # print(s)\n",
    "  # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98122ef1-8145-4ceb-b923-e983f5ce1047",
   "metadata": {
    "executionInfo": {
     "elapsed": 408,
     "status": "ok",
     "timestamp": 1710872963063,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "98122ef1-8145-4ceb-b923-e983f5ce1047"
   },
   "outputs": [],
   "source": [
    "sections = [\n",
    "    \"Setting 1: Private Conversations with Self-Reported Labels\",\n",
    "    \"Setting 2: Public Conversations with Labels from 3rd Party \",\n",
    "    \"Setting 3: Private Conversations with Labels from 3rd Party \",\n",
    "]\n",
    "\n",
    "table_contents = [\n",
    "    (texts[0], texts[3]),\n",
    "    (texts[1], texts[4]),\n",
    "    (texts[2], texts[5]),\n",
    "]\n",
    "\n",
    "printed_text = \"\"\n",
    "for section, (t1, t2) in zip(sections, table_contents):\n",
    "\n",
    "    printed_text += \"\\subsection{\"+section+\"}\"\n",
    "\n",
    "    printed_text += '''\n",
    "\\\\begin{longtable}[h]{\n",
    "        p{\\dimexpr 0.20\\linewidth-2\\\\tabcolsep}\n",
    "        p{\\dimexpr 0.25\\linewidth-2\\\\tabcolsep}\n",
    "        p{\\dimexpr 0.55\\linewidth-2\\\\tabcolsep}\n",
    "    }\n",
    "        \\hline\n",
    "        Relationship Label  & Lexicon Category & Lexicons\\\\\\\\\n",
    "        \\hline\n",
    "        \\endfirsthead\n",
    "        \\hline\n",
    "        Relationship Label  & Lexicon Category & Lexicons\\\\\\\\\n",
    "        \\hline\n",
    "        \\endhead\n",
    "\n",
    "\n",
    "        \\multicolumn{3}{l}{\\\\textit{Closeness}} \\\\\\\\\n",
    "        \\hline\n",
    "'''\n",
    "    printed_text += t1\n",
    "    printed_text += '''\n",
    "            \\hline\n",
    "\n",
    "        \\multicolumn{3}{l}{\\\\textit{Respect}} \\\\\\\\\n",
    "        \\hline\n",
    "'''\n",
    "\n",
    "    printed_text += t2\n",
    "    printed_text += '''\n",
    "            \\hline\n",
    "\\end{longtable}\n",
    "\\clearpage\n",
    "\\n\\n\n",
    "'''\n",
    "\n",
    "# print(printed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ec1ec-dc25-42f4-87d0-784fb39ce092",
   "metadata": {
    "id": "316ec1ec-dc25-42f4-87d0-784fb39ce092"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56404aea-e87b-4672-acd9-cfd337d45460",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710862726513,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "56404aea-e87b-4672-acd9-cfd337d45460"
   },
   "outputs": [],
   "source": [
    "# lexicon_paths = [\n",
    "#   # \"Lexicons/v3/lexicon_task1_clse.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task2_clse.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task3_clse.jsonl\",\n",
    "\n",
    "#   # \"Lexicons/v3/lexicon_task1_auth.jsonl\",\n",
    "#   # \"Lexicons/v3/lexicon_task2_auth.jsonl\",\n",
    "#   # \"Lexicons/v3/lexicon_task3_auth.jsonl\"\n",
    "# ]\n",
    "\n",
    "# allSelectedWords = set()\n",
    "# for path in lexicon_paths:\n",
    "#   lexicons, lexicons_by_feat = load_jsonl(path)\n",
    "\n",
    "#   selectedWords = set()\n",
    "#   for cat in lexicons_by_feat:\n",
    "#     # print(\"all\", len(lexicons[cat].keys()))\n",
    "\n",
    "#     keys = [k for k, v in sorted(lexicons[cat].items(), key=lambda item: -item[1])]\n",
    "#     keys = keys[0:200]\n",
    "#     words = set(keys)\n",
    "#     selectedWords.update(words)\n",
    "#   print(\"Final\", len(selectedWords))\n",
    "\n",
    "#   for cat in lexicons_by_feat:\n",
    "#     # print(\"all\", len(lexicons[cat].keys()))\n",
    "#     for f in lexicons_by_feat[cat]:\n",
    "#       feats = set(lexicons_by_feat[cat][f].keys())\n",
    "#       # if f==\"pronoun\":\n",
    "#       #   print(sorted(feats))\n",
    "#       N = len(lexicons_by_feat[cat][f].keys())\n",
    "#       print(f, len(feats&selectedWords), N, f\"{len(feats&selectedWords)*100/N:.0f}%\")\n",
    "\n",
    "#   allSelectedWords.update(selectedWords)\n",
    "#   print(\"==============\")\n",
    "\n",
    "# print(len(allSelectedWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "OSdWMyMtAdQe",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710862726513,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "OSdWMyMtAdQe"
   },
   "outputs": [],
   "source": [
    "# rows = []\n",
    "# for word in allSelectedWords:\n",
    "#   rows.append({\n",
    "#       \"คำ (Word)\": word,\n",
    "#       \"สนิทกันมาก (Intimate)\": \"\",\n",
    "#       \"สนิทกัน (Close)\": \"\",\n",
    "#       \"แค่คนรู้จักกัน (Acquainted)\": \"\",\n",
    "#       \"ไม่รู้จักกัน (Unfamiliar)\": \"\",\n",
    "#       \"ไม่ชอบหน้ากัน  (Dislike)\": \"\",\n",
    "#       \"บอกไม่ได้ (Cannot describe)\": \"\",\n",
    "#   })\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(rows)\n",
    "# df.to_csv(\"clse_lexicons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "JnmdECRdAdT1",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1710862727151,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "JnmdECRdAdT1"
   },
   "outputs": [],
   "source": [
    "# lexicon_paths = [\n",
    "#   # \"Lexicons/v3/lexicon_task1_clse.jsonl\",\n",
    "#   # \"Lexicons/v3/lexicon_task2_clse.jsonl\",\n",
    "#   # \"Lexicons/v3/lexicon_task3_clse.jsonl\",\n",
    "\n",
    "#   # \"Lexicons/v3/lexicon_task1_auth.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task2_auth.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task3_auth.jsonl\"\n",
    "# ]\n",
    "\n",
    "# allSelectedWords = set()\n",
    "# for path in lexicon_paths:\n",
    "#   lexicons, lexicons_by_feat = load_jsonl(path)\n",
    "\n",
    "#   selectedWords = set()\n",
    "#   for cat in lexicons_by_feat:\n",
    "#     # print(\"all\", len(lexicons[cat].keys()))\n",
    "\n",
    "#     keys = [k for k, v in sorted(lexicons[cat].items(), key=lambda item: -item[1])]\n",
    "#     keys = keys[0:200]\n",
    "#     words = set(keys)\n",
    "#     selectedWords.update(words)\n",
    "#   print(\"Final\", len(selectedWords))\n",
    "\n",
    "#   for cat in lexicons_by_feat:\n",
    "#     # print(\"all\", len(lexicons[cat].keys()))\n",
    "#     for f in lexicons_by_feat[cat]:\n",
    "#       feats = set(lexicons_by_feat[cat][f].keys())\n",
    "#       # if f==\"pronoun\":\n",
    "#       #   print(sorted(feats))\n",
    "#       N = len(lexicons_by_feat[cat][f].keys())\n",
    "#       print(f, len(feats&selectedWords), N, f\"{len(feats&selectedWords)*100/N:.0f}%\")\n",
    "\n",
    "#   allSelectedWords.update(selectedWords)\n",
    "#   print(\"==============\")\n",
    "\n",
    "# print(len(allSelectedWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ZUQLzH8eAdXP",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1710862729827,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "ZUQLzH8eAdXP"
   },
   "outputs": [],
   "source": [
    "# rows = []\n",
    "# for word in allSelectedWords:\n",
    "#   rows.append({\n",
    "#       \"คำ (Word)\": word,\n",
    "#       \"ให้เกียรติมาก (Highly Respectful)\": \"\",\n",
    "#       \"ให้เกียรติ (Respectful)\": \"\",\n",
    "#       \"ปกติ (Normal)\": \"\",\n",
    "#       \"ไม่ให้เกียรติ (Disrespectful)\": \"\",\n",
    "#       \"ไม่ให้เกียรติมาก (Highly Disrespectful)\": \"\",\n",
    "#       \"บอกไม่ได้ (Cannot describe)\": \"\",\n",
    "#   })\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(rows)\n",
    "# df.to_csv(\"auth_lexicons.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578a2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822d228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74600da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "420ee96d-4d5e-4f97-9651-b706c5cde1e9",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1710867222098,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "420ee96d-4d5e-4f97-9651-b706c5cde1e9"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# ordinal_weights\n",
    "def get_weights(categories):\n",
    "    weights = defaultdict(dict)\n",
    "    if len(categories)==3:\n",
    "        mat = [[1.00, 0.67, 0.00], \n",
    "               [0.67, 1.00, 0.67], \n",
    "               [0.00, 0.67, 1.00]]\n",
    "    elif len(categories)==4:\n",
    "        mat = [[1.00, 0.83, 0.50, 0.00], \n",
    "               [0.83, 1.00, 0.83, 0.50], \n",
    "               [0.50, 0.83, 1.00, 0.83], \n",
    "               [0.00, 0.50, 0.83, 1.00]]\n",
    "    elif len(categories)==5:\n",
    "        mat = [[1, 0.9, 0.7, 0.4, 0.0], \n",
    "               [0.9, 1, 0.9, 0.7, 0.4], \n",
    "               [0.7, 0.9, 1, 0.9, 0.7], \n",
    "               [0.4, 0.7, 0.9, 1, 0.9],\n",
    "               [0.0, 0.4, 0.7, 0.9, 1]]\n",
    "    else:\n",
    "        # Lazy to implement in case of len(categories) > 5\n",
    "        raise Exception(\"No Implementation\")\n",
    "        \n",
    "    for i, l in enumerate(categories): \n",
    "        for j, k in enumerate(categories): \n",
    "            weights[l][k] = mat[i][j]\n",
    "    return weights\n",
    "\n",
    "    \n",
    "def cal_agreement(df1, df2, column, categories, cat_column):\n",
    "    merged = pd.merge(df1, df2, on=column)\n",
    "#     print(len(df1), len(df2), len(merged))\n",
    "#     assert(len(df1)==len(merged))\n",
    "    merged = merged.dropna()    \n",
    "    cnt_matrix = defaultdict(dict)\n",
    "    acc_matrix = defaultdict(dict)\n",
    "    \n",
    "    for l in categories: \n",
    "        for k in categories: \n",
    "            d = merged\n",
    "            d = d[d[f\"{cat_column}_x\"]==k]\n",
    "            d = d[d[f\"{cat_column}_y\"]==l]\n",
    "            cnt_matrix[l][k] = len(d)\n",
    "    \n",
    "    for l in categories: \n",
    "        d = merged\n",
    "        d = d[d[f\"{cat_column}_x\"]==l]\n",
    "        acc_matrix[\"x\"][l] = len(d)\n",
    "        \n",
    "        d = merged\n",
    "        d = d[d[f\"{cat_column}_y\"]==l]\n",
    "        acc_matrix[\"y\"][l] = len(d)\n",
    "    \n",
    "    weights = get_weights(categories)\n",
    "    \n",
    "    N = len(merged)\n",
    "    Pa = 0\n",
    "    for l in categories: \n",
    "        for k in categories: \n",
    "            Pa += weights[l][k]*cnt_matrix[l][k]/N\n",
    "    \n",
    "    Pe = 0\n",
    "    for l in categories: \n",
    "        for k in categories: \n",
    "            Pe += weights[l][k]*(acc_matrix[\"x\"][l]/N)*(acc_matrix[\"y\"][k]/N)\n",
    "    \n",
    "    if Pe==1:\n",
    "        raise Exception(\"Divide by zero\")\n",
    "    \n",
    "    kappa = (Pa-Pe)/(1-Pe)\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c0e3335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanScores/auth_arm.csv    HumanScores/auth_film.csv\r\n",
      "HumanScores/auth_b.csv      HumanScores/auth_get.csv\r\n",
      "HumanScores/auth_bow.csv    HumanScores/auth_sky.csv\r\n",
      "HumanScores/auth_ch.csv     HumanScores/auth_stars.csv\r\n",
      "HumanScores/auth_dy.csv     HumanScores/auth_top.csv\r\n",
      "HumanScores/auth_earth.csv  HumanScores/auth_view.csv\r\n",
      "HumanScores/auth_fern.csv   HumanScores/auth_wee.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls HumanScores/auth_*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "30c2c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [\"arm\", \"b\", \"bow\", \"ch\", \"dy\", \"earth\", \"fern\", \"film\", \"get\", \"sky\", \"stars\", \"top\", \"wee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "feb7a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clse_get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fmxshjo4aokO",
   "metadata": {
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1710873278921,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "fmxshjo4aokO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE HumanScores/auth_arm.csv 872\n",
      "DONE HumanScores/auth_b.csv 898\n",
      "DONE HumanScores/auth_bow.csv 908\n",
      "DONE HumanScores/auth_ch.csv 898\n",
      "DONE HumanScores/auth_dy.csv 908\n",
      "DONE HumanScores/auth_earth.csv 883\n",
      "DONE HumanScores/auth_fern.csv 904\n",
      "DONE HumanScores/auth_film.csv 897\n",
      "DONE HumanScores/auth_get.csv 903\n",
      "DONE HumanScores/auth_sky.csv 906\n",
      "DONE HumanScores/auth_stars.csv 323\n",
      "DONE HumanScores/auth_top.csv 903\n",
      "DONE HumanScores/auth_wee.csv 898\n",
      "DONE HumanScores/clse_arm.csv 479\n",
      "DONE HumanScores/clse_b.csv 450\n",
      "DONE HumanScores/clse_bow.csv 447\n",
      "DONE HumanScores/clse_ch.csv 388\n",
      "DONE HumanScores/clse_dy.csv 249\n",
      "DONE HumanScores/clse_earth.csv 399\n",
      "DONE HumanScores/clse_fern.csv 543\n",
      "DONE HumanScores/clse_film.csv 523\n",
      "SKIP HumanScores/clse_get.csv\n",
      "DONE HumanScores/clse_sky.csv 397\n",
      "DONE HumanScores/clse_stars.csv 245\n",
      "DONE HumanScores/clse_top.csv 551\n",
      "DONE HumanScores/clse_wee.csv 450\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tasks = [\"auth\", \"clse\"]\n",
    "\n",
    "scores = {\"auth\": [], \"clse\": []}\n",
    "for task in tasks:\n",
    "    filenames = [f\"HumanScores/{task}_{u}.csv\" for u in users]\n",
    "    for fin in filenames:\n",
    "        try:\n",
    "            hum = pd.read_csv(fin, skiprows=[0])\n",
    "        except:\n",
    "            print(f\"SKIP {fin}\")\n",
    "            continue\n",
    "\n",
    "        words = {}\n",
    "        columns = hum.columns\n",
    "        #     print(columns)\n",
    "        rows = []\n",
    "        for idx, row in hum.iterrows():\n",
    "            w = row[\"คำ (Word)\"]\n",
    "\n",
    "            for col in columns:\n",
    "                if col == \"Unnamed: 7\":\n",
    "                    break\n",
    "\n",
    "                if col in [\n",
    "                    \"คำ (Word)\",\n",
    "                    \"Unnamed: 7\",\n",
    "                    \"บอกไม่ได้ \\n(Cannot describe)\",\n",
    "                    \"บอกไม่ได้ \\r\\n(Cannot describe)\",\n",
    "                ]:\n",
    "                    continue\n",
    "\n",
    "                if pd.isna(row[col]):\n",
    "                    continue\n",
    "\n",
    "                label2score = {\n",
    "                    \"ไม่ชอบหน้ากัน  (Dislike)\": -2,\n",
    "                    \"ไม่รู้จักกัน (Unfamiliar)\": -1,\n",
    "                    \"แค่คนรู้จักกัน (acquainted)\": 0,\n",
    "                    \"สนิทกัน (Close)\": 1,\n",
    "                    \"สนิทกันมาก (Intimate)\": 2,\n",
    "                    \"ให้เกียรติมาก \\n(Highly Respectful)\": 2,\n",
    "                    \"ให้เกียรติ (Respectful)\": 1,\n",
    "                    \"ปกติ \\n(Normal)\": 0,\n",
    "                    \"ไม่ให้เกียรติ (Disrespectful)\": -1,\n",
    "                    \"ไม่ให้เกียรติมาก (Highly Disrespectful)\": -2,\n",
    "                }\n",
    "\n",
    "                if w in words:\n",
    "                    words[w].append(col)\n",
    "                else:\n",
    "                    words[w] = [col]\n",
    "\n",
    "                rows.append({\"word\": w, \"label\": col, \"score\": label2score[col]})\n",
    "\n",
    "        scores[task].append((fin, pd.DataFrame(rows)))\n",
    "        print(\"DONE\", fin, len(rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5ErwY2K8baQl",
   "metadata": {
    "executionInfo": {
     "elapsed": 368,
     "status": "ok",
     "timestamp": 1710872985480,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "5ErwY2K8baQl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN 13\n",
      "0 1 0.6192713262637028 872 898 863\n",
      "0 2 0.63598819659246 872 908 870\n",
      "0 3 0.6313825250072483 872 898 866\n",
      "0 4 0.6342332809787535 872 908 871\n",
      "0 5 0.7535291738050593 872 883 864\n",
      "0 6 0.577342664492725 872 904 867\n",
      "0 7 0.5972578215400749 872 897 862\n",
      "0 8 0.5913622014476444 872 903 866\n",
      "0 9 0.6414542175302737 872 906 869\n",
      "0 10 0.7241841096292495 872 323 310\n",
      "0 11 0.5956798213125805 872 903 868\n",
      "0 12 0.6311976431921142 872 898 866\n",
      "HumanScores/auth_arm.csv AVG 0.6360735818159905\n",
      "============\n",
      "1 0 0.6192713262637023 898 872 863\n",
      "1 2 0.6429634956933113 898 908 895\n",
      "1 3 0.661818773750652 898 898 886\n",
      "1 4 0.6473176324725831 898 908 896\n",
      "1 5 0.663849821605874 898 883 874\n",
      "1 6 0.568020525932211 898 904 890\n",
      "1 7 0.9603588121587129 898 897 899\n",
      "1 8 0.5966923548383851 898 903 892\n",
      "1 9 0.6398421075213465 898 906 893\n",
      "1 10 0.6894301126085746 898 323 315\n",
      "1 11 0.8278562995364465 898 903 895\n",
      "1 12 0.6552051413780987 898 898 886\n",
      "HumanScores/auth_b.csv AVG 0.6810522003133248\n",
      "============\n",
      "2 0 0.63598819659246 908 872 870\n",
      "2 1 0.6429634956933092 908 898 895\n",
      "2 3 0.7490880662840024 908 898 895\n",
      "2 4 0.9564186092600405 908 908 908\n",
      "2 5 0.6460169770714113 908 883 881\n",
      "2 6 0.6246448997172881 908 904 900\n",
      "2 7 0.6208620178530222 908 897 894\n",
      "2 8 0.6676978124158056 908 903 899\n",
      "2 9 0.7764078126819148 908 906 903\n",
      "2 10 0.7885092198814454 908 323 322\n",
      "2 11 0.6049637452460394 908 903 900\n",
      "2 12 0.7542539418504208 908 898 895\n",
      "HumanScores/auth_bow.csv AVG 0.70565123287893\n",
      "============\n",
      "3 0 0.6313825250072479 898 872 866\n",
      "3 1 0.6618187737506537 898 898 886\n",
      "3 2 0.7490880662840027 898 908 895\n",
      "3 4 0.7363249227319808 898 908 896\n",
      "3 5 0.6987646630420329 898 883 873\n",
      "3 6 0.6340901881734196 898 904 894\n",
      "3 7 0.6394879630442407 898 897 885\n",
      "3 8 0.6419156621616152 898 903 893\n",
      "3 9 0.881196808725186 898 906 898\n",
      "3 10 0.7187268092795144 898 323 317\n",
      "3 11 0.6064219791260315 898 903 890\n",
      "3 12 0.9917355752196108 898 898 898\n",
      "HumanScores/auth_ch.csv AVG 0.7159128280454614\n",
      "============\n",
      "4 0 0.6342332809787535 908 872 871\n",
      "4 1 0.6473176324725831 908 898 896\n",
      "4 2 0.9564186092600404 908 908 908\n",
      "4 3 0.7363249227319811 908 898 896\n",
      "4 5 0.6557972746118332 908 883 883\n",
      "4 6 0.6313622959899025 908 904 901\n",
      "4 7 0.6256705997796175 908 897 895\n",
      "4 8 0.6791485070288646 908 903 900\n",
      "4 9 0.7582375948563076 908 906 904\n",
      "4 10 0.7885092198814454 908 323 322\n",
      "4 11 0.6038885615469949 908 903 901\n",
      "4 12 0.7372356552893765 908 898 896\n",
      "HumanScores/auth_dy.csv AVG 0.704512012868975\n",
      "============\n",
      "5 0 0.7535291738050597 883 872 864\n",
      "5 1 0.6638498216058745 883 898 874\n",
      "5 2 0.6460169770714099 883 908 881\n",
      "5 3 0.6987646630420338 883 898 873\n",
      "5 4 0.655797274611834 883 908 883\n",
      "5 6 0.6608561417264943 883 904 876\n",
      "5 7 0.6423110031563412 883 897 873\n",
      "5 8 0.6620056685692776 883 903 875\n",
      "5 9 0.6748966125711024 883 906 880\n",
      "5 10 0.7616020823712154 883 323 312\n",
      "5 11 0.5949321758996063 883 903 878\n",
      "5 12 0.7037011670593394 883 898 873\n",
      "HumanScores/auth_earth.csv AVG 0.6765218967907991\n",
      "============\n",
      "6 0 0.5773426644927234 904 872 867\n",
      "6 1 0.5680205259322104 904 898 890\n",
      "6 2 0.6246448997172875 904 908 900\n",
      "6 3 0.6340901881734184 904 898 894\n",
      "6 4 0.631362295989903 904 908 901\n",
      "6 5 0.6608561417264943 904 883 876\n",
      "6 7 0.5505908198928074 904 897 889\n",
      "6 8 0.6676453897209399 904 903 898\n",
      "6 9 0.625976645135814 904 906 899\n",
      "6 10 0.7164748657965602 904 323 321\n",
      "6 11 0.5103735491748929 904 903 896\n",
      "6 12 0.6309305988063627 904 898 894\n",
      "HumanScores/auth_fern.csv AVG 0.6165257153799512\n",
      "============\n",
      "7 0 0.5972578215400745 897 872 862\n",
      "7 1 0.9603588121587129 897 898 899\n",
      "7 2 0.6208620178530226 897 908 894\n",
      "7 3 0.6394879630442418 897 898 885\n",
      "7 4 0.6256705997796178 897 908 895\n",
      "7 5 0.6423110031563407 897 883 873\n",
      "7 6 0.5505908198928056 897 904 889\n",
      "7 8 0.5793867027885664 897 903 891\n",
      "7 9 0.617430518495337 897 906 892\n",
      "7 10 0.6894301126085746 897 323 315\n",
      "7 11 0.8071031860983257 897 903 894\n",
      "7 12 0.633053447440659 897 898 885\n",
      "HumanScores/auth_film.csv AVG 0.6635785837380233\n",
      "============\n",
      "8 0 0.5913622014476423 903 872 866\n",
      "8 1 0.5966923548383857 903 898 892\n",
      "8 2 0.6676978124158076 903 908 899\n",
      "8 3 0.6419156621616152 903 898 893\n",
      "8 4 0.6791485070288646 903 908 900\n",
      "8 5 0.6620056685692776 903 883 875\n",
      "8 6 0.6676453897209403 903 904 898\n",
      "8 7 0.5793867027885676 903 897 891\n",
      "8 9 0.6536573288892826 903 906 898\n",
      "8 10 0.7218589096207003 903 323 320\n",
      "8 11 0.5270309841966822 903 903 895\n",
      "8 12 0.6476069873091813 903 898 893\n",
      "HumanScores/auth_get.csv AVG 0.636334042415579\n",
      "============\n",
      "9 0 0.6414542175302752 906 872 869\n",
      "9 1 0.6398421075213477 906 898 893\n",
      "9 2 0.7764078126819148 906 908 903\n",
      "9 3 0.881196808725186 906 898 898\n",
      "9 4 0.7582375948563062 906 908 904\n",
      "9 5 0.674896612571102 906 883 880\n",
      "9 6 0.625976645135814 906 904 899\n",
      "9 7 0.6174305184953366 906 897 892\n",
      "9 8 0.6536573288892837 906 903 898\n",
      "9 10 0.7257357012052295 906 323 322\n",
      "9 11 0.5968544379279594 906 903 898\n",
      "9 12 0.8816128155783122 906 898 898\n",
      "HumanScores/auth_sky.csv AVG 0.7061085500931722\n",
      "============\n",
      "10 0 0.7241841096292495 323 872 310\n",
      "10 1 0.6894301126085746 323 898 315\n",
      "10 2 0.7885092198814452 323 908 322\n",
      "10 3 0.7187268092795148 323 898 317\n",
      "10 4 0.7885092198814452 323 908 322\n",
      "10 5 0.7616020823712164 323 883 312\n",
      "10 6 0.7164748657965602 323 904 321\n",
      "10 7 0.6894301126085746 323 897 315\n",
      "10 8 0.7218589096207003 323 903 320\n",
      "10 9 0.7257357012052297 323 906 322\n",
      "10 11 0.6528684179526452 323 903 318\n",
      "10 12 0.7187268092795148 323 898 317\n",
      "HumanScores/auth_stars.csv AVG 0.7246713641762225\n",
      "============\n",
      "11 0 0.5956798213125792 903 872 868\n",
      "11 1 0.8278562995364476 903 898 895\n",
      "11 2 0.6049637452460381 903 908 900\n",
      "11 3 0.606421979126032 903 898 890\n",
      "11 4 0.6038885615469949 903 908 901\n",
      "11 5 0.5949321758996063 903 883 878\n",
      "11 6 0.5103735491748935 903 904 896\n",
      "11 7 0.8071031860983254 903 897 894\n",
      "11 8 0.5270309841966827 903 903 895\n",
      "11 9 0.5968544379279582 903 906 898\n",
      "11 10 0.6528684179526454 903 323 318\n",
      "11 12 0.5989344738764794 903 898 890\n",
      "HumanScores/auth_top.csv AVG 0.6272423026578903\n",
      "============\n",
      "12 0 0.6311976431921142 898 872 866\n",
      "12 1 0.6552051413780983 898 898 886\n",
      "12 2 0.7542539418504212 898 908 895\n",
      "12 3 0.9917355752196108 898 898 898\n",
      "12 4 0.7372356552893783 898 908 896\n",
      "12 5 0.703701167059338 898 883 873\n",
      "12 6 0.6309305988063643 898 904 894\n",
      "12 7 0.633053447440659 898 897 885\n",
      "12 8 0.6476069873091803 898 903 893\n",
      "12 9 0.8816128155783122 898 906 898\n",
      "12 10 0.7187268092795144 898 323 317\n",
      "12 11 0.5989344738764798 898 903 890\n",
      "HumanScores/auth_wee.csv AVG 0.7153495213566226\n",
      "============\n",
      "LEN 12\n",
      "0 1 0.8986849734086093 479 450 442\n",
      "0 2 0.6147003501778266 479 447 359\n",
      "0 3 0.6080160320641288 479 388 326\n",
      "0 4 0.4884742041712406 479 249 233\n",
      "0 5 0.587209517808828 479 399 330\n",
      "0 6 0.568146736230751 479 543 415\n",
      "0 7 0.5505266961042448 479 523 398\n",
      "0 8 0.5956061170733552 479 397 330\n",
      "0 9 0.5368642850301693 479 245 231\n",
      "0 10 0.5678714843414931 479 551 419\n",
      "0 11 0.6080624667412384 479 450 360\n",
      "HumanScores/clse_arm.csv AVG 0.6021966239228985\n",
      "============\n",
      "1 0 0.8986849734086093 450 479 442\n",
      "1 2 0.6605511187855239 450 447 350\n",
      "1 3 0.551053501028671 450 388 321\n",
      "1 4 0.5553573658205948 450 249 228\n",
      "1 5 0.5402253194481368 450 399 324\n",
      "1 6 0.6478707605050773 450 543 393\n",
      "1 7 0.6416267310667526 450 523 378\n",
      "1 8 0.5483622627230055 450 397 325\n",
      "1 9 0.5371222120012265 450 245 224\n",
      "1 10 0.6466035510223771 450 551 397\n",
      "1 11 0.6534470950804704 450 450 351\n",
      "HumanScores/clse_b.csv AVG 0.6255368082627677\n",
      "============\n",
      "2 0 0.6147003501778268 447 479 359\n",
      "2 1 0.6605511187855246 447 450 350\n",
      "2 3 0.6009233153401874 447 388 334\n",
      "2 4 0.6084369214493521 447 249 231\n",
      "2 5 0.5741644278526971 447 399 346\n",
      "2 6 0.6526220514225068 447 543 395\n",
      "2 7 0.6217493771765182 447 523 386\n",
      "2 8 0.6044819750586163 447 397 340\n",
      "2 9 0.604511507035329 447 245 233\n",
      "2 10 0.6565506470983185 447 551 398\n",
      "2 11 0.9817226341735721 447 450 447\n",
      "HumanScores/clse_bow.csv AVG 0.6527649386882225\n",
      "============\n",
      "3 0 0.6080160320641281 388 479 326\n",
      "3 1 0.551053501028671 388 450 321\n",
      "3 2 0.6009233153401874 388 447 334\n",
      "3 4 0.607919658653346 388 249 220\n",
      "3 5 0.8940194420963653 388 399 374\n",
      "3 6 0.5239851718939509 388 543 352\n",
      "3 7 0.5979552163582693 388 523 349\n",
      "3 8 0.9118786231842269 388 397 382\n",
      "3 9 0.6961295563533447 388 245 223\n",
      "3 10 0.5228079781199239 388 551 352\n",
      "3 11 0.5987375721642454 388 450 334\n",
      "HumanScores/clse_ch.csv AVG 0.6466750970233326\n",
      "============\n",
      "4 0 0.48847420417124043 249 479 233\n",
      "4 1 0.5553573658205948 249 450 228\n",
      "4 2 0.6084369214493524 249 447 231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3 0.607919658653346 249 388 220\n",
      "4 5 0.648638847553427 249 399 223\n",
      "4 6 0.5434078206432265 249 543 236\n",
      "4 7 0.6253251554628777 249 523 234\n",
      "4 8 0.6381737876922875 249 397 219\n",
      "4 9 0.6801786236410515 249 245 188\n",
      "4 10 0.5410035104389366 249 551 235\n",
      "4 11 0.5973832177349078 249 450 231\n",
      "HumanScores/clse_dy.csv AVG 0.5940271921146589\n",
      "============\n",
      "5 0 0.5872095178088289 399 479 330\n",
      "5 1 0.5402253194481373 399 450 324\n",
      "5 2 0.5741644278526978 399 447 346\n",
      "5 3 0.8940194420963653 399 388 374\n",
      "5 4 0.648638847553427 399 249 223\n",
      "5 6 0.44995980512750944 399 543 360\n",
      "5 7 0.560036220640678 399 523 353\n",
      "5 8 0.9520879770731933 399 397 383\n",
      "5 9 0.7160999961201712 399 245 224\n",
      "5 10 0.4480456766967597 399 551 359\n",
      "5 11 0.5924213178136354 399 450 346\n",
      "HumanScores/clse_earth.csv AVG 0.632991686202855\n",
      "============\n",
      "6 0 0.568146736230751 543 479 415\n",
      "6 1 0.6478707605050773 543 450 393\n",
      "6 2 0.6526220514225065 543 447 395\n",
      "6 3 0.5239851718939509 543 388 352\n",
      "6 4 0.5434078206432263 543 249 236\n",
      "6 5 0.44995980512750977 543 399 360\n",
      "6 7 0.8639448367184628 543 523 503\n",
      "6 8 0.48733924394723166 543 397 358\n",
      "6 9 0.5793781321815218 543 245 233\n",
      "6 10 0.9993647422576499 543 551 542\n",
      "6 11 0.636984127317788 543 450 397\n",
      "HumanScores/clse_fern.csv AVG 0.632091220749607\n",
      "============\n",
      "7 0 0.5505266961042444 523 479 398\n",
      "7 1 0.6416267310667519 523 450 378\n",
      "7 2 0.6217493771765182 523 447 386\n",
      "7 3 0.5979552163582693 523 388 349\n",
      "7 4 0.6253251554628777 523 249 234\n",
      "7 5 0.5600362206406777 523 399 353\n",
      "7 6 0.8639448367184631 523 543 503\n",
      "7 8 0.5687093694476455 523 397 352\n",
      "7 9 0.597823141801327 523 245 231\n",
      "7 10 0.8628219926536899 523 551 509\n",
      "7 11 0.6062344523940562 523 450 388\n",
      "HumanScores/clse_film.csv AVG 0.6451593808931383\n",
      "============\n",
      "8 0 0.595606117073356 397 479 330\n",
      "8 1 0.5483622627230058 397 450 325\n",
      "8 2 0.6044819750586166 397 447 340\n",
      "8 3 0.9118786231842269 397 388 382\n",
      "8 4 0.6381737876922875 397 249 219\n",
      "8 5 0.9520879770731929 397 399 383\n",
      "8 6 0.48733924394723166 397 543 358\n",
      "8 7 0.5687093694476457 397 523 352\n",
      "8 9 0.680993680063473 397 245 220\n",
      "8 10 0.4862229907533307 397 551 358\n",
      "8 11 0.6024252480708934 397 450 340\n",
      "HumanScores/clse_sky.csv AVG 0.6432982977352054\n",
      "============\n",
      "9 0 0.5368642850301696 245 479 231\n",
      "9 1 0.5371222120012268 245 450 224\n",
      "9 2 0.6045115070353289 245 447 233\n",
      "9 3 0.6961295563533448 245 388 223\n",
      "9 4 0.6801786236410519 245 249 188\n",
      "9 5 0.7160999961201712 245 399 224\n",
      "9 6 0.5793781321815216 245 543 233\n",
      "9 7 0.597823141801327 245 523 231\n",
      "9 8 0.680993680063473 245 397 220\n",
      "9 10 0.5773051700217852 245 551 232\n",
      "9 11 0.59419919652602 245 450 233\n",
      "HumanScores/clse_stars.csv AVG 0.6182368637068564\n",
      "============\n",
      "10 0 0.5678714843414931 551 479 419\n",
      "10 1 0.6466035510223763 551 450 397\n",
      "10 2 0.6565506470983181 551 447 398\n",
      "10 3 0.5228079781199239 551 388 352\n",
      "10 4 0.5410035104389369 551 249 235\n",
      "10 5 0.4480456766967597 551 399 359\n",
      "10 6 0.9993647422576499 551 543 542\n",
      "10 7 0.86282199265369 551 523 509\n",
      "10 8 0.4862229907533307 551 397 358\n",
      "10 9 0.5773051700217853 551 245 232\n",
      "10 11 0.6409699601882015 551 450 400\n",
      "HumanScores/clse_top.csv AVG 0.6317788821447696\n",
      "============\n",
      "11 0 0.6080624667412392 450 479 360\n",
      "11 1 0.6534470950804704 450 450 351\n",
      "11 2 0.9817226341735721 450 447 447\n",
      "11 3 0.5987375721642451 450 388 334\n",
      "11 4 0.5973832177349079 450 249 231\n",
      "11 5 0.5924213178136354 450 399 346\n",
      "11 6 0.636984127317788 450 543 397\n",
      "11 7 0.6062344523940562 450 523 388\n",
      "11 8 0.6024252480708935 450 397 340\n",
      "11 9 0.5941991965260203 450 245 233\n",
      "11 10 0.6409699601882011 450 551 400\n",
      "HumanScores/clse_wee.csv AVG 0.6465988443822754\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "CLOSENESS_LABELS = [\n",
    "    'ไม่ชอบหน้ากัน  (Dislike)',\n",
    "    'ไม่รู้จักกัน (Unfamiliar)',\n",
    "    'แค่คนรู้จักกัน (acquainted)',\n",
    "    'สนิทกัน (Close)',\n",
    "    'สนิทกันมาก (Intimate)',\n",
    "]\n",
    "\n",
    "AUTHORITY_LABELS = [\n",
    "    'ให้เกียรติมาก \\n(Highly Respectful)',\n",
    "    'ให้เกียรติ (Respectful)',\n",
    "    'ปกติ \\n(Normal)',\n",
    "    'ไม่ให้เกียรติ (Disrespectful)',\n",
    "    'ไม่ให้เกียรติมาก (Highly Disrespectful)',\n",
    "]\n",
    "\n",
    "for task in tasks:\n",
    "    if task==\"clse\":\n",
    "        LABELS = CLOSENESS_LABELS\n",
    "    else:\n",
    "        LABELS = AUTHORITY_LABELS\n",
    "\n",
    "    print(\"LEN\", len(scores[task]))\n",
    "\n",
    "    for i in range(len(scores[task])):\n",
    "        fin1, data1 = scores[task][i]\n",
    "        all_iaa = []\n",
    "        for j in range(len(scores[task])):\n",
    "            if i==j:\n",
    "                continue\n",
    "                \n",
    "            _, data2 = scores[task][j]\n",
    "            iaa = cal_agreement(data1, data2, column=\"word\", categories=LABELS, cat_column=\"label\")\n",
    "            M = pd.merge(data1, data2, on=\"word\")\n",
    "            print(i, j, iaa, len(data1), len(data2), len(M))\n",
    "            all_iaa.append(iaa)\n",
    "\n",
    "    \n",
    "        avg_iaa = sum(all_iaa)/len(all_iaa)\n",
    "        print(fin1, \"AVG\", avg_iaa)\n",
    "        print(\"============\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "76f91289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d5cac1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# users[9], users[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec0c0d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = pd.merge(data1, data2, on=\"word\")\n",
    "# m[m[\"label_x\"]==m[\"label_y\"]]\n",
    "# confusion_matrix(m[\"label_x\"], m[\"label_y\"], labels=CLOSENESS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cd37eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "SxP9m8wgXbyh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1710873281495,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "SxP9m8wgXbyh",
    "outputId": "ffc6da0b-3151-4efd-bd11-728148290baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 records from Lexicons/v3/lexicon_task2_clse.jsonl\n",
      "Loaded 2 records from Lexicons/v3/lexicon_task3_clse.jsonl\n"
     ]
    }
   ],
   "source": [
    "lexicon_paths = [\n",
    "    # \"Lexicons/v3/lexicon_task1_clse.jsonl\",\n",
    "    \"Lexicons/v3/lexicon_task2_clse.jsonl\",\n",
    "    \"Lexicons/v3/lexicon_task3_clse.jsonl\",\n",
    "    # \"Lexicons/v3/lexicon_task1_auth.jsonl\",\n",
    "    # \"Lexicons/v3/lexicon_task2_auth.jsonl\",\n",
    "    # \"Lexicons/v3/lexicon_task3_auth.jsonl\"\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "data = {}\n",
    "for path in lexicon_paths:\n",
    "    lexicons, lexicons_by_feat = load_jsonl(path)\n",
    "    rows = []\n",
    "    for cat in lexicons:\n",
    "\n",
    "        keys = [k for k, v in sorted(lexicons[cat].items(), key=lambda item: -item[1])]\n",
    "        keys = keys[0:200]\n",
    "        \n",
    "        \n",
    "        \n",
    "        for w in keys:\n",
    "            o = {\n",
    "                \"word\": w,\n",
    "                \"shapley\": (lexicons[cat][w]),\n",
    "                \"category\": cat,\n",
    "                \"path\": path,\n",
    "            }\n",
    "            \n",
    "            for feat in lexicons_by_feat[\"overall\"]:\n",
    "                if w in lexicons_by_feat[\"overall\"][feat]:\n",
    "                    o[feat] = True\n",
    "                else:\n",
    "                    o[feat] = False\n",
    "    \n",
    "            rows.append(o)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    M = df\n",
    "    for idx, score in enumerate(scores[\"clse\"]):\n",
    "        fin, D = score\n",
    "        M = pd.merge(M, D, how=\"left\", on=\"word\", suffixes=(\"\", f\"_{idx}\"))\n",
    "    \n",
    "    agg_labels = {\n",
    "        \"common\": [], \n",
    "        \"common_w_cutoff\": [], \n",
    "        \"avg\": [],\n",
    "        \"avg_w_cutoff\": [], \n",
    "    }\n",
    "    \n",
    "    for idx, row in M.iterrows():\n",
    "        values = defaultdict(int)\n",
    "        for col in M.columns:\n",
    "            if not col.startswith(\"score_\"):\n",
    "                continue\n",
    "            \n",
    "            if pd.isna(row[col]):\n",
    "                continue\n",
    "                \n",
    "            values[row[col]] += 1\n",
    "        \n",
    "        \n",
    "        if values=={}:\n",
    "            agg_labels[\"common\"].append(None)\n",
    "            agg_labels[\"common_w_cutoff\"].append(None)\n",
    "            agg_labels[\"avg\"].append(None)\n",
    "            agg_labels[\"avg_w_cutoff\"].append(None)\n",
    "            continue\n",
    "        \n",
    "            \n",
    "        common_label = max(values, key=values.get)\n",
    "        agg_labels[\"common\"].append(common_label)\n",
    "        \n",
    "        if values[common_label] >= len(scores[\"clse\"])*0.5:\n",
    "            agg_labels[\"common_w_cutoff\"].append(common_label)\n",
    "        else:\n",
    "            agg_labels[\"common_w_cutoff\"].append(None)\n",
    "        \n",
    "        weight_avg_a = 0\n",
    "        weight_avg_b = 0\n",
    "        for w, c in values.items():\n",
    "            weight_avg_a += w*c\n",
    "            weight_avg_b += c\n",
    "        \n",
    "        weight_avg = weight_avg_a/weight_avg_b\n",
    "        agg_labels[\"avg\"].append(weight_avg)\n",
    "        \n",
    "        if values[common_label] >= len(scores[\"clse\"])*0.5:\n",
    "            agg_labels[\"avg_w_cutoff\"].append(weight_avg)\n",
    "        else:\n",
    "            agg_labels[\"avg_w_cutoff\"].append(None)\n",
    "            \n",
    "    for agg_type in agg_labels:\n",
    "        M[agg_type] = agg_labels[agg_type]\n",
    "    \n",
    "#         break\n",
    "    M = M.rename(columns={\"label\": \"label_0\", \"score\": \"score_0\"})\n",
    "    M.to_csv(\"HumanScores/cls.csv\", index=False)\n",
    "    data[path] = M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7UhLIFj4coci",
   "metadata": {
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1710873283032,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "7UhLIFj4coci"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "YrFus_iZjobz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1710873299248,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "YrFus_iZjobz",
    "outputId": "523fdf97-d4db-4dbe-9ced-eb5d717a5796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicons/v3/lexicon_task2_clse.jsonl\n",
      "score_0 544 0.180 0.000\n",
      "score_1 522 0.171 0.000\n",
      "score_2 511 0.051 0.250\n",
      "score_3 456 0.056 0.230\n",
      "score_4 330 0.107 0.052\n",
      "score_5 470 0.049 0.287\n",
      "score_6 576 0.164 0.000\n",
      "score_7 550 0.147 0.001\n",
      "score_8 463 0.067 0.147\n",
      "score_9 323 0.194 0.000\n",
      "score_10 579 0.154 0.000\n",
      "score_11 517 0.054 0.217\n",
      "common 667 0.152 0.000\n",
      "\t >> pronoun 42 0.341 0.027\n",
      "\t >> particles 35 0.323 0.058\n",
      "\t >> misspelling 309 0.261 0.000\n",
      "common_w_cutoff 368 0.107 0.040\n",
      "\t >> pronoun 29 0.350 0.063\n",
      "\t >> particles 23 0.115 0.601\n",
      "\t >> misspelling 194 0.064 0.376\n",
      "avg 667 0.149 0.000\n",
      "\t >> pronoun 42 0.411 0.007\n",
      "\t >> particles 35 0.322 0.059\n",
      "\t >> misspelling 309 0.272 0.000\n",
      "avg_w_cutoff 368 0.109 0.036\n",
      "\t >> pronoun 29 0.353 0.060\n",
      "\t >> particles 23 0.225 0.303\n",
      "\t >> misspelling 194 0.146 0.042\n",
      "============\n",
      "Lexicons/v3/lexicon_task3_clse.jsonl\n",
      "score_0 428 0.204 0.000\n",
      "score_1 407 0.237 0.000\n",
      "score_2 403 0.117 0.019\n",
      "score_3 372 0.111 0.032\n",
      "score_4 235 0.103 0.116\n",
      "score_5 379 0.065 0.204\n",
      "score_6 486 0.147 0.001\n",
      "score_7 481 0.095 0.037\n",
      "score_8 376 0.092 0.075\n",
      "score_9 226 0.199 0.003\n",
      "score_10 496 0.164 0.000\n",
      "score_11 405 0.110 0.026\n",
      "common 581 0.208 0.000\n",
      "\t >> pronoun 34 0.567 0.000\n",
      "\t >> particles 30 0.177 0.350\n",
      "\t >> misspelling 244 0.282 0.000\n",
      "common_w_cutoff 276 0.190 0.002\n",
      "\t >> pronoun 23 0.671 0.000\n",
      "\t >> particles 18 -0.152 0.546\n",
      "\t >> misspelling 145 0.236 0.004\n",
      "avg 581 0.209 0.000\n",
      "\t >> pronoun 34 0.555 0.001\n",
      "\t >> particles 30 0.281 0.133\n",
      "\t >> misspelling 244 0.287 0.000\n",
      "avg_w_cutoff 276 0.174 0.004\n",
      "\t >> pronoun 23 0.674 0.000\n",
      "\t >> particles 18 -0.043 0.865\n",
      "\t >> misspelling 145 0.122 0.145\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "# o = [\n",
    "#     'ไม่ชอบหน้ากัน  (Dislike)',\n",
    "#     'ไม่รู้จักกัน (Unfamiliar)',\n",
    "#     'แค่คนรู้จักกัน (Acquainted)',\n",
    "#     'สนิทกัน (Close)',\n",
    "#     'สนิทกันมาก (Intimate)',\n",
    "# ]\n",
    "\n",
    "# ho = [ '1. Close', '2. Know each other', \"3. Don't know each other\", \"overall\"]\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "for path in lexicon_paths:\n",
    "    M = data[path]\n",
    "    print(path)\n",
    "    for idx, score in enumerate(scores[\"clse\"]):\n",
    "        col = f\"score_{idx}\"\n",
    "        d = M[[\"shapley\", col]]\n",
    "        d = d.dropna()\n",
    "        res = stats.spearmanr(d[\"shapley\"], d[col])\n",
    "#         print(d[\"shapley\"], d[col])\n",
    "        print(f\"{col} {len(d)} {res.statistic:.3f} {res.pvalue:.3f}\")\n",
    "    \n",
    "    for agg_type in [\n",
    "        \"common\",\n",
    "        \"common_w_cutoff\",\n",
    "        \"avg\",\n",
    "        \"avg_w_cutoff\",\n",
    "    ]:\n",
    "            \n",
    "        col = agg_type\n",
    "        d = M[[\"shapley\", col]]\n",
    "        d = d.dropna()\n",
    "        res = stats.spearmanr(d[\"shapley\"], d[col])\n",
    "        print(f\"{col} {len(d)} {res.statistic:.3f} {res.pvalue:.3f}\")\n",
    "        for feat in ['pronoun', 'particles', 'misspelling']:\n",
    "            d = M[M[feat]==True][[\"shapley\", col]]\n",
    "            d = d.dropna()\n",
    "            res = stats.spearmanr(d[\"shapley\"], d[col])\n",
    "            print(f\"\\t >> {feat} {len(d)} {res.statistic:.3f} {res.pvalue:.3f}\")\n",
    "    \n",
    "    print(\"============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f45bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c2f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bc266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d5d17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee1c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2072462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ef2d7708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 records from Lexicons/v3/lexicon_task2_auth.jsonl\n",
      "Loaded 2 records from Lexicons/v3/lexicon_task3_auth.jsonl\n"
     ]
    }
   ],
   "source": [
    "lexicon_paths = [\n",
    "    # \"Lexicons/v3/lexicon_task1_clse.jsonl\",\n",
    "#     \"Lexicons/v3/lexicon_task2_clse.jsonl\",\n",
    "#     \"Lexicons/v3/lexicon_task3_clse.jsonl\",\n",
    "    # \"Lexicons/v3/lexicon_task1_auth.jsonl\",\n",
    "    \"Lexicons/v3/lexicon_task2_auth.jsonl\",\n",
    "    \"Lexicons/v3/lexicon_task3_auth.jsonl\"\n",
    "]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "data = {}\n",
    "for path in lexicon_paths:\n",
    "    lexicons, lexicons_by_feat = load_jsonl(path)\n",
    "    rows = []\n",
    "    for cat in lexicons:\n",
    "\n",
    "        keys = [k for k, v in sorted(lexicons[cat].items(), key=lambda item: -item[1])]\n",
    "        keys = keys[0:200]\n",
    "\n",
    "        for w in keys:\n",
    "            o = {\n",
    "                \"word\": w,\n",
    "                \"shapley\": (lexicons[cat][w]),\n",
    "                \"category\": cat,\n",
    "                \"path\": path,\n",
    "            }\n",
    "            \n",
    "            for feat in lexicons_by_feat[\"overall\"]:\n",
    "                if w in lexicons_by_feat[\"overall\"][feat]:\n",
    "                    o[feat] = True\n",
    "                else:\n",
    "                    o[feat] = False\n",
    "    \n",
    "            rows.append(o)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    M = df\n",
    "    for idx, score in enumerate(scores[\"auth\"]):\n",
    "        fin, D = score\n",
    "        M = pd.merge(M, D, how=\"left\", on=\"word\", suffixes=(\"\", f\"_{idx}\"))\n",
    "    \n",
    "    agg_labels = {\n",
    "        \"common\": [], \n",
    "        \"common_w_cutoff\": [], \n",
    "        \"avg\": [],\n",
    "        \"avg_w_cutoff\": [], \n",
    "    }\n",
    "    \n",
    "    for idx, row in M.iterrows():\n",
    "        values = defaultdict(int)\n",
    "        for col in M.columns:\n",
    "            if not col.startswith(\"score_\"):\n",
    "                continue\n",
    "            \n",
    "            if pd.isna(row[col]):\n",
    "                continue\n",
    "                \n",
    "            values[row[col]] += 1\n",
    "        \n",
    "        \n",
    "        if values=={}:\n",
    "            agg_labels[\"common\"].append(None)\n",
    "            agg_labels[\"common_w_cutoff\"].append(None)\n",
    "            agg_labels[\"avg\"].append(None)\n",
    "            agg_labels[\"avg_w_cutoff\"].append(None)\n",
    "            continue\n",
    "        \n",
    "            \n",
    "        common_label = max(values, key=values.get)\n",
    "        agg_labels[\"common\"].append(common_label)\n",
    "        \n",
    "        if values[common_label] >= len(scores[\"clse\"])*0.5:\n",
    "            agg_labels[\"common_w_cutoff\"].append(common_label)\n",
    "        else:\n",
    "            agg_labels[\"common_w_cutoff\"].append(None)\n",
    "        \n",
    "        weight_avg_a = 0\n",
    "        weight_avg_b = 0\n",
    "        for w, c in values.items():\n",
    "            weight_avg_a += w*c\n",
    "            weight_avg_b += c\n",
    "        \n",
    "        weight_avg = weight_avg_a/weight_avg_b\n",
    "        agg_labels[\"avg\"].append(weight_avg)\n",
    "        \n",
    "        if values[common_label] >= len(scores[\"clse\"])*0.5:\n",
    "            agg_labels[\"avg_w_cutoff\"].append(weight_avg)\n",
    "        else:\n",
    "            agg_labels[\"avg_w_cutoff\"].append(None)\n",
    "            \n",
    "    for agg_type in agg_labels:\n",
    "        M[agg_type] = agg_labels[agg_type]\n",
    "    \n",
    "#         break\n",
    "    M = M.rename(columns={\"label\": \"label_0\", \"score\": \"score_0\"})\n",
    "    M.to_csv(\"HumanScores/auth.csv\", index=False)\n",
    "    data[path] = M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7e72eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DrcsE0Qdo_ZH",
   "metadata": {
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1710873017041,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "DrcsE0Qdo_ZH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "sISYbibTo0-d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1710873270322,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "sISYbibTo0-d",
    "outputId": "e700bffa-f131-4d22-ccd6-07fb042bbe91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicons/v3/lexicon_task2_auth.jsonl\n",
      "score_0 758 0.074 0.042\n",
      "score_1 786 0.117 0.001\n",
      "score_2 793 0.087 0.014\n",
      "score_3 783 0.051 0.156\n",
      "score_4 792 0.091 0.011\n",
      "score_5 771 0.101 0.005\n",
      "score_6 790 0.068 0.055\n",
      "score_7 786 0.099 0.006\n",
      "score_8 785 0.089 0.013\n",
      "score_9 791 0.075 0.036\n",
      "score_10 295 -0.072 0.218\n",
      "score_11 788 0.115 0.001\n",
      "common 797 0.089 0.012\n",
      "\t >> pronoun 7 0.118 0.801\n",
      "\t >> particles 21 -0.032 0.891\n",
      "\t >> misspelling 231 0.077 0.242\n",
      "common_w_cutoff 771 0.076 0.034\n",
      "\t >> pronoun 7 0.118 0.801\n",
      "\t >> particles 21 -0.032 0.891\n",
      "\t >> misspelling 227 0.083 0.215\n",
      "avg 797 0.092 0.009\n",
      "\t >> pronoun 7 0.000 1.000\n",
      "\t >> particles 21 -0.032 0.891\n",
      "\t >> misspelling 231 0.044 0.510\n",
      "avg_w_cutoff 771 0.078 0.030\n",
      "\t >> pronoun 7 0.000 1.000\n",
      "\t >> particles 21 -0.032 0.891\n",
      "\t >> misspelling 227 0.036 0.593\n",
      "============\n",
      "Lexicons/v3/lexicon_task3_auth.jsonl\n",
      "score_0 775 0.027 0.451\n",
      "score_1 790 -0.010 0.784\n",
      "score_2 801 0.072 0.040\n",
      "score_3 794 0.067 0.061\n",
      "score_4 800 0.080 0.023\n",
      "score_5 777 -0.014 0.689\n",
      "score_6 793 0.037 0.294\n",
      "score_7 789 0.009 0.811\n",
      "score_8 798 0.014 0.684\n",
      "score_9 799 0.080 0.024\n",
      "score_10 281 0.002 0.970\n",
      "score_11 797 0.077 0.030\n",
      "common 804 0.035 0.322\n",
      "\t >> pronoun 21 0.310 0.172\n",
      "\t >> particles 32 0.200 0.272\n",
      "\t >> misspelling 236 0.045 0.493\n",
      "common_w_cutoff 770 0.026 0.467\n",
      "\t >> pronoun 19 0.430 0.066\n",
      "\t >> particles 32 0.200 0.272\n",
      "\t >> misspelling 227 0.032 0.636\n",
      "avg 804 0.069 0.051\n",
      "\t >> pronoun 21 0.433 0.050\n",
      "\t >> particles 32 0.380 0.032\n",
      "\t >> misspelling 236 0.065 0.319\n",
      "avg_w_cutoff 770 0.051 0.154\n",
      "\t >> pronoun 19 0.453 0.051\n",
      "\t >> particles 32 0.380 0.032\n",
      "\t >> misspelling 227 0.033 0.616\n",
      "============\n"
     ]
    }
   ],
   "source": [
    "# o = [\n",
    "#     'ให้เกียรติมาก \\n(Highly Respectful)',\n",
    "#     'ให้เกียรติ (Respectful)',\n",
    "#     'ปกติ \\n(Normal)',\n",
    "#     'ไม่ให้เกียรติ (Disrespectful)',\n",
    "#     'ไม่ให้เกียรติมาก (Highly Disrespectful)',\n",
    "# ]\n",
    "\n",
    "# ho = ['1. Respect', '2. Normal', '3. Not respect', \"overall\"]\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "for path in lexicon_paths:\n",
    "    M = data[path]\n",
    "    print(path)\n",
    "    for idx, score in enumerate(scores[\"clse\"]):\n",
    "        col = f\"score_{idx}\"\n",
    "        d = M[[\"shapley\", col]]\n",
    "        d = d.dropna()\n",
    "        res = stats.spearmanr(d[\"shapley\"], d[col])\n",
    "#         print(d[\"shapley\"], d[col])\n",
    "        print(f\"{col} {len(d)} {res.statistic:.3f} {res.pvalue:.3f}\")\n",
    "    \n",
    "    for agg_type in [\n",
    "        \"common\",\n",
    "        \"common_w_cutoff\",\n",
    "        \"avg\",\n",
    "        \"avg_w_cutoff\",\n",
    "    ]:\n",
    "            \n",
    "        col = agg_type\n",
    "        d = M[[\"shapley\", col]]\n",
    "        d = d.dropna()\n",
    "        res = stats.spearmanr(d[\"shapley\"], d[col])\n",
    "        print(f\"{col} {len(d)} {res.statistic:.3f} {res.pvalue:.3f}\")\n",
    "        for feat in ['pronoun', 'particles', 'misspelling']:\n",
    "            d = M[M[feat]==True][[\"shapley\", col]]\n",
    "            d = d.dropna()\n",
    "            res = stats.spearmanr(d[\"shapley\"], d[col])\n",
    "            print(f\"\\t >> {feat} {len(d)} {res.statistic:.3f} {res.pvalue:.3f}\")\n",
    "    \n",
    "    print(\"============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GPN5y0vf_UR2",
   "metadata": {
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1710873215194,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "GPN5y0vf_UR2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6lqn3EbBXa5N",
   "metadata": {
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1710873394234,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "6lqn3EbBXa5N"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# print(ho[0])\n",
    "# # dd = df[df[\"category\"]==ho[0]]\n",
    "# # dd = df\n",
    "# plot = sns.violinplot(data=dd, x=\"human\", y=\"shapley\", order=o, hue_order=ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "E4_HAz9NXa8F",
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1710873151673,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "E4_HAz9NXa8F"
   },
   "outputs": [],
   "source": [
    "# dd = df[df[\"category\"]==ho[1]]\n",
    "# plot = sns.violinplot(data=dd, x=\"human\", y=\"shapley\", order=o, hue_order=ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "aXGjcXbaXa-r",
   "metadata": {
    "executionInfo": {
     "elapsed": 371,
     "status": "ok",
     "timestamp": 1710873149359,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "aXGjcXbaXa-r"
   },
   "outputs": [],
   "source": [
    "# dd = df[df[\"category\"]==ho[2]]\n",
    "# plot = sns.violinplot(data=dd, x=\"human\", y=\"shapley\", order=o, hue_order=ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "s2io-lrjXbBj",
   "metadata": {
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1710865768590,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "s2io-lrjXbBj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mt7-C4ykXbE8",
   "metadata": {
    "id": "Mt7-C4ykXbE8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b2123-4393-4a0d-99ad-053aa728d4e6",
   "metadata": {
    "id": "440b2123-4393-4a0d-99ad-053aa728d4e6"
   },
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "# import pandas as pd\n",
    "\n",
    "# def cal_corr(shap_lexicons, weights, label_fn):\n",
    "#     df = []\n",
    "#     for label in shap_lexicons:\n",
    "#         count = defaultdict(int)\n",
    "#         for w in shap_lexicons[label]:\n",
    "#             if w not in lexicons:\n",
    "#                 continue\n",
    "\n",
    "#             for t in lexicons[w]:\n",
    "#                 count[t] += 1\n",
    "\n",
    "#         l = label_fn(label)\n",
    "#         A = []\n",
    "#         B = []\n",
    "#         for feat in count:\n",
    "#             k = (l, feat)\n",
    "#             if k in weights and weights[k]!=\"\":\n",
    "#                 A.append(weights[k])\n",
    "#                 B.append(count[feat])\n",
    "\n",
    "#         spm = stats.spearmanr(A, B)\n",
    "#         df.append({\n",
    "#             \"label\": label,\n",
    "#             \"N\": len(A),\n",
    "#             \"spearman\": spm.statistic,\n",
    "#             \"pvalue\": spm.pvalue\n",
    "#         })\n",
    "\n",
    "#     return pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe937c-da1f-4854-b4cb-057c5ea3e41a",
   "metadata": {
    "id": "a1fe937c-da1f-4854-b4cb-057c5ea3e41a"
   },
   "outputs": [],
   "source": [
    "# shap_lexicons = load_jsonl(\"Classifier/lexicon_task1_clse.jsonl\")[0]\n",
    "# weights = load_obj_values(\"linear_weights_task1_clse.pkl\")\n",
    "\n",
    "# cal_corr(shap_lexicons, weights, closeness_to_eng1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40ee15-75cd-462a-9f47-a10d61c9477d",
   "metadata": {
    "id": "4a40ee15-75cd-462a-9f47-a10d61c9477d"
   },
   "outputs": [],
   "source": [
    "# lexicon_paths = [\n",
    "#   \"Lexicons/v3/lexicon_task1_clse.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task2_clse.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task3_clse.jsonl\",\n",
    "\n",
    "#   \"Lexicons/v3/lexicon_task1_auth.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task2_auth.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task3_auth.jsonl\"\n",
    "# ]\n",
    "\n",
    "# texts = []\n",
    "# for path in lexicon_paths:\n",
    "#   lexicons, lexicons_by_feat = load_jsonl(path)\n",
    "#   print(path)\n",
    "\n",
    "\n",
    "#   N = 20\n",
    "#   count = {}\n",
    "#   # dict_keys(['pronoun', 'particles', 'misspelling'])\n",
    "#   for cat in lexicons_by_feat:\n",
    "#     for idx, w in enumerate(lexicons_by_feat[cat][\"pronoun\"]):\n",
    "#       if idx >= N:\n",
    "#         break\n",
    "\n",
    "#       if w in count:\n",
    "#         count[w].append(idx)\n",
    "#       else:\n",
    "#         count[w] = [idx]\n",
    "\n",
    "#   words = {\n",
    "#       (1, \"same\"):[],\n",
    "#       (1, \"div\"):[],\n",
    "#       (2, \"same\"):[],\n",
    "#       (2, \"div\"):[],\n",
    "#       (3, \"same\"):[],\n",
    "#       (3, \"div\"):[],\n",
    "#   }\n",
    "#   for w in count:\n",
    "#     n = len(count[w])\n",
    "\n",
    "#     if n==1:\n",
    "#       words[(n, \"same\")].append(w)\n",
    "#     elif n>=2:\n",
    "#       # print(w, count[w])\n",
    "#       mx = max(count[w])\n",
    "#       mn = min(count[w])\n",
    "\n",
    "#       if mx-mn > 5:\n",
    "#         words[(n, \"div\")].append((w, count[w]))\n",
    "#       else:\n",
    "#         words[(n, \"same\")].append((w, count[w]))\n",
    "\n",
    "#   for n in words:\n",
    "#     print(n, len(words[n]))\n",
    "#     print(words[n])\n",
    "#     print()\n",
    "#   print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "NRNAYi8yCv6d",
   "metadata": {
    "executionInfo": {
     "elapsed": 891,
     "status": "ok",
     "timestamp": 1710864357146,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "NRNAYi8yCv6d"
   },
   "outputs": [],
   "source": [
    "# !wget -q http://www.arts.chula.ac.th/ling/wp-content/uploads/TH-Sarabun_Chula1.1.zip -O font.zip\n",
    "# !unzip -qj font.zip TH-Sarabun_Chula1.1/THSarabunChula-Regular.ttf\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.font_manager.fontManager.addfont('THSarabunChula-Regular.ttf')\n",
    "matplotlib.rc('font', family='TH Sarabun Chula')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OnymKicLCyZg",
   "metadata": {
    "id": "OnymKicLCyZg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def quadrant_chart(x, y, xtick_labels=None, ytick_labels=None, data_labels=None,\n",
    " highlight_quadrants=None, ax=None):\n",
    "    \"\"\"\n",
    "    Create the classic four-quadrant chart.\n",
    "    Args:\n",
    "        x -- array-like, the x-coordinates to plot\n",
    "        y -- array-like, the y-coordinates to plot\n",
    "        xtick_labels -- list, default: None, a two-value list xtick labels\n",
    "        ytick_labels -- list, default: None, a two-value list of ytick labels\n",
    "        data_labels -- array-like, default: None, data point annotations\n",
    "        highlight_quadrants -- list, default: None, list of quadrants to\n",
    "            emphasize (quadrants are numbered 1-4)\n",
    "        ax -- matplotlib.axes object, default: None, the user can pass their own\n",
    "            axes object if desired\n",
    "    \"\"\"\n",
    "    # allow user to specify their own axes\n",
    "    ax = ax if ax else plt.axes()\n",
    "\n",
    "    data = pd.DataFrame({'x': x, 'y': y, 'data_labels': data_labels})\n",
    "\n",
    "    # calculate averages up front to avoid repeated calculations\n",
    "    y_avg = data['y'].mean()\n",
    "    x_avg = data['x'].mean()\n",
    "\n",
    "    # set x limits\n",
    "    adj_x = max((data['x'].max() - x_avg), (x_avg - data['x'].min())) * 1.1\n",
    "    lb_x, ub_x = (x_avg - adj_x, x_avg + adj_x)\n",
    "    ax.set_xlim(lb_x, ub_x)\n",
    "\n",
    "    # set y limits\n",
    "    adj_y = max((data['y'].max() - y_avg), (y_avg - data['y'].min())) * 1.1\n",
    "    lb_y, ub_y = (y_avg - adj_y, y_avg + adj_y)\n",
    "    ax.set_ylim(lb_y, ub_y)\n",
    "\n",
    "    # set x tick labels\n",
    "    if xtick_labels:\n",
    "        ax.set_xticks([(x_avg - adj_x / 2), (x_avg + adj_x / 2)])\n",
    "        ax.set_xticklabels(xtick_labels)\n",
    "\n",
    "    # set y tick labels\n",
    "    if ytick_labels:\n",
    "        ax.set_yticks([(y_avg - adj_y / 2), (y_avg + adj_y / 2)])\n",
    "        ax.set_yticklabels(ytick_labels, rotation='vertical', va='center')\n",
    "\n",
    "    # determine which points to highlight\n",
    "    if highlight_quadrants:\n",
    "        quadrants = []\n",
    "        for x_val, y_val in zip(x, y):\n",
    "            q = []\n",
    "            if (x_val >= x_avg) and (y_val >= y_avg):\n",
    "                q.append(1)\n",
    "            if (x_val <= x_avg) and (y_val >= y_avg):\n",
    "                q.append(2)\n",
    "            if (x_val <= x_avg) and (y_val <= y_avg):\n",
    "                q.append(3)\n",
    "            if (x_val >= x_avg) and (y_val <= y_avg):\n",
    "                q.append(4)\n",
    "            quadrants.append(q)\n",
    "        data['quadrant'] = quadrants\n",
    "\n",
    "        # boolean mask - True = highlight, False = don't highlight\n",
    "        highlight = data['quadrant'].apply(lambda q: len(set(\n",
    "        highlight_quadrants) & set(q)) > 0)\n",
    "\n",
    "        # plot the non-highlighted points within the conditional block\n",
    "        ax.scatter(data['x'][~highlight], data['y'][~highlight], alpha=0.5,\n",
    "        c='lightblue', edgecolor='darkblue', zorder=99)\n",
    "        data = data[highlight]\n",
    "\n",
    "    # plot remaining points and quadrant lines\n",
    "    ax.scatter(x=data['x'], y=data['y'], c='lightblue', edgecolor='darkblue',\n",
    "    zorder=99)\n",
    "    ax.axvline(x_avg, c='k', lw=1)\n",
    "    ax.axhline(y_avg, c='k', lw=1)\n",
    "\n",
    "    # add data labels\n",
    "    for ix, row in data.iterrows():\n",
    "        ax.annotate(row['data_labels'], (row['x'], row['y']), xytext=(2, 5),\n",
    "        textcoords='offset pixels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "UEbjwSTV05_A",
   "metadata": {
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1710864367078,
     "user": {
      "displayName": "Pakawat Nakwijit",
      "userId": "14261168557301921642"
     },
     "user_tz": 0
    },
    "id": "UEbjwSTV05_A"
   },
   "outputs": [],
   "source": [
    "# lexicon_paths = [\n",
    "#   \"Lexicons/v3/lexicon_task1_clse.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task2_clse.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task3_clse.jsonl\",\n",
    "\n",
    "#   \"Lexicons/v3/lexicon_task1_auth.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task2_auth.jsonl\",\n",
    "#   \"Lexicons/v3/lexicon_task3_auth.jsonl\"\n",
    "# ]\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# texts = []\n",
    "# for path in lexicon_paths:\n",
    "#   lexicons, lexicons_by_feat = load_jsonl(path)\n",
    "#   print(path)\n",
    "\n",
    "\n",
    "#   N = 50\n",
    "#   count = {}\n",
    "#   # dict_keys(['pronoun', 'particles', 'misspelling'])\n",
    "#   for cat in lexicons_by_feat:\n",
    "#     for idx, w in enumerate(lexicons_by_feat[cat][\"misspelling\"]):\n",
    "#       if idx >= N:\n",
    "#         break\n",
    "\n",
    "#       if w in count:\n",
    "#         count[w].append(idx)\n",
    "#       else:\n",
    "#         count[w] = [idx]\n",
    "\n",
    "#   words = []\n",
    "#   points = []\n",
    "\n",
    "#   for w in count:\n",
    "#     n = len(count[w])\n",
    "\n",
    "#     if n==1:\n",
    "#       x = count[w][0]\n",
    "#       y = 0\n",
    "#       continue\n",
    "#     elif n>=2:\n",
    "#       x = np.mean(count[w])\n",
    "#       y = np.std(count[w])\n",
    "#     points.append((x, y, w))\n",
    "\n",
    "\n",
    "\n",
    "#   # plt.rcParams.update({'font.size': 25})\n",
    "#   x = [p1 for p1, p2, _ in points]\n",
    "#   y = [p2 for p1, p2, _ in points]\n",
    "#   l = [p3 for p1, p2, p3 in points]\n",
    "\n",
    "#   quadrant_chart(\n",
    "#       x=x,\n",
    "#       y=y,\n",
    "#       xtick_labels=['Top', 'Bottom'],\n",
    "#       ytick_labels=['Low', 'High'],\n",
    "#       data_labels=l,\n",
    "#       # highlight_quadrants=[2]\n",
    "#   )\n",
    "#   plt.title('', fontsize=16)\n",
    "#   plt.ylabel('Variance', fontsize=14)\n",
    "#   plt.xlabel('Average Rank', fontsize=14)\n",
    "#   plt.show()\n",
    "#   plt.clf()\n",
    "#   # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egNo9WQF_c0Q",
   "metadata": {
    "id": "egNo9WQF_c0Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meZwciVh1BdN",
   "metadata": {
    "id": "meZwciVh1BdN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QwiKFYSc2MjH",
   "metadata": {
    "id": "QwiKFYSc2MjH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZpHbcQI1B8sQ",
   "metadata": {
    "id": "ZpHbcQI1B8sQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
