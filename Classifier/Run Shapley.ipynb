{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09fa70a1-14b2-426d-8f81-a56b1ec9e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_task1_conver, get_task2_conver, preprocess\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import shap\n",
    "\n",
    "from utils import dump_jsonl, load_jsonl, set_random_seed\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AdamW, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8c3c250c-fedf-4555-856e-a9193a669487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_shap_values(filepath):\n",
    "    with open(filepath, 'rb') as fin:\n",
    "        obj = pickle.load(fin)\n",
    "    return obj\n",
    "\n",
    "def save_shap_values(filepath, obj):\n",
    "    with open(filepath, 'wb') as fin:\n",
    "        pickle.dump(obj, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc1b9491-556f-47d2-b7dd-0609881b0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88456832-11b3-47d8-ae2e-f2b2b8b27fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = \"none\"\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58af1e07-c213-449d-94ec-c8b018ae49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierPredictor:\n",
    "    def __init__(self, model, tokenizer, device, max_length=256):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "    # define a prediction function\n",
    "    def predict(self, texts):\n",
    "        tokenizer = self.tokenizer\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "        max_length = self.max_length\n",
    "\n",
    "        text_ids = [tokenizer.encode(text, max_length=max_length, padding='max_length', truncation=True) for text in texts]\n",
    "\n",
    "        att_masks = []\n",
    "        for ids in text_ids:\n",
    "                masks = [int(id > 0) for id in ids]\n",
    "                att_masks.append(masks)\n",
    "\n",
    "        text_ids = torch.tensor(text_ids).to(device)\n",
    "        att_masks = torch.tensor(att_masks).to(device)\n",
    "\n",
    "        outputs = model(text_ids, attention_mask=att_masks)\n",
    "        outputs = outputs[0].detach().cpu().numpy()\n",
    "        scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "        \n",
    "        val = sp.special.logit(scores[:,1]) # use one vs rest logit units\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37853962-9dcb-4edb-8569-9504f35e6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressorPredictor:\n",
    "    def __init__(self, model, tokenizer, device, label_fn, max_length=256):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.label_fn = label_fn\n",
    "\n",
    "\n",
    "    # define a prediction function\n",
    "    def predict(self, texts):\n",
    "        tokenizer = self.tokenizer\n",
    "        model = self.model\n",
    "        device = self.device\n",
    "        max_length = self.max_length\n",
    "\n",
    "        text_ids = [tokenizer.encode(text, max_length=max_length, padding='max_length', truncation=True) for text in texts]\n",
    "\n",
    "        att_masks = []\n",
    "        for ids in text_ids:\n",
    "                masks = [int(id > 0) for id in ids]\n",
    "                att_masks.append(masks)\n",
    "#         print(len(texts))\n",
    "        text_ids = torch.tensor(text_ids).to(device)\n",
    "        att_masks = torch.tensor(att_masks).to(device)\n",
    "\n",
    "        outputs = model(text_ids, attention_mask=att_masks)\n",
    "        outputs = outputs[0].detach().cpu().numpy()\n",
    "#         print(outputs)\n",
    "#         print(text_ids.shape)\n",
    "#         print(\"OUTPUT\", outputs.shape)\n",
    "        return outputs.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc74c0-297a-465c-a357-bad53aa5de2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de6951-1b88-4ecd-89f1-2ad30e71b6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6f48f-754b-411d-af2d-d8c8db779d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ea832-8e95-4ffd-8b5d-972f0b34e726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f9533-a1b8-4c4f-b2ca-37a1f1a9faf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9de5b0a-f3c0-413e-9b8a-33dee4b6a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def get_shapley(model_path, df, regressor=False, regressor_label_fn=None):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    if regressor:\n",
    "        p = RegressorPredictor(model, tokenizer, device, regressor_label_fn, max_length=max_length)\n",
    "    else:\n",
    "        p = ClassifierPredictor(model, tokenizer, device, max_length=max_length)\n",
    "        \n",
    "    texts = [preprocess(t) for t in df[\"text\"].values]\n",
    "\n",
    "    d = {\"text\": texts}\n",
    "    explainer = shap.Explainer(p.predict, tokenizer)\n",
    "    shap_values = explainer(d, fixed_context=1, batch_size=batch_size)\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a15f63f3-fca4-4d20-ac0f-2447f69d0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # shap_values[0, 0, 0]\n",
    "# shap.plots.text(shap_values[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb427203-26bf-4f83-bebb-54401996338b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c835b08-a7fa-4a2f-b513-d92082114743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ae4ad-9e20-4dd2-8ebf-c851c1c90009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "67efd803-e384-4bf8-b02b-71b6dea81a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_added_toks = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"usr\", \"sys\", \"rep\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9275f11-c82d-4034-8dfd-73cfcdc34751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd4a9c-5671-4c8a-8e68-bc411c961c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a51a20-cdc6-41dc-a04d-482a828b3e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "99d3abc3-27f5-4856-9385-2269dae96c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _tmp = shap_values[:, :]\n",
    "# shap_data = _tmp.data\n",
    "# shap_values = _tmp.values\n",
    "\n",
    "def get_shap_lexicons(df, shap_values, fn_abs=True):\n",
    "    lexicons = {}\n",
    "    label_values = df[\"label\"].unique()\n",
    "    \n",
    "\n",
    "    for _, label in enumerate(label_values):\n",
    "        d = df[df[\"label\"]==label]\n",
    "        if fn_abs:\n",
    "            s = shap_values.abs[d.index.to_numpy()]\n",
    "        else:\n",
    "            s = shap_values[d.index.to_numpy()]\n",
    "\n",
    "        feature_names = s.mean(0).feature_names\n",
    "        shapley_values = s.mean(0).values\n",
    "\n",
    "        sorted_values = sorted(zip(shapley_values, feature_names), key=lambda pair: -pair[0])\n",
    "        lexicons[label] = {x.lower():v for v, x in sorted_values}\n",
    "        print(label, \"done\")\n",
    "    return lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f974a0ea-2f8d-4040-8fb4-c3ebaffc2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c01b23fe-e4d7-4a3d-9965-60861adfcd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "def merge_subwords(df, lexicons, tokenizer):\n",
    "    tf = defaultdict(int)\n",
    "    newlexicons = {}\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        label = row[\"label\"]\n",
    "        if label not in newlexicons:\n",
    "            newlexicons[label] = {}\n",
    "\n",
    "        words = word_tokenize(preprocess(row[\"text\"]))\n",
    "        words = [w for w in words if len(w.strip())>0]\n",
    "        \n",
    "        tokenized_input = tokenizer(words, is_split_into_words=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "        word_ids = tokenized_input.word_ids(batch_index=0)\n",
    "\n",
    "        tidx = 0\n",
    "        for widx, group in itertools.groupby(word_ids):\n",
    "            val = 0\n",
    "            for _ in list(group): \n",
    "                t = tokens[tidx].replace(\"▁\", \"\")\n",
    "                tidx += 1\n",
    "\n",
    "                if t in lexicons[label]:\n",
    "                    val += lexicons[label][t]\n",
    "\n",
    "            if widx is None:\n",
    "                continue\n",
    "\n",
    "            w = words[widx]\n",
    "            newlexicons[label][w] = val\n",
    "\n",
    "            tf[w] += 1\n",
    "    return newlexicons, tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd7abcc3-f4bb-4512-8442-0eb227c2b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def filter_lexicon(lexicons, tf, q=0.50):\n",
    "    values = []\n",
    "    for label in lexicons:\n",
    "        for word in lexicons[label]:\n",
    "            values.append(lexicons[label][word])\n",
    "\n",
    "\n",
    "    threshold = np.quantile(values, q)\n",
    "\n",
    "    filtered_lexicons = {}\n",
    "    cc = 0\n",
    "    for label in lexicons:\n",
    "        filtered_lexicons[label] = {}\n",
    "        for word in lexicons[label]:\n",
    "            val = lexicons[label][word]\n",
    "            # if val <= threshold:\n",
    "            #     continue\n",
    "            \n",
    "            if tf[word] < 5:\n",
    "                continue\n",
    "\n",
    "            filtered_lexicons[label][word] = val\n",
    "            cc += 1\n",
    "    print(\"TOTAL\", cc)\n",
    "    return filtered_lexicons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6fe6f73-468c-4e5e-bec1-e3d27e668d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicons(df, model_path, shap_path, lex_path, regressor=False):\n",
    "    train, val, test = df\n",
    "    train[\"split\"] = \"train\"\n",
    "    val[\"split\"] = \"val\"\n",
    "    test[\"split\"] = \"test\"\n",
    "\n",
    "    df = pd.concat([train, test, val])\n",
    "    shap_values = get_shapley(model_path, df, regressor=regressor)\n",
    "    \n",
    "#     shap_values = load_shap_values(shap_path)\n",
    "#     assert(len(df)==len(shap_values))\n",
    "    save_shap_values(shap_path, shap_values)\n",
    "    \n",
    "    lexicons = get_shap_lexicons(df, shap_values)\n",
    "    newlexicons, tf = merge_subwords(df, lexicons, tokenizer)\n",
    "    newlexicons = filter_lexicon(newlexicons, tf)\n",
    "    dump_jsonl(lex_path, [newlexicons])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd83ad-2bdf-465b-8ff0-622b00c6a296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3317a-ac80-406d-87d2-7d54adea86ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7cc6d-a0ba-44c6-aa69-e045478914a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf904cc-d516-45d9-a9d9-379967b8bc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2340bde3-45ab-4012-a10c-e194e5ddd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "# get_lexicons(df, f'./Models/task1_clse_usr/best_model', f\"./ShapleyValues/task1_clse.pkl\", \"lexicon_task1_clse.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d864762-d937-454c-8690-57d821bec39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "get_lexicons(df, f'./Regressors/task1_clse_usr/best_model', f\"./ShapleyValues/task1_clse_regressor.pkl\", \"lexicon_task1_clse.jsonl\", regressor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fdb16cc-0771-481b-a6cd-6cad309e4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"authority\", skips = [\"3. Not respect\"], only_user=True)\n",
    "# get_lexicons(df, f'./Models/task1_auth_usr/best_model', f\"./ShapleyValues/task1_auth.pkl\", \"lexicon_task1_auth.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0cd0daf3-88a8-4d2a-b2f3-b493430ed105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"authority\", skips = [\"3. Not respect\"], only_user=True)\n",
    "get_lexicons(df, f'./Regressors/task1_auth_usr/best_model', f\"./ShapleyValues/task1_auth_regressor.pkl\", \"lexicon_task1_auth.jsonl\", regressor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc788236-bbb7-4f33-bc88-d0599a3a5735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ee10ea6-42e4-4557-abd4-493a9c71257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "# get_lexicons(df, f'./Models/task2_clse_usr/best_model', f\"./ShapleyValues/task2_clse.pkl\", \"lexicon_task2_clse.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "853ec56e-fde6-421c-ba24-8be3270aa85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "get_lexicons(df, f'./Regressors/task2_clse_usr/best_model', f\"./ShapleyValues/task2_clse_regressor.pkl\", \"lexicon_task2_clse.jsonl\", regressor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c69e2ab-7a57-4ef5-8ffe-5fd43bad1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "# get_lexicons(df, f'./Models/task2_auth_usr/best_model', f\"./ShapleyValues/task2_auth.pkl\", \"lexicon_task2_auth.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "724a7eab-80ab-4401-be59-a2104ac6719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "get_lexicons(df, f'./Regressors/task2_auth_usr/best_model', f\"./ShapleyValues/task2_auth_regressor.pkl\", \"lexicon_task2_auth.jsonl\", regressor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f0278-77bb-4313-9dc0-04dc4c82e080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c10cb-1a7a-4a24-99a3-545259fedbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f666fe50-674f-426b-b7f3-f0cad8aeb891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "# get_lexicons(df, f'./Models/task3_clse_usr/best_model', f\"./ShapleyValues/task3_clse.pkl\", \"lexicon_task3_clse.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b90d94da-7626-4df9-923e-c2df85b677dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "get_lexicons(df, f'./Regressors/task3_clse_usr/best_model', f\"./ShapleyValues/task3_clse_regressor.pkl\", \"lexicon_task3_clse.jsonl\", regressor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94b1a8c4-4b89-42e1-aa5c-cf9aa391f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "# get_lexicons(df, f'./Models/task3_auth_usr/best_model', f\"./ShapleyValues/task3_auth.pkl\", \"lexicon_task3_auth.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0c588d60-040c-401d-996d-042731af943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "get_lexicons(df, f'./Regressors/task3_auth_usr/best_model', f\"./ShapleyValues/task3_auth_regressor.pkl\", \"lexicon_task3_auth.jsonl\", regressor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330a82d-6769-44c7-b4e1-f37b9dd7c808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a607db-8fef-414a-8509-77c06ac69e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
