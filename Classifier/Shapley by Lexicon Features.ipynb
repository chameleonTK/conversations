{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826189f6-f662-4078-b677-686f7866488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_task1_conver, get_task2_conver\n",
    "from utils import dump_jsonl, load_jsonl\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_shap_values(filepath):\n",
    "  with open(filepath, 'rb') as fin:\n",
    "    obj = pickle.load(fin)\n",
    "  return obj\n",
    "\n",
    "def save_shap_values(filepath, obj):\n",
    "  with open(filepath, 'wb') as fin:\n",
    "    pickle.dump(obj, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae1169a-3167-4ba9-b958-f9135805f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666cb3a3-2ec9-4842-870a-b89b96f75e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_added_toks = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"usr\", \"sys\", \"rep\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edb0a8-9802-47dc-bed9-b2189b1fe41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7cec2-f5eb-4a72-9cb0-b6d88e97707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53dd5afe-b3b9-484e-8628-c9d0c60c3122",
   "metadata": {},
   "source": [
    "## Load Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f52407bf-66ad-4a78-b7ff-cecd72d64d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345c8438-4810-453d-8e69-a7c2504e5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../words.json\", encoding=\"utf-8\") as fin:\n",
    "    raw = json.load(fin)\n",
    "    thaidict_royal = set()\n",
    "    for k in raw:\n",
    "        thaidict_royal.update(raw[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07775b4b-a694-471c-9e00-07157af10be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25573 records from ../lexicons.jsonl\n"
     ]
    }
   ],
   "source": [
    "lexicons_arr = load_jsonl(\"../lexicons.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99344938-fa6d-48cf-97e2-04aa8353adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "tags = set()\n",
    "lexicons = {}\n",
    "lexicons_keys = defaultdict(list)\n",
    "\n",
    "for key, values  in lexicons_arr:\n",
    "    if len(key) <= 1:\n",
    "        continue\n",
    "        \n",
    "    key = key.lower()\n",
    "    if key.endswith(\"rep\"):\n",
    "        key = key.replace(\"rep\", \"\")\n",
    "        \n",
    "    w = word_tokenize(key)\n",
    "    \n",
    "    lexicons_keys[w[0]].append(key)\n",
    "    \n",
    "    tag = [t for t in values[\"tags\"] if not t.startswith(\"cat:\")]\n",
    "    lexicons[key] = tag\n",
    "    tags.update(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad5b26e3-65f2-4adf-8901-dd81f83d7dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc = 0\n",
    "for k in lexicons:\n",
    "    for t in lexicons[k]:\n",
    "        if t==\"transliterated\":\n",
    "            cc += 1\n",
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25524732-2d62-4a46-b49a-e7dded1affbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74004a85-74b4-4e91-82a9-e71d60abb0aa",
   "metadata": {},
   "source": [
    "## Calculate Shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd74d9f2-c5cc-4c07-9b55-618539ecca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = {\n",
    "    \"Reference\" : {\n",
    "#         \"all\": \"All words\",\n",
    "        \"pertoken\": \"Average per token\"\n",
    "    },\n",
    "    \"Linguistic Complexity\" : {\n",
    "#         \"nunique\": \"Vocabulary size\",\n",
    "        \"nthai\": \"Thai words\",\n",
    "        \"nnotthai\": \"Non-Thai words\",\n",
    "        \"nlongword\": \"Long words\",\n",
    "        \"ndict\": \"Dictionary words\",\n",
    "        \"transliterated\": \"Transliteration\",\n",
    "    },\n",
    "    \"Pronoun\": {\n",
    "        \"pronoun\": \"All pronoun\",\n",
    "        \"pronoun_1st\": \">> 1st person pronoun\",\n",
    "        \"pronoun_2nd\": \">> 2nd person pronoun\",\n",
    "        \"pronoun_3rd\": \">> 3rd person pronoun\",\n",
    "        \"pronoun_misspelling\": \">> Pronoun in non-standard spelling\",\n",
    "    },\n",
    "    \n",
    "    \"Sentence-ending Particles\": {\n",
    "        \"particles\": \"All particles\",\n",
    "        \"particles_SARP\": \">> Socially-related particles\",\n",
    "        \"particles_notSARP\": \">> Non-socially-related particles\",\n",
    "        \"particles_misspelling\": \">> Particle in non-standard spelling\",\n",
    "    },\n",
    "    \n",
    "    \"Sentiment-related\": {\n",
    "        \"sentiment\": \"Sentiment words\",\n",
    "        \"sentiment_positive\": \">> Positive words\",\n",
    "        \"sentiment_negative\": \">> Negative words\",\n",
    "    },\n",
    "    \n",
    "    \"Spelling Variation\": {\n",
    "        \"misspelling\": \"All spelling variation\",\n",
    "        \"misspelling_common\": \">> Common misspelt words\",\n",
    "        \"misspelling_intention\": \">> Morphophonemic variation\",\n",
    "        \"misspelling_shorten\": \">> Simplified variation\",\n",
    "        \"nrepeat\": \">> Repeated characters\",\n",
    "        \"nemoji\": \">> Emoji\",\n",
    "#         \"abbr\": \"Abbreviation\",\n",
    "#         \"slang\": \"Slang\",\n",
    "#         \"swear\": \"Swear words\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2702eead-4dbf-41bc-80a5-d59c87993d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_token_2_words(words, shap_tokens, debug=False):\n",
    "    tokens = [w for w, _ in  shap_tokens]\n",
    "    values = np.array([v for _, v in  shap_tokens])\n",
    "    \n",
    "    idxs = []\n",
    "    sidx = 0\n",
    "    windows = 10\n",
    "        \n",
    "    newwords = []\n",
    "    newtokens = []\n",
    "    newvalues = []\n",
    "    \n",
    "    \n",
    "    widx = 0\n",
    "    w = \"\"\n",
    "    while widx < len(words):\n",
    "        w += words[widx]\n",
    "        if sidx >= len(tokens):\n",
    "#             print(newwords)\n",
    "#             assert(False)\n",
    "            break\n",
    "            \n",
    "#         print(widx, w, sidx, tokens[sidx])\n",
    "#         break\n",
    "        s = \"\"\n",
    "        matched = False\n",
    "        for tidx in range(sidx, min(sidx+windows, len(tokens))):\n",
    "            s += tokens[tidx]\n",
    "            if s==w:\n",
    "                matched = True\n",
    "                break\n",
    "                \n",
    "        if matched:\n",
    "            if debug:\n",
    "                print(\"MATCHED\", w)\n",
    "            idxs.append([sidx, tidx+1])\n",
    "            newwords.append(w)\n",
    "            newtokens.append(\"\".join(tokens[sidx:tidx+1]))\n",
    "            newvalues.append(values[sidx:tidx+1].sum())\n",
    "            sidx = tidx+1\n",
    "            w = \"\"\n",
    "            widx += 1\n",
    "            continue\n",
    "        \n",
    "        if debug:\n",
    "            print(\"NOT MATCHED\", w, s)\n",
    "            \n",
    "        if not s.startswith(w):\n",
    "            sidx += 1\n",
    "            w = \"\"\n",
    "#             print(\"SKIP TOKEN\")\n",
    "            continue\n",
    "        else:                \n",
    "            widx += 1\n",
    "#             print(\"MERGE WORDS\")\n",
    "            continue\n",
    "            \n",
    "    if debug:    \n",
    "        print(newwords)\n",
    "        print(newtokens)\n",
    "    \n",
    "    \n",
    "    return newtokens, newvalues\n",
    "\n",
    "def get_shap_lexicons(df, raw_shap_values):\n",
    "    shap_lexicons = {}\n",
    "    label_values = df[\"label\"].unique()\n",
    "    \n",
    "\n",
    "    _tmp = raw_shap_values[:, :]\n",
    "    shap_data = _tmp.data\n",
    "    shap_values = _tmp.values\n",
    "\n",
    "    for _, label in enumerate(label_values):\n",
    "        feats = []\n",
    "        for idx, row in df.iterrows():\n",
    "            if row[\"label\"]!=label:\n",
    "                continue\n",
    "            \n",
    "            text = row[\"text\"]\n",
    "            words = word_tokenize(preprocess(row[\"text\"]))\n",
    "            words = [w.strip() for w in words if len(w.strip())>0]\n",
    "            \n",
    "            shap_tokens = [(w.strip(), v) for w,v in zip(shap_data[idx], shap_values[idx]) if len(w.strip())>0]\n",
    "            shap_tokens = map_token_2_words(words, shap_tokens, debug=False)\n",
    "            feats.append(shap_tokens)\n",
    "        shap_lexicons[label] = feats\n",
    "    return shap_lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d5717b6-9cf1-4ee6-bf34-71a650055789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util import countthai\n",
    "import re \n",
    "import emoji\n",
    "\n",
    "def get_lexicon_feats(token, ref_text):\n",
    "    feats = [\"all\"]\n",
    "    \n",
    "    if token==\"rep\":\n",
    "        feats.append(\"nrepeat\")\n",
    "        \n",
    "    if token in lexicons_keys:\n",
    "        for l in lexicons_keys[token]:\n",
    "            if not ref_text.startswith(l):\n",
    "                continue\n",
    "\n",
    "            feats.extend(lexicons[l])\n",
    "            \n",
    "    if token in thaidict_royal:\n",
    "        feats.append(\"ndict\")\n",
    "    \n",
    "    if len(token) > 7:\n",
    "        feats.append(\"nlongword\")\n",
    "    \n",
    "    if countthai(token) < 50:\n",
    "        nt = re.sub(r'\\W+', '', token)\n",
    "        if token not in [\"usr\", \"sys\", \"rep\"] and len(nt) > 0 and not nt.isnumeric():\n",
    "            feats.append(\"nnotthai\")\n",
    "    else:\n",
    "        feats.append(\"nthai\")\n",
    "    \n",
    "    if \"particles\" in feats and \"particles_SARP\" not in feats:\n",
    "        feats.append(\"particles_notSARP\")\n",
    "    \n",
    "    if emoji.emoji_count(token) > 0:\n",
    "        feats.append(\"nemoji\")\n",
    "    \n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22de286d-e8d1-4402-aa6d-1f17f66ff010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_shap_feats(shap_lexicons):\n",
    "    output = {}\n",
    "    for label in shap_lexicons:\n",
    "        all_shap_feats = []\n",
    "        for tokens, values in shap_lexicons[label]:\n",
    "            shap_feats = defaultdict(list)\n",
    "            for tidx, (t, v) in enumerate(zip(tokens, values)):\n",
    "                feats = get_lexicon_feats(t, \"\".join(tokens[tidx:]))\n",
    "                    \n",
    "                for f in feats:\n",
    "                    shap_feats[f].append(v)\n",
    "            \n",
    "            shap_feats[\"pertoken\"] = values\n",
    "            all_shap_feats.append(shap_feats)\n",
    "            \n",
    "        mean_shap_feats = {}\n",
    "        for g in metric_names:\n",
    "            for m in metric_names[g]:\n",
    "                values = []\n",
    "                for feats in all_shap_feats:\n",
    "                    if m in feats:\n",
    "                        absum = np.sum(np.abs(np.array(feats[m])))\n",
    "                        values += [absum]\n",
    "                        #values.append(feats[m])\n",
    "                \n",
    "                if len(values)==0:\n",
    "                    mean_shap_feats[m] = (0, 0)\n",
    "                    continue\n",
    "                    \n",
    "                rms = np.mean(np.array(values))\n",
    "                mean_shap_feats[m] = (rms, len(values))\n",
    "        \n",
    "        output[label] = mean_shap_feats\n",
    "    return output\n",
    "\n",
    "def get_shap_feats(shap_lexicons):\n",
    "    output = {}\n",
    "    for label in shap_lexicons:\n",
    "        all_shap_feats = []\n",
    "        for tokens, values in shap_lexicons[label]:\n",
    "            # shap_feats is per conversation\n",
    "            shap_feats = defaultdict(list)\n",
    "            for tidx, (t, v) in enumerate(zip(tokens, values)):\n",
    "                feats = get_lexicon_feats(t, \"\".join(tokens[tidx:]))\n",
    "                    \n",
    "                for f in feats:\n",
    "                    shap_feats[f].append(v)\n",
    "            \n",
    "#             shap_feats[\"pertoken\"] = sum(values)/len(values)\n",
    "            shap_feats[\"pertoken\"] = values\n",
    "            all_shap_feats.append(shap_feats)\n",
    "            \n",
    "        mean_shap_feats = {}\n",
    "        for g in metric_names:\n",
    "            for m in metric_names[g]:\n",
    "                values = []\n",
    "                for feats in all_shap_feats:\n",
    "                    if m in feats:\n",
    "                        values += feats[m]\n",
    "                \n",
    "                if len(values)==0:\n",
    "                    mean_shap_feats[m] = (0, 0)\n",
    "                    continue\n",
    "                    \n",
    "                values = np.array(values)\n",
    "                rms = np.sqrt(np.mean(values**2))\n",
    "                mean_shap_feats[m] = (rms, len(values))\n",
    "        \n",
    "        output[label] = mean_shap_feats\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bff40ac-916b-4112-8515-f9fc1e074186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38d92ffa-4e27-4dc7-86ff-bdf87a52907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from data_loader import preprocess\n",
    "\n",
    "def run_preprocess(train, val, test):\n",
    "    train[\"text\"] = train[\"text\"].apply(preprocess)\n",
    "    val[\"text\"] = val[\"text\"].apply(preprocess)\n",
    "    test[\"text\"] = test[\"text\"].apply(preprocess)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4247e-6a21-4fdf-ae2d-8bed8674f10e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24a91f89-942a-4214-9c3d-b24e70d17e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lexicons(df, shap_path):\n",
    "    train, val, test = df\n",
    "    train[\"split\"] = \"train\"\n",
    "    val[\"split\"] = \"val\"\n",
    "    test[\"split\"] = \"test\"\n",
    "\n",
    "    df = pd.concat([train, test, val])\n",
    "    shap_values = load_shap_values(shap_path)\n",
    "\n",
    "    assert(len(df)==len(shap_values))\n",
    "    \n",
    "    shap_lexicons = get_shap_lexicons(df, shap_values)\n",
    "    shap_feats = get_shap_feats(shap_lexicons)\n",
    "    all_shap_feats = get_all_shap_feats(shap_lexicons)\n",
    "    \n",
    "    return shap_feats, all_shap_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456e12c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc99fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0bd5f35-d78d-46fd-98bd-1e761348178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1234 records from ../Task1/annotated_conersations.jsonl\n",
      "N 1096 60 60\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats1 = run_lexicons(df, f\"./ShapleyValues/task1_clse_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2fbf5fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53bdfd3a-ef93-4617-9adf-4e3ed2abc961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1234 records from ../Task1/annotated_conersations.jsonl\n",
      "N 1098 61 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"authority\", skips = [\"3. Not respect\"], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats2 = run_lexicons(df, f\"./ShapleyValues/task1_auth_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23316172-1250-4382-bc8f-536304b03837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55770a4f-723b-4d05-8a75-6a25c2cf0586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f57ea61c-16e1-44ee-b03e-91799e04d4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2486 records from ../Task2/annotated/annotated.jsonl\n",
      "N 1495 186 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "df = df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats3 = run_lexicons(df, f\"./ShapleyValues/task2_clse_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4a934c67-2576-4279-afb7-94922d146e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2486 records from ../Task2/annotated/annotated.jsonl\n",
      "N 1876 234 234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats4 = run_lexicons(df, f\"./ShapleyValues/task2_auth_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6f715-29e6-466d-bf9e-53fea698d56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2103cd4-4000-4215-bb1c-4893aad27503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ac61f618-6857-4a33-8263-33a5bb4482d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ../Task3/annotated/annotated.jsonl\n",
      "N 1090 60 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats5 = run_lexicons(df, f\"./ShapleyValues/task3_clse_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51caf043-eef6-4c61-8569-d75710699aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ../Task3/annotated/annotated.jsonl\n",
      "N 1099 61 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats6 = run_lexicons(df, f\"./ShapleyValues/task3_auth_regressor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a2b71-6e9f-429b-a573-23a68120b2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1f454-7223-4697-8333-40ea073ad662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197af2d-bf31-412a-84db-00e7ff675496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0d775-a0f4-450f-807b-09faf6b9e9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ad025-fe9b-47e8-b42e-9b2d310643de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5bf3225b-68a6-4507-9c72-f31aa090c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shap(shap_feats, print_labels):\n",
    "    print_text = \"\"\n",
    "    for g in metric_names:\n",
    "        if g!=\"Reference\":\n",
    "            print_text += (\"\\multicolumn{5}{l}{\\\\textit{\"+g+\"}} \\\\\\\\\") +\"\\n\"\n",
    "            \n",
    "        for m in metric_names[g]:\n",
    "            s = f\"{metric_names[g][m]} \"\n",
    "            for l in print_labels:\n",
    "                if l not in shap_feats:\n",
    "                    s += f\"& - \"\n",
    "                elif m not in shap_feats[l]:\n",
    "                    s += f\"& 0.00 \"\n",
    "                else:\n",
    "                    s += f\"& {shap_feats[l][m]*100:.3f} \"\n",
    "\n",
    "            s += \"\\\\\\\\\"\n",
    "            print_text += (s)+\"\\n\"\n",
    "        \n",
    "        if g!=\"Reference\":\n",
    "            print_text += (\"&  & &  & \\\\\\\\\") +\"\\n\"\n",
    "        print_text += (\"\\hline\")+\"\\n\"\n",
    "    return print_text\n",
    "\n",
    "clse_print_labels = ['1. Close', '2. Know each other', \"3. Don't know each other\", \"4. Don't like each other\"]\n",
    "auth_print_labels = ['0. Very respect', '1. Respect', '2. Normal', '3. Not respect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a19ebad8-36dd-4331-b924-18ed850562f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sections = [\n",
    "#     \"Setting 1: Private Conversations with Self-Reported Labels\",\n",
    "#     \"Setting 2: Public Conversations with Labels from 3rd Party \",\n",
    "#     \"Setting 3: Private Conversations with Labels from 3rd Party \",\n",
    "# ]\n",
    "\n",
    "# table_contents = [\n",
    "#     (shap_feats1, shap_feats2),\n",
    "#     (shap_feats3, shap_feats4),\n",
    "#     (shap_feats5, shap_feats6),\n",
    "# ]\n",
    "\n",
    "# printed_text = \"\"\n",
    "# for section, (t1, t2) in zip(sections, table_contents):\n",
    "    \n",
    "#     printed_text += \"\\subsection{\"+section+\"}\"\n",
    "    \n",
    "#     printed_text += '''\n",
    "# \\subsubsection{Closeness}\n",
    "# \\\\begin{longtable}[h]{\n",
    "#         p{\\dimexpr 0.40\\linewidth-2\\\\tabcolsep}|\n",
    "#         p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "#         p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "#         p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "#         p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "#     }\n",
    "#         \\hline\n",
    "\n",
    "#         Lexical Features & Close & Know each other & Don't know each other &  Don't like each other\\\\\\\\\n",
    "#         \\hline\n",
    "#         \\endfirsthead\n",
    "        \n",
    "#         \\endhead\n",
    "#             '''\n",
    "    \n",
    "#     s = print_shap(t1, clse_print_labels)\n",
    "#     printed_text += \"\\n            \".join(s.split(\"\\n\"))\n",
    "#     printed_text += '''\n",
    "# \\end{longtable}\n",
    "# \\clearpage\n",
    "\n",
    "# '''\n",
    "#     printed_text += '''\n",
    "# \\subsubsection{Respect}\n",
    "# \\\\begin{longtable}[h]{\n",
    "#         p{\\dimexpr 0.40\\linewidth-2\\\\tabcolsep}|\n",
    "#         p{\\dimexpr 0.16\\linewidth-2\\\\tabcolsep}\n",
    "#         p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "#         p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "#         p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "#     }\n",
    "#         \\hline\n",
    "\n",
    "#         Lexical Features & Very respect & Respect & Normal &  Not respect\\\\\\\\\n",
    "#         \\hline\n",
    "#         \\endfirsthead\n",
    "        \n",
    "#         \\endhead\n",
    "#             '''\n",
    "    \n",
    "#     s = print_shap(t2, auth_print_labels)\n",
    "#     printed_text += \"\\n            \".join(s.split(\"\\n\"))\n",
    "#     printed_text += '''\n",
    "# \\end{longtable}\n",
    "# \\clearpage\n",
    "\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0788d23-625f-4e05-9f29-23c3aad4a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(printed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8f48647-ce76-4955-9dc0-6c376dac2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"particles\": \"All particles\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b53fcfb6-0782-4111-9441-b7af0893c875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DONE'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508f48c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf908482",
   "metadata": {},
   "source": [
    "#### Print version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f3318fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputs_to_dict(outputs):\n",
    "    coefs = {}\n",
    "    for s in outputs[0]:\n",
    "        for feat in outputs[0][s]:\n",
    "            if feat not in coefs:\n",
    "                coefs[feat] = (0, 0)\n",
    "            \n",
    "            val1, n1 = outputs[0][s][feat]\n",
    "            val2, n2 = coefs[feat]\n",
    "            \n",
    "            if n1+n2 == 0:\n",
    "                coefs[feat] = (0, 0)\n",
    "            else:\n",
    "                val = (val1*n1 + val2*n2)*1.0/(n1+n2)\n",
    "                n = n1 + n2\n",
    "                coefs[feat] = (val, n)\n",
    "                \n",
    "    all_coefs = {}\n",
    "    for s in outputs[1]:\n",
    "        for feat in outputs[1][s]:\n",
    "            if feat not in all_coefs:\n",
    "                all_coefs[feat] = (0, 0)\n",
    "            \n",
    "            val1, n1 = outputs[1][s][feat]\n",
    "            val2, n2 = all_coefs[feat]\n",
    "            \n",
    "            if n1+n2 == 0:\n",
    "                all_coefs[feat] = (0, 0)\n",
    "            else:\n",
    "                val = (val1*n1 + val2*n2)*1.0/(n1+n2)\n",
    "                n = n1 + n2\n",
    "                all_coefs[feat] = (val, n)\n",
    "                \n",
    "    return coefs, all_coefs\n",
    "\n",
    "printed_text = \"\" \n",
    "printed_text += \"\\subsection{Closeness}\"+\"\\n\"\n",
    "outputs = [\n",
    "    outputs_to_dict(shap_feats1),\n",
    "    outputs_to_dict(shap_feats3),\n",
    "    outputs_to_dict(shap_feats5)\n",
    "]\n",
    "\n",
    "printed_text += \"\\\\begin{longtable}[h]{\"+\"\\n\"\n",
    "printed_text += \"    p{\\dimexpr 0.40\\linewidth-2\\tabcolsep}|c|c|c|c|c|c|\"+\"\\n\"\n",
    "printed_text += \"}\"+\"\\n\"\n",
    "printed_text += \"    \\hline\"+\"\\n\"\n",
    "printed_text += \"    Lexical Features & \" + \"\\n\"\n",
    "printed_text += \"    \\\\multicolumn{2}{|c|}{Setting 1} & \" + \"\\n\"\n",
    "printed_text += \"    \\\\multicolumn{2}{|c|}{Setting 2} & \" + \"\\n\"\n",
    "printed_text += \"    \\\\multicolumn{2}{|c|}{Setting 3} \\\\\\\\\" + \"\\n\"\n",
    "printed_text += \"    \\\\cline{2-7}\" + \"\\n\"\n",
    "printed_text += \"    & \" + \"\\n\"\n",
    "printed_text += \"    Per \\\\newline token & Total & \" + \"\\n\"\n",
    "printed_text += \"    Per \\\\newline token & Total & \" + \"\\n\"\n",
    "printed_text += \"    Per \\\\newline token & Total \\\\\\\\\" + \"\\n\"\n",
    "\n",
    "printed_text += \"    \\hline\"+\"\\n\"\n",
    "#     printed_text += \"    \\endfirsthead\"+\"\\n\"\n",
    "#     printed_text += \"\"+\"\\n\"\n",
    "printed_text += \"    \\endhead\"+\"\\n\"\n",
    "printed_text += \"\"+\"\\n\"\n",
    "\n",
    "# for sec, results in zip(sections, outputs):\n",
    "for g in metric_names:\n",
    "    if g in [\"Conversational Statistics\"]:\n",
    "        continue\n",
    "\n",
    "    printed_text += \"    \\multicolumn{7}{l}{\\\\textit{\"+g+\"}} \\\\\\\\\"+\"\\n\"\n",
    "    printed_text += \"    \\hline\"+\"\\n\"\n",
    "\n",
    "    for m in metric_names[g]:\n",
    "        s = f\"        {metric_names[g][m]} \"\n",
    "        for out, all_out in outputs:    \n",
    "            if m not in out:\n",
    "                s += f\"& - \"\n",
    "            else:\n",
    "                val, n = out[m]\n",
    "                all_val, _ = all_out[m]\n",
    "                ref, _ = out[\"pertoken\"]\n",
    "                \n",
    "                if (val - ref)*100 > 0.1:\n",
    "                    s += \"& \\cellcolor{gray!25} \"+f\"{val*100:.2f}\"+\" & \\cellcolor{gray!25} \"+f\"{all_val*100:.2f}\"\n",
    "                else:\n",
    "                    s += f\"& {val*100:.2f} & {all_val*100:.2f}\"\n",
    "                \n",
    "        s += \"\\\\\\\\\"\n",
    "        # print(s)\n",
    "        printed_text += s+\"\\n\"\n",
    "\n",
    "\n",
    "    printed_text += \"        & & & & & &\\\\\\\\\"+\"\\n\"\n",
    "    printed_text += \"    \\hline\"+\"\\n\"\n",
    "    printed_text += \"\"+\"\\n\"\n",
    "    \n",
    "printed_text += \"\\label{closeness_wangchanberta_shapley_value}\"+\"\\n\"\n",
    "printed_text += \"\\end{longtable}\"+\"\\n\"\n",
    "printed_text += \"\\clearpage\"+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a4145b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "printed_text += \"\\subsection{Respect}\"+\"\\n\"\n",
    "outputs = [\n",
    "    outputs_to_dict(shap_feats2),\n",
    "    outputs_to_dict(shap_feats4),\n",
    "    outputs_to_dict(shap_feats6)\n",
    "]\n",
    "\n",
    "printed_text += \"\\\\begin{longtable}[h]{\"+\"\\n\"\n",
    "printed_text += \"    p{\\dimexpr 0.40\\linewidth-2\\tabcolsep}|c|c|c|c|c|c|\"+\"\\n\"\n",
    "printed_text += \"}\"+\"\\n\"\n",
    "printed_text += \"    \\hline\"+\"\\n\"\n",
    "printed_text += \"    Lexical Features & \" + \"\\n\"\n",
    "printed_text += \"    \\\\multicolumn{2}{|c|}{Setting 1} & \" + \"\\n\"\n",
    "printed_text += \"    \\\\multicolumn{2}{|c|}{Setting 2} & \" + \"\\n\"\n",
    "printed_text += \"    \\\\multicolumn{2}{|c|}{Setting 3} \\\\\\\\\" + \"\\n\"\n",
    "printed_text += \"    \\\\cline{2-7}\" + \"\\n\"\n",
    "printed_text += \"    & \" + \"\\n\"\n",
    "printed_text += \"    Per \\\\newline token & Total & \" + \"\\n\"\n",
    "printed_text += \"    Per \\\\newline token & Total & \" + \"\\n\"\n",
    "printed_text += \"    Per \\\\newline token & Total \\\\\\\\\" + \"\\n\"\n",
    "\n",
    "printed_text += \"    \\hline\"+\"\\n\"\n",
    "#     printed_text += \"    \\endfirsthead\"+\"\\n\"\n",
    "#     printed_text += \"\"+\"\\n\"\n",
    "printed_text += \"    \\endhead\"+\"\\n\"\n",
    "printed_text += \"\"+\"\\n\"\n",
    "\n",
    "# for sec, results in zip(sections, outputs):\n",
    "for g in metric_names:\n",
    "    if g in [\"Conversational Statistics\"]:\n",
    "        continue\n",
    "\n",
    "    printed_text += \"    \\multicolumn{7}{l}{\\\\textit{\"+g+\"}} \\\\\\\\\"+\"\\n\"\n",
    "    printed_text += \"    \\hline\"+\"\\n\"\n",
    "\n",
    "    for m in metric_names[g]:\n",
    "        s = f\"        {metric_names[g][m]} \"\n",
    "        for out, all_out in outputs:    \n",
    "            if m not in out:\n",
    "                s += f\"& - \"\n",
    "            else:\n",
    "                val, n = out[m]\n",
    "                all_val, _ = all_out[m]\n",
    "                ref, _ = out[\"pertoken\"]\n",
    "                \n",
    "                if (val - ref)*100 > 0.1:\n",
    "                    s += \"& \\cellcolor{gray!25} \"+f\"{val*100:.2f}\"+\" & \\cellcolor{gray!25} \"+f\"{all_val*100:.2f}\"\n",
    "                else:\n",
    "                    s += f\"& {val*100:.2f} & {all_val*100:.2f}\"\n",
    "                \n",
    "        s += \"\\\\\\\\\"\n",
    "        # print(s)\n",
    "        printed_text += s+\"\\n\"\n",
    "\n",
    "\n",
    "    printed_text += \"        & & & & & &\\\\\\\\\"+\"\\n\"\n",
    "    printed_text += \"    \\hline\"+\"\\n\"\n",
    "    printed_text += \"\"+\"\\n\"\n",
    "printed_text += \"\\label{respect_wangchanberta_shapley_value}\"+\"\\n\"\n",
    "printed_text += \"\\end{longtable}\"+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b0cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1a766d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\subsection{Closeness}\n",
      "\\begin{longtable}[h]{\n",
      "    p{\\dimexpr 0.40\\linewidth-2\tabcolsep}|c|c|c|c|c|c|\n",
      "}\n",
      "    \\hline\n",
      "    Lexical Features & \n",
      "    \\multicolumn{2}{|c|}{Setting 1} & \n",
      "    \\multicolumn{2}{|c|}{Setting 2} & \n",
      "    \\multicolumn{2}{|c|}{Setting 3} \\\\\n",
      "    \\cline{2-7}\n",
      "    & \n",
      "    Per \\newline token & Total & \n",
      "    Per \\newline token & Total & \n",
      "    Per \\newline token & Total \\\\\n",
      "    \\hline\n",
      "    \\endhead\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Reference}} \\\\\n",
      "    \\hline\n",
      "        Average per token & 1.97 & 77.89& 2.73 & 37.52& 2.65 & 108.07\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Linguistic Complexity}} \\\\\n",
      "    \\hline\n",
      "        Thai words & \\cellcolor{gray!25} 2.09 & \\cellcolor{gray!25} 68.47& 2.74 & 40.69& 2.65 & 90.46\\\\\n",
      "        Non-Thai words & 2.06 & 3.40& \\cellcolor{gray!25} 3.25 & \\cellcolor{gray!25} 6.48& 2.71 & 4.17\\\\\n",
      "        Long words & \\cellcolor{gray!25} 2.43 & \\cellcolor{gray!25} 10.90& 2.75 & 7.71& \\cellcolor{gray!25} 2.86 & \\cellcolor{gray!25} 13.25\\\\\n",
      "        Dictionary words & 1.96 & 40.55& 2.55 & 23.39& 2.59 & 56.01\\\\\n",
      "        Transliteration & 1.60 & 1.40& \\cellcolor{gray!25} 3.32 & \\cellcolor{gray!25} 2.83& 2.32 & 1.87\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Pronoun}} \\\\\n",
      "    \\hline\n",
      "        All pronoun & \\cellcolor{gray!25} 2.40 & \\cellcolor{gray!25} 4.32& \\cellcolor{gray!25} 3.34 & \\cellcolor{gray!25} 4.24& \\cellcolor{gray!25} 2.90 & \\cellcolor{gray!25} 5.73\\\\\n",
      "        >> 1st person pronoun & \\cellcolor{gray!25} 2.48 & \\cellcolor{gray!25} 3.23& \\cellcolor{gray!25} 3.59 & \\cellcolor{gray!25} 3.45& \\cellcolor{gray!25} 2.79 & \\cellcolor{gray!25} 3.92\\\\\n",
      "        >> 2nd person pronoun & \\cellcolor{gray!25} 2.61 & \\cellcolor{gray!25} 3.51& \\cellcolor{gray!25} 3.31 & \\cellcolor{gray!25} 3.71& \\cellcolor{gray!25} 3.07 & \\cellcolor{gray!25} 4.59\\\\\n",
      "        >> 3rd person pronoun & 2.07 & 2.29& 2.06 & 2.40& 2.53 & 2.90\\\\\n",
      "        >> Pronoun in non-standard spelling & \\cellcolor{gray!25} 2.27 & \\cellcolor{gray!25} 2.19& \\cellcolor{gray!25} 3.24 & \\cellcolor{gray!25} 2.98& \\cellcolor{gray!25} 2.82 & \\cellcolor{gray!25} 3.04\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Sentence-ending Particles}} \\\\\n",
      "    \\hline\n",
      "        All particles & \\cellcolor{gray!25} 2.33 & \\cellcolor{gray!25} 6.00& 2.70 & 3.14& \\cellcolor{gray!25} 2.89 & \\cellcolor{gray!25} 8.29\\\\\n",
      "        >> Socially-related particles & \\cellcolor{gray!25} 2.96 & \\cellcolor{gray!25} 4.95& \\cellcolor{gray!25} 3.00 & \\cellcolor{gray!25} 2.68& \\cellcolor{gray!25} 3.48 & \\cellcolor{gray!25} 7.22\\\\\n",
      "        >> Non-socially-related particles & 1.83 & 3.63& 2.39 & 2.56& 2.42 & 4.74\\\\\n",
      "        >> Particle in non-standard spelling & \\cellcolor{gray!25} 2.15 & \\cellcolor{gray!25} 2.04& \\cellcolor{gray!25} 3.43 & \\cellcolor{gray!25} 2.65& 2.61 & 2.58\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Sentiment-related}} \\\\\n",
      "    \\hline\n",
      "        Sentiment words & 1.89 & 4.49& 2.57 & 4.10& 2.60 & 6.25\\\\\n",
      "        >> Positive words & 2.00 & 3.47& 2.26 & 2.65& 2.63 & 4.74\\\\\n",
      "        >> Negative words & 1.69 & 2.54& \\cellcolor{gray!25} 2.83 & \\cellcolor{gray!25} 3.54& 2.55 & 3.69\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Spelling Variation}} \\\\\n",
      "    \\hline\n",
      "        All spelling variation & \\cellcolor{gray!25} 2.14 & \\cellcolor{gray!25} 12.51& 2.75 & 7.45& 2.63 & 16.17\\\\\n",
      "        >> Common misspelt words & 1.87 & 1.84& 2.79 & 2.54& \\cellcolor{gray!25} 2.99 & \\cellcolor{gray!25} 2.92\\\\\n",
      "        >> Morphophonemic variation & \\cellcolor{gray!25} 2.35 & \\cellcolor{gray!25} 8.85& \\cellcolor{gray!25} 3.13 & \\cellcolor{gray!25} 5.47& 2.64 & 10.67\\\\\n",
      "        >> Simplified variation & 1.77 & 6.00& 2.41 & 4.43& 2.53 & 8.34\\\\\n",
      "        >> Repeated characters & 1.41 & 1.58& 2.59 & 2.38& 2.59 & 2.82\\\\\n",
      "        >> Emoji & \\cellcolor{gray!25} 3.29 & \\cellcolor{gray!25} 2.36& \\cellcolor{gray!25} 4.75 & \\cellcolor{gray!25} 4.99& 2.14 & 1.66\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "\\label{closeness_wangchanberta_shapley_value}\n",
      "\\end{longtable}\n",
      "\\clearpage\n",
      "\\subsection{Respect}\n",
      "\\begin{longtable}[h]{\n",
      "    p{\\dimexpr 0.40\\linewidth-2\tabcolsep}|c|c|c|c|c|c|\n",
      "}\n",
      "    \\hline\n",
      "    Lexical Features & \n",
      "    \\multicolumn{2}{|c|}{Setting 1} & \n",
      "    \\multicolumn{2}{|c|}{Setting 2} & \n",
      "    \\multicolumn{2}{|c|}{Setting 3} \\\\\n",
      "    \\cline{2-7}\n",
      "    & \n",
      "    Per \\newline token & Total & \n",
      "    Per \\newline token & Total & \n",
      "    Per \\newline token & Total \\\\\n",
      "    \\hline\n",
      "    \\endhead\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Reference}} \\\\\n",
      "    \\hline\n",
      "        Average per token & 3.10 & 127.09& 2.15 & 28.54& 0.72 & 28.09\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Linguistic Complexity}} \\\\\n",
      "    \\hline\n",
      "        Thai words & 3.17 & 108.38& 2.19 & 31.64& 0.76 & 24.63\\\\\n",
      "        Non-Thai words & 2.84 & 5.08& \\cellcolor{gray!25} 2.80 & \\cellcolor{gray!25} 4.62& 0.65 & 1.14\\\\\n",
      "        Long words & \\cellcolor{gray!25} 3.23 & \\cellcolor{gray!25} 14.84& \\cellcolor{gray!25} 2.48 & \\cellcolor{gray!25} 6.83& \\cellcolor{gray!25} 0.96 & \\cellcolor{gray!25} 4.17\\\\\n",
      "        Dictionary words & 3.06 & 65.43& 2.02 & 17.99& 0.75 & 14.87\\\\\n",
      "        Transliteration & 2.18 & 2.01& 1.91 & 1.73& 0.51 & 0.49\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Pronoun}} \\\\\n",
      "    \\hline\n",
      "        All pronoun & \\cellcolor{gray!25} 4.15 & \\cellcolor{gray!25} 7.80& \\cellcolor{gray!25} 2.33 & \\cellcolor{gray!25} 3.14& \\cellcolor{gray!25} 1.44 & \\cellcolor{gray!25} 2.64\\\\\n",
      "        >> 1st person pronoun & \\cellcolor{gray!25} 4.38 & \\cellcolor{gray!25} 5.49& 2.09 & 2.12& \\cellcolor{gray!25} 1.11 & \\cellcolor{gray!25} 1.65\\\\\n",
      "        >> 2nd person pronoun & \\cellcolor{gray!25} 4.64 & \\cellcolor{gray!25} 6.57& \\cellcolor{gray!25} 2.52 & \\cellcolor{gray!25} 2.86& \\cellcolor{gray!25} 1.62 & \\cellcolor{gray!25} 2.17\\\\\n",
      "        >> 3rd person pronoun & \\cellcolor{gray!25} 3.74 & \\cellcolor{gray!25} 4.38& 2.05 & 2.23& 0.74 & 0.95\\\\\n",
      "        >> Pronoun in non-standard spelling & \\cellcolor{gray!25} 3.50 & \\cellcolor{gray!25} 3.71& 2.20 & 2.12& \\cellcolor{gray!25} 0.98 & \\cellcolor{gray!25} 1.18\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Sentence-ending Particles}} \\\\\n",
      "    \\hline\n",
      "        All particles & \\cellcolor{gray!25} 3.41 & \\cellcolor{gray!25} 9.67& 2.05 & 2.47& 0.81 & 2.00\\\\\n",
      "        >> Socially-related particles & \\cellcolor{gray!25} 3.90 & \\cellcolor{gray!25} 7.27& \\cellcolor{gray!25} 2.51 & \\cellcolor{gray!25} 2.37& \\cellcolor{gray!25} 1.07 & \\cellcolor{gray!25} 1.55\\\\\n",
      "        >> Non-socially-related particles & 3.04 & 6.37& 1.69 & 1.84& 0.59 & 1.29\\\\\n",
      "        >> Particle in non-standard spelling & \\cellcolor{gray!25} 3.78 & \\cellcolor{gray!25} 3.57& 2.18 & 1.87& 0.69 & 0.70\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Sentiment-related}} \\\\\n",
      "    \\hline\n",
      "        Sentiment words & 2.70 & 6.73& \\cellcolor{gray!25} 2.77 & \\cellcolor{gray!25} 4.08& 0.69 & 1.67\\\\\n",
      "        >> Positive words & 2.78 & 5.07& \\cellcolor{gray!25} 2.95 & \\cellcolor{gray!25} 2.96& 0.72 & 1.24\\\\\n",
      "        >> Negative words & 2.55 & 3.94& \\cellcolor{gray!25} 2.57 & \\cellcolor{gray!25} 3.12& 0.65 & 1.00\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "    \\multicolumn{7}{l}{\\textit{Spelling Variation}} \\\\\n",
      "    \\hline\n",
      "        All spelling variation & \\cellcolor{gray!25} 3.57 & \\cellcolor{gray!25} 21.37& 2.04 & 5.13& 0.77 & 4.40\\\\\n",
      "        >> Common misspelt words & 2.81 & 2.75& \\cellcolor{gray!25} 2.28 & \\cellcolor{gray!25} 1.74& 0.67 & 0.67\\\\\n",
      "        >> Morphophonemic variation & \\cellcolor{gray!25} 3.88 & \\cellcolor{gray!25} 15.05& 2.21 & 3.58& \\cellcolor{gray!25} 0.87 & \\cellcolor{gray!25} 3.19\\\\\n",
      "        >> Simplified variation & 2.97 & 10.19& 1.80 & 3.24& 0.54 & 1.89\\\\\n",
      "        >> Repeated characters & 2.92 & 3.33& 1.66 & 1.45& 0.50 & 0.59\\\\\n",
      "        >> Emoji & 2.80 & 2.27& \\cellcolor{gray!25} 4.04 & \\cellcolor{gray!25} 3.92& 0.69 & 0.55\\\\\n",
      "        & & & & & &\\\\\n",
      "    \\hline\n",
      "\n",
      "\\label{respect_wangchanberta_shapley_value}\n",
      "\\end{longtable}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(printed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d80c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97d19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51644f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
