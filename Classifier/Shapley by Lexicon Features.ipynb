{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826189f6-f662-4078-b677-686f7866488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_task1_conver, get_task2_conver\n",
    "from utils import dump_jsonl, load_jsonl\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "def load_shap_values(filepath):\n",
    "  with open(filepath, 'rb') as fin:\n",
    "    obj = pickle.load(fin)\n",
    "  return obj\n",
    "\n",
    "def save_shap_values(filepath, obj):\n",
    "  with open(filepath, 'wb') as fin:\n",
    "    pickle.dump(obj, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae1169a-3167-4ba9-b958-f9135805f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666cb3a3-2ec9-4842-870a-b89b96f75e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_added_toks = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"usr\", \"sys\", \"rep\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8edb0a8-9802-47dc-bed9-b2189b1fe41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7cec2-f5eb-4a72-9cb0-b6d88e97707e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53dd5afe-b3b9-484e-8628-c9d0c60c3122",
   "metadata": {},
   "source": [
    "## Load Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f52407bf-66ad-4a78-b7ff-cecd72d64d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "345c8438-4810-453d-8e69-a7c2504e5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../words.json\", encoding=\"utf-8\") as fin:\n",
    "    raw = json.load(fin)\n",
    "    thaidict_royal = set()\n",
    "    for k in raw:\n",
    "        thaidict_royal.update(raw[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07775b4b-a694-471c-9e00-07157af10be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25573 records from ../lexicons.jsonl\n"
     ]
    }
   ],
   "source": [
    "lexicons_arr = load_jsonl(\"../lexicons.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99344938-fa6d-48cf-97e2-04aa8353adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "tags = set()\n",
    "lexicons = {}\n",
    "lexicons_keys = defaultdict(list)\n",
    "\n",
    "for key, values  in lexicons_arr:\n",
    "    if len(key) <= 1:\n",
    "        continue\n",
    "        \n",
    "    key = key.lower()\n",
    "    if key.endswith(\"rep\"):\n",
    "        key = key.replace(\"rep\", \"\")\n",
    "        \n",
    "    w = word_tokenize(key)\n",
    "    \n",
    "    lexicons_keys[w[0]].append(key)\n",
    "    \n",
    "    tag = [t for t in values[\"tags\"] if not t.startswith(\"cat:\")]\n",
    "    lexicons[key] = tag\n",
    "    tags.update(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b26e3-65f2-4adf-8901-dd81f83d7dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25524732-2d62-4a46-b49a-e7dded1affbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74004a85-74b4-4e91-82a9-e71d60abb0aa",
   "metadata": {},
   "source": [
    "## Calculate Shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd74d9f2-c5cc-4c07-9b55-618539ecca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = {\n",
    "    \"Reference\" : {\n",
    "        \"all\": \"All words\",\n",
    "        \"pertoken\": \"Average per token\"\n",
    "    },\n",
    "    \n",
    "    \"Function Words\": {\n",
    "\n",
    "        \"pronoun\": \"Pronoun\",\n",
    "        \"pronoun_1st\": \">> 1st person pronoun\",\n",
    "        \"pronoun_2nd\": \">> 2nd person pronoun\",\n",
    "        \"pronoun_3rd\": \">> 3rd person pronoun\",\n",
    "        \"pronoun_misspelling\": \">> Pronoun in non-standard spelling\",\n",
    "\n",
    "        \"particles\": \"Particles\",\n",
    "        \"particles_SARP\": \">> Socially-related particles\",\n",
    "        \"particles_misspelling\": \">> Particle in non-standard spelling\",\n",
    "    },\n",
    "    \n",
    "    \"Sentiment-related\": {\n",
    "    \n",
    "        \"sentiment\": \"Sentiment words\",\n",
    "        \"sentiment_positive\": \">> Positive words\",\n",
    "        \"sentiment_negative\": \">> Negative words\",\n",
    "    },\n",
    "    \n",
    "    \"Internet Language\": {\n",
    "        \"misspelling\": \"Spelling variation\",\n",
    "        \"misspelling_common\": \">> Common misspelt words\",\n",
    "        \"misspelling_intention\": \">> Semantic variation\",\n",
    "        \"misspelling_shorten\": \">> Simplified variation\",\n",
    "        \"nrepeat\": \">> Repeated characters\",\n",
    "\n",
    "        \"abbr\": \"Abbreviation\",\n",
    "        \"slang\": \"Slang\",\n",
    "        \"swear\": \"Swear words\",\n",
    "        \"transliterated\": \"Transliteration\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2702eead-4dbf-41bc-80a5-d59c87993d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_token_2_words(words, shap_tokens, debug=False):\n",
    "    tokens = [w for w, _ in  shap_tokens]\n",
    "    values = np.array([v for _, v in  shap_tokens])\n",
    "    \n",
    "    idxs = []\n",
    "    sidx = 0\n",
    "    windows = 10\n",
    "        \n",
    "    newwords = []\n",
    "    newtokens = []\n",
    "    newvalues = []\n",
    "    \n",
    "    \n",
    "    widx = 0\n",
    "    w = \"\"\n",
    "    while widx < len(words):\n",
    "        w += words[widx]\n",
    "        if sidx >= len(tokens):\n",
    "#             print(newwords)\n",
    "#             assert(False)\n",
    "            break\n",
    "            \n",
    "#         print(widx, w, sidx, tokens[sidx])\n",
    "#         break\n",
    "        s = \"\"\n",
    "        matched = False\n",
    "        for tidx in range(sidx, min(sidx+windows, len(tokens))):\n",
    "            s += tokens[tidx]\n",
    "            if s==w:\n",
    "                matched = True\n",
    "                break\n",
    "                \n",
    "        if matched:\n",
    "            if debug:\n",
    "                print(\"MATCHED\", w)\n",
    "            idxs.append([sidx, tidx+1])\n",
    "            newwords.append(w)\n",
    "            newtokens.append(\"\".join(tokens[sidx:tidx+1]))\n",
    "            newvalues.append(values[sidx:tidx+1].sum())\n",
    "            sidx = tidx+1\n",
    "            w = \"\"\n",
    "            widx += 1\n",
    "            continue\n",
    "        \n",
    "        if debug:\n",
    "            print(\"NOT MATCHED\", w, s)\n",
    "            \n",
    "        if not s.startswith(w):\n",
    "            sidx += 1\n",
    "            w = \"\"\n",
    "#             print(\"SKIP TOKEN\")\n",
    "            continue\n",
    "        else:                \n",
    "            widx += 1\n",
    "#             print(\"MERGE WORDS\")\n",
    "            continue\n",
    "            \n",
    "    if debug:    \n",
    "        print(newwords)\n",
    "        print(newtokens)\n",
    "    \n",
    "    \n",
    "    return newtokens, newvalues\n",
    "\n",
    "\n",
    "def get_lexicon_feats(token, ref_text):\n",
    "#     print(token, ref_text)\n",
    "    if token not in lexicons_keys:\n",
    "        return []\n",
    "    \n",
    "    feats = [\"all\"]\n",
    "    for l in lexicons_keys[token]:\n",
    "        if not ref_text.startswith(l):\n",
    "            continue\n",
    "        \n",
    "        feats.extend(lexicons[l])\n",
    "    return feats\n",
    "\n",
    "def get_shap_lexicons(df, raw_shap_values):\n",
    "    shap_lexicons = {}\n",
    "    label_values = df[\"label\"].unique()\n",
    "    \n",
    "\n",
    "    _tmp = raw_shap_values[:, :]\n",
    "    shap_data = _tmp.data\n",
    "    shap_values = _tmp.values\n",
    "\n",
    "    for _, label in enumerate(label_values):\n",
    "        feats = []\n",
    "        for idx, row in df.iterrows():\n",
    "            if row[\"label\"]!=label:\n",
    "                continue\n",
    "            \n",
    "            text = row[\"text\"]\n",
    "            words = word_tokenize(preprocess(row[\"text\"]))\n",
    "            words = [w.strip() for w in words if len(w.strip())>0]\n",
    "            \n",
    "            shap_tokens = [(w.strip(), v) for w,v in zip(shap_data[idx], shap_values[idx]) if len(w.strip())>0]\n",
    "            shap_tokens = map_token_2_words(words, shap_tokens, debug=False)\n",
    "            feats.append(shap_tokens)\n",
    "        shap_lexicons[label] = feats\n",
    "    return shap_lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22de286d-e8d1-4402-aa6d-1f17f66ff010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_feats(shap_lexicons):\n",
    "    output = {}\n",
    "    for label in shap_lexicons:\n",
    "        all_shap_feats = []\n",
    "        for tokens, values in shap_lexicons[label]:\n",
    "            shap_feats = defaultdict(int)\n",
    "            for tidx, (t, v) in enumerate(zip(tokens, values)):\n",
    "                feats = get_lexicon_feats(t, \"\".join(tokens[tidx:]))\n",
    "                \n",
    "                if t==\"rep\":\n",
    "                    feats.append(\"nrepeat\")\n",
    "                    \n",
    "                for f in feats:\n",
    "                    shap_feats[f] += v\n",
    "            \n",
    "            shap_feats[\"pertoken\"] = sum(values)/len(values)\n",
    "            all_shap_feats.append(shap_feats)\n",
    "            \n",
    "        mean_shap_feats = {}\n",
    "        for g in metric_names:\n",
    "            for m in metric_names[g]:\n",
    "                values = []\n",
    "                for feats in all_shap_feats:\n",
    "                    if m in feats:\n",
    "                        values.append(feats[m])\n",
    "                \n",
    "                if len(values)==0:\n",
    "                    mean_shap_feats[m] = 0\n",
    "                    continue\n",
    "                    \n",
    "                values = np.array(values)\n",
    "                rms = np.sqrt(np.mean(values**2))\n",
    "                mean_shap_feats[m] = rms\n",
    "        \n",
    "        output[label] = mean_shap_feats\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bff40ac-916b-4112-8515-f9fc1e074186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "import itertools\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38d92ffa-4e27-4dc7-86ff-bdf87a52907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "from data_loader import preprocess\n",
    "\n",
    "def run_preprocess(train, val, test):\n",
    "    train[\"text\"] = train[\"text\"].apply(preprocess)\n",
    "    val[\"text\"] = val[\"text\"].apply(preprocess)\n",
    "    test[\"text\"] = test[\"text\"].apply(preprocess)\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34d4247e-6a21-4fdf-ae2d-8bed8674f10e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a91f89-942a-4214-9c3d-b24e70d17e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lexicons(df, shap_path):\n",
    "    train, val, test = df\n",
    "    train[\"split\"] = \"train\"\n",
    "    val[\"split\"] = \"val\"\n",
    "    test[\"split\"] = \"test\"\n",
    "\n",
    "    df = pd.concat([train, test, val])\n",
    "    shap_values = load_shap_values(shap_path)\n",
    "\n",
    "    assert(len(df)==len(shap_values))\n",
    "    \n",
    "    shap_lexicons = get_shap_lexicons(df, shap_values)\n",
    "    shap_feats = get_shap_feats(shap_lexicons)\n",
    "    return shap_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c0cc5-c51a-4397-bbad-8986f83686f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0bd5f35-d78d-46fd-98bd-1e761348178f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1234 records from ../Task1/annotated_conersations.jsonl\n",
      "N 1096 60 60\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats1 = run_lexicons(df, f\"./ShapleyValues/task1_clse.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53bdfd3a-ef93-4617-9adf-4e3ed2abc961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1234 records from ../Task1/annotated_conersations.jsonl\n",
      "N 1098 61 61\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"authority\", skips = [\"3. Not respect\"], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats2 = run_lexicons(df, f\"./ShapleyValues/task1_auth.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23316172-1250-4382-bc8f-536304b03837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55770a4f-723b-4d05-8a75-6a25c2cf0586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f57ea61c-16e1-44ee-b03e-91799e04d4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ../Task2/annotated/annotated.jsonl\n",
      "N 1495 186 186\n"
     ]
    }
   ],
   "source": [
    "df = df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats3 = run_lexicons(df, f\"./ShapleyValues/task2_clse.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a934c67-2576-4279-afb7-94922d146e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ../Task2/annotated/annotated.jsonl\n",
      "N 1642 205 205\n"
     ]
    }
   ],
   "source": [
    "df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats4 = run_lexicons(df, f\"./ShapleyValues/task2_auth.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6f715-29e6-466d-bf9e-53fea698d56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2103cd4-4000-4215-bb1c-4893aad27503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac61f618-6857-4a33-8263-33a5bb4482d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ../Task3/annotated/annotated.jsonl\n",
      "N 1090 60 60\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats5 = run_lexicons(df, f\"./ShapleyValues/task3_clse.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51caf043-eef6-4c61-8569-d75710699aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ../Task3/annotated/annotated.jsonl\n",
      "N 1099 61 61\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "df = run_preprocess(*df)\n",
    "shap_feats6 = run_lexicons(df, f\"./ShapleyValues/task3_auth.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a2b71-6e9f-429b-a573-23a68120b2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e1f454-7223-4697-8333-40ea073ad662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197af2d-bf31-412a-84db-00e7ff675496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0d775-a0f4-450f-807b-09faf6b9e9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ad025-fe9b-47e8-b42e-9b2d310643de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bf3225b-68a6-4507-9c72-f31aa090c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shap(shap_feats, print_labels):\n",
    "    print_text = \"\"\n",
    "    for g in metric_names:\n",
    "        if g!=\"Reference\":\n",
    "            print_text += (\"\\multicolumn{5}{l}{\\\\textit{\"+g+\"}} \\\\\\\\\") +\"\\n\"\n",
    "            \n",
    "        for m in metric_names[g]:\n",
    "            s = f\"{metric_names[g][m]} \"\n",
    "            for l in print_labels:\n",
    "                if l not in shap_feats:\n",
    "                    s += f\"& - \"\n",
    "                elif m not in shap_feats[l]:\n",
    "                    s += f\"& 0.00 \"\n",
    "                else:\n",
    "                    s += f\"& {shap_feats[l][m]:.2f} \"\n",
    "\n",
    "            s += \"\\\\\\\\\"\n",
    "            print_text += (s)+\"\\n\"\n",
    "        \n",
    "        if g!=\"Reference\":\n",
    "            print_text += (\"&  & &  & \\\\\\\\\") +\"\\n\"\n",
    "        print_text += (\"\\hline\")+\"\\n\"\n",
    "    return print_text\n",
    "\n",
    "clse_print_labels = ['1. Close', '2. Know each other', \"3. Don't know each other\", \"4. Don't like each other\"]\n",
    "auth_print_labels = ['0. Very respect', '1. Respect', '2. Normal', '3. Not respect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ebad8-36dd-4331-b924-18ed850562f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = [\n",
    "    \"Setting 1: Private Conversations with Self-Reported Labels\",\n",
    "    \"Setting 2: Public Conversations with Labels from 3rd Party \",\n",
    "    \"Setting 3: Private Conversations with Labels from 3rd Party \",\n",
    "]\n",
    "\n",
    "table_contents = [\n",
    "    (shap_feats1, shap_feats2),\n",
    "    (shap_feats3, shap_feats4),\n",
    "    (shap_feats5, shap_feats6),\n",
    "]\n",
    "\n",
    "printed_text = \"\"\n",
    "for section, (t1, t2) in zip(sections, table_contents):\n",
    "    \n",
    "    printed_text += \"\\subsection{\"+section+\"}\"\n",
    "    \n",
    "    printed_text += '''\n",
    "\\subsubsection{Closeness}\n",
    "\\\\begin{longtable}[h]{\n",
    "        p{\\dimexpr 0.40\\linewidth-2\\\\tabcolsep}|\n",
    "        p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "        p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "        p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "        p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "    }\n",
    "        \\hline\n",
    "\n",
    "        Lexical Features & Close & Know each other & Don't know each other &  Don't like each other\\\\\\\\\n",
    "        \\hline\n",
    "        \\endfirsthead\n",
    "        \n",
    "        \\endhead\n",
    "            '''\n",
    "    \n",
    "    s = print_shap(t1, clse_print_labels)\n",
    "    printed_text += \"\\n            \".join(s.split(\"\\n\"))\n",
    "    printed_text += '''\n",
    "\\end{longtable}\n",
    "\\clearpage\n",
    "\n",
    "'''\n",
    "    printed_text += '''\n",
    "\\subsubsection{Respect}\n",
    "\\\\begin{longtable}[h]{\n",
    "        p{\\dimexpr 0.40\\linewidth-2\\\\tabcolsep}|\n",
    "        p{\\dimexpr 0.16\\linewidth-2\\\\tabcolsep}\n",
    "        p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "        p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "        p{\\dimexpr 0.15\\linewidth-2\\\\tabcolsep}\n",
    "    }\n",
    "        \\hline\n",
    "\n",
    "        Lexical Features & Very respect & Respect & Normal &  Not respect\\\\\\\\\n",
    "        \\hline\n",
    "        \\endfirsthead\n",
    "        \n",
    "        \\endhead\n",
    "            '''\n",
    "    \n",
    "    s = print_shap(t2, auth_print_labels)\n",
    "    printed_text += \"\\n            \".join(s.split(\"\\n\"))\n",
    "    printed_text += '''\n",
    "\\end{longtable}\n",
    "\\clearpage\n",
    "\n",
    "'''\n",
    "\n",
    "#     break\n",
    "    \n",
    "# print(printed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0788d23-625f-4e05-9f29-23c3aad4a7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f48647-ce76-4955-9dc0-6c376dac2472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
