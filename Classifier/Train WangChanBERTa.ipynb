{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee748ab1-ddbf-45ee-ad21-9011cdffb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_task1_conver, get_task2_conver, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7808dd-31f7-422c-9626-47a3d2647577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d2ed16-a74e-403f-b754-c6bc208b65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task1_conver(\"../Task1//annotated_conersations.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=False)\n",
    "# # print(df[0][\"text\"][0])\n",
    "# pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0fdc65-bee0-48df-9ce1-38db99cb8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"closeness\", skips = [], only_user=False)\n",
    "# # print(df[0][\"text\"][0])\n",
    "# pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045bb11d-dcae-4ac1-ad23-7458fcfe2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"closeness\", skips = [], only_user=False)\n",
    "# # print(df[0][\"text\"][0])\n",
    "# pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0003391-29d3-4de6-a36e-4a3846047fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import load_jsonl, dump_jsonl, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a16a7f-c577-4736-b400-73983925692c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb03998c-5faa-42e0-96b5-2d609f1f5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import wandb\n",
    "# from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "# from pythainlp.tokenize import word_tokenize\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f343df2c-a90d-405b-ae84-207c74e0a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "import torch.nn as nn\n",
    "import os, shutil\n",
    "\n",
    "def run_exp(out_dir, df, report=\"none\"):\n",
    "\n",
    "    set_random_seed()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device(\"cpu\")\n",
    "    print(\"START\")\n",
    "    print(\"step 1: load data\")\n",
    "    train, val, test = df\n",
    "    \n",
    "    #train = train.head(100)\n",
    "    #val = val.head(100)\n",
    "    #test = test.head(100)\n",
    "\n",
    "    print(\"step 2: load tokenizer\")\n",
    "    model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    num_added_toks = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"usr\", \"sys\", \"rep\"]})\n",
    "\n",
    "    print(\"step 3: init data\")\n",
    "    ds = DatasetDict()\n",
    "    ds['train'] = Dataset.from_pandas(train)\n",
    "    ds['val'] = Dataset.from_pandas(val)\n",
    "    ds['test'] = Dataset.from_pandas(test)\n",
    "\n",
    "    labels = train[\"label\"].unique()\n",
    "    num_labels = len(labels)\n",
    "    print(labels)\n",
    "    \n",
    "    class_weights = compute_class_weight(\"balanced\", classes=labels, y=train[\"label\"].values)\n",
    "    class_weights = torch.tensor(class_weights).float().to(device)\n",
    "    \n",
    "    id2label = {i:l for i, l in enumerate(labels)}\n",
    "    label2id = {l:i for i, l in enumerate(labels)}\n",
    "\n",
    "    def word_tokenize(d, tokenizer=None, label2id=None, max_length=256):\n",
    "        texts = [preprocess(t) for t in d[\"text\"]]\n",
    "#         print(texts)\n",
    "        tokens = tokenizer(texts, truncation=True, max_length=max_length)\n",
    "        num = [len(t) for t in tokens[\"input_ids\"]]\n",
    "#         print(num)\n",
    "#         print(\"AVG\", len(num), sum(num)/len(num))\n",
    "        tokens[\"label\"] = [label2id[label] for label in d[\"label\"]]\n",
    "        return tokens\n",
    "\n",
    "    tokenized_ds = ds.map(word_tokenize, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"label2id\": label2id, \"max_length\":max_length})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"step 4: load model\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id);\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(device)\n",
    "\n",
    "    metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        return metrics.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"step 5: fine-tune\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=report,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        run_name=out_dir,\n",
    "    )\n",
    "    \n",
    "    class CustomTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            # forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get(\"logits\")\n",
    "            \n",
    "            loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,   \n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    best_ckpt_path = trainer.state.best_model_checkpoint\n",
    "    print(best_ckpt_path)\n",
    "\n",
    "    modle_out_path = out_dir+\"/best_model\"\n",
    "    if os.path.exists(modle_out_path):\n",
    "        shutil.rmtree(modle_out_path)\n",
    "        \n",
    "    os.rename(best_ckpt_path, modle_out_path)\n",
    "    best_ckpt_path = modle_out_path\n",
    "    \n",
    "    print(\"step 6: evaluate\")\n",
    "    e = trainer.evaluate(tokenized_ds[\"test\"])\n",
    "    print(e)\n",
    "\n",
    "    print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1c3b6-9d95-4a3c-9a77-b35b7d268b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6de0fefe-0d41-4e0e-94e8-d23cb2591f71",
   "metadata": {},
   "source": [
    "## Task1: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7fb066-1968-43e1-8f28-e5d8f592e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = \"none\"\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2724398-dbfb-4630-a9cd-84ec8fabc07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# stream = os.popen('nohup python3 run_train_task_classifier.py > train2.out &')\n",
    "# output = stream.read()\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59404733-41e6-4b1d-a275-8622006a5e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1234 records from ../Task1/annotated_conersations.jsonl\n",
      "N 1096 60 60\n",
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n",
      "step 3: init data\n",
      "['1. Close' \"3. Don't know each other\" '2. Know each other']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b29ce0bf2a474d8a282a025ee6ed31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2956ce0464b448768ea960947c68af2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607106086ae9426fb7940701c31d8cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1096\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 345\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [345/345 02:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858184</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.490110</td>\n",
       "      <td>0.484416</td>\n",
       "      <td>0.598148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.810594</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.546458</td>\n",
       "      <td>0.542588</td>\n",
       "      <td>0.648148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.703888</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.540055</td>\n",
       "      <td>0.529085</td>\n",
       "      <td>0.605556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.548822</td>\n",
       "      <td>0.545139</td>\n",
       "      <td>0.618519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.640477</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.554167</td>\n",
       "      <td>0.540476</td>\n",
       "      <td>0.612963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-69\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-138\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-138/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-1370] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-207\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-276\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-345\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-207] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task1_clse_usr/checkpoint-345 (score: 0.5541666666666667).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task1_clse_usr/checkpoint-345\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8123156428337097, 'eval_accuracy': 0.6666666666666666, 'eval_f1': 0.3947746195808211, 'eval_precision': 0.4102564102564103, 'eval_recall': 0.3871158392434988, 'eval_runtime': 0.6838, 'eval_samples_per_second': 87.749, 'eval_steps_per_second': 5.85, 'epoch': 5.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "# df = (df[0].head(), df[1].head(), df[2].head())\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()\n",
    "run_exp(\"./Models/task1_clse_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2522b5e-0d0a-4515-a821-3dbb003248c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d026221-27c1-48f4-b01e-a9c59ef53ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1234 records from ../Task1/annotated_conersations.jsonl\n",
      "N 1098 61 61\n",
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "['2. Normal' '0. Very respect' '1. Respect']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09cd0afdebc4333a890a636fd50bf58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e65ee8965b44dcbad37bbef9e7afe96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f4cf6f25ab415c9e9f5ca78d97944f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"2. Normal\",\n",
      "    \"1\": \"0. Very respect\",\n",
      "    \"2\": \"1. Respect\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0. Very respect\": 1,\n",
      "    \"1. Respect\": 2,\n",
      "    \"2. Normal\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1098\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 345\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [345/345 02:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.101633</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.328449</td>\n",
       "      <td>0.417647</td>\n",
       "      <td>0.396693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.071011</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.348709</td>\n",
       "      <td>0.365801</td>\n",
       "      <td>0.349735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.119959</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.424339</td>\n",
       "      <td>0.405556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.090216</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.439948</td>\n",
       "      <td>0.448686</td>\n",
       "      <td>0.442063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.075473</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.412387</td>\n",
       "      <td>0.417262</td>\n",
       "      <td>0.409392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-69\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-138\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-138/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-207\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-276\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-345\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-207] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task1_auth_usr/checkpoint-276 (score: 0.4399484374017481).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task1_auth_usr/checkpoint-276\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1755664348602295, 'eval_accuracy': 0.3770491803278688, 'eval_f1': 0.3663824504160638, 'eval_precision': 0.37103174603174605, 'eval_recall': 0.3637037037037037, 'eval_runtime': 0.7009, 'eval_samples_per_second': 87.035, 'eval_steps_per_second': 5.707, 'epoch': 5.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"authority\", skips = [\"3. Not respect\"], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()\n",
    "run_exp(\"./Models/task1_auth_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0dd0a-99a3-481e-8ea4-4ced21a3d033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bff50-4304-4106-bbf1-4633c5e6e187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efbe5ee6-da8d-446e-a14f-85ff9c617546",
   "metadata": {},
   "source": [
    "## Task2: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2228a1-c5f6-4220-92aa-406e3f3108d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ../Task2/annotated/annotated.jsonl\n",
      "N 1495 186 186\n",
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "[\"3. Don't know each other\" '2. Know each other' '1. Close']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f7afb18f0644a7a0d1ef28b3d44c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebb2e6aa1fb4e00b426ee36d80faa56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa67e7e89e334d1686f743c207f28b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"3. Don't know each other\",\n",
      "    \"1\": \"2. Know each other\",\n",
      "    \"2\": \"1. Close\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"1. Close\": 2,\n",
      "    \"2. Know each other\": 1,\n",
      "    \"3. Don't know each other\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1495\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 470\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='470' max='470' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [470/470 03:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.100057</td>\n",
       "      <td>0.311828</td>\n",
       "      <td>0.294830</td>\n",
       "      <td>0.426487</td>\n",
       "      <td>0.404218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.998645</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.435973</td>\n",
       "      <td>0.470728</td>\n",
       "      <td>0.462818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.996740</td>\n",
       "      <td>0.586022</td>\n",
       "      <td>0.457904</td>\n",
       "      <td>0.491238</td>\n",
       "      <td>0.479289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.046070</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.454364</td>\n",
       "      <td>0.487032</td>\n",
       "      <td>0.477223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.006600</td>\n",
       "      <td>0.655914</td>\n",
       "      <td>0.482706</td>\n",
       "      <td>0.494429</td>\n",
       "      <td>0.515221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-94\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-94/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-94/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-94/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-94/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-188\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-188/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-188/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-188/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-188/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-282\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-282/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-282/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-282/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-282/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-94] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-376\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-376/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-376/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-376/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-376/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-188] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-470\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-470/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-470/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-470/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-470/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-282] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task2_clse_usr/checkpoint-470 (score: 0.4827057087731245).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task2_clse_usr/checkpoint-470\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0347660779953003, 'eval_accuracy': 0.6182795698924731, 'eval_f1': 0.4694461789009754, 'eval_precision': 0.4980901451489687, 'eval_recall': 0.47104668381264125, 'eval_runtime': 1.8693, 'eval_samples_per_second': 99.505, 'eval_steps_per_second': 6.42, 'epoch': 5.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()\n",
    "\n",
    "run_exp(\"./Models/task2_clse_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4aec330-7005-4c77-b427-1489374cc63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ../Task2/annotated/annotated.jsonl\n",
      "N 1642 205 205\n",
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "['2. Normal' '3. Not respect' '1. Respect']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270bf8572bfa48849ac2fe6d9b5200e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d2cbaad1a24d519d2091bc93b9fbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226d676273214b138eefad1e7373f426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"2. Normal\",\n",
      "    \"1\": \"3. Not respect\",\n",
      "    \"2\": \"1. Respect\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"1. Respect\": 2,\n",
      "    \"2. Normal\": 0,\n",
      "    \"3. Not respect\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1642\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 515\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='515' max='515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [515/515 03:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.581241</td>\n",
       "      <td>0.663415</td>\n",
       "      <td>0.634901</td>\n",
       "      <td>0.640859</td>\n",
       "      <td>0.692504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.430690</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.787835</td>\n",
       "      <td>0.776331</td>\n",
       "      <td>0.802583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593266</td>\n",
       "      <td>0.795122</td>\n",
       "      <td>0.759779</td>\n",
       "      <td>0.764052</td>\n",
       "      <td>0.787101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.516170</td>\n",
       "      <td>0.839024</td>\n",
       "      <td>0.795870</td>\n",
       "      <td>0.794421</td>\n",
       "      <td>0.806861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.565616</td>\n",
       "      <td>0.809756</td>\n",
       "      <td>0.769680</td>\n",
       "      <td>0.768041</td>\n",
       "      <td>0.794113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-103\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-103/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-103/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-103/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-103/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-206\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-206/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-206/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-206/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-206/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-309\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-309/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-309/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-309/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-309/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-103] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-412\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-412/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-412/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-412/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-412/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-206] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-515\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-515/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-515/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-515/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-515/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-309] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task2_auth_usr/checkpoint-412 (score: 0.7958696340338852).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 205\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task2_auth_usr/checkpoint-412\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8025819063186646, 'eval_accuracy': 0.7707317073170732, 'eval_f1': 0.7648739164696611, 'eval_precision': 0.7783709434773266, 'eval_recall': 0.7665112665112664, 'eval_runtime': 2.0915, 'eval_samples_per_second': 98.016, 'eval_steps_per_second': 6.216, 'epoch': 5.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()\n",
    "\n",
    "run_exp(\"./Models/task2_auth_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89a668-3680-4d2d-82ac-4b705dac628a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccd70a91-a596-4118-834c-ed6567ac8c53",
   "metadata": {},
   "source": [
    "## Task3: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c1a0dbb-4004-44de-ac45-5085086dbd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ../Task3/annotated/annotated.jsonl\n",
      "N 1090 60 60\n",
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n",
      "step 3: init data\n",
      "['1. Close' '2. Know each other' \"3. Don't know each other\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bdaa5949854ff287850a1683dd5bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d42fd7afebc46b9a69cd8975c11c810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d72f597f1be4b618815ad697bb5b896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1090\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 345\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [345/345 02:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.970821</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.439338</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.454718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.810533</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.493582</td>\n",
       "      <td>0.478114</td>\n",
       "      <td>0.524419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850519</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.517598</td>\n",
       "      <td>0.500561</td>\n",
       "      <td>0.550972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.779988</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.629574</td>\n",
       "      <td>0.818860</td>\n",
       "      <td>0.591157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.753837</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.674884</td>\n",
       "      <td>0.701502</td>\n",
       "      <td>0.656472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-69\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-138\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-138/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-345] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-207\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-276\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-345\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-207] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task3_clse_usr/checkpoint-345 (score: 0.674883674883675).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task3_clse_usr/checkpoint-345\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6559752821922302, 'eval_accuracy': 0.65, 'eval_f1': 0.38003565062388595, 'eval_precision': 0.39829059829059826, 'eval_recall': 0.375, 'eval_runtime': 0.6718, 'eval_samples_per_second': 89.313, 'eval_steps_per_second': 5.954, 'epoch': 5.0}\n",
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()\n",
    "run_exp(\"./Models/task3_clse_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "675c4a41-36fd-406e-bca1-826c580d6c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ../Task3/annotated/annotated.jsonl\n",
      "N 1099 61 61\n",
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "['1. Respect' '2. Normal' '3. Not respect']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d55298f18b4048b3605121a55d5599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3eb6e303b846649b9461e6f1303391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b294c0c86ff4519ab1c2ff92d76ab92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"1. Respect\",\n",
      "    \"1\": \"2. Normal\",\n",
      "    \"2\": \"3. Not respect\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"1. Respect\": 0,\n",
      "    \"2. Normal\": 1,\n",
      "    \"3. Not respect\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1099\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 345\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [345/345 02:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.905223</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.516219</td>\n",
       "      <td>0.504006</td>\n",
       "      <td>0.655674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.957578</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.448141</td>\n",
       "      <td>0.464806</td>\n",
       "      <td>0.613121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.857010</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.589075</td>\n",
       "      <td>0.576907</td>\n",
       "      <td>0.679078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.798238</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.630644</td>\n",
       "      <td>0.593567</td>\n",
       "      <td>0.738652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.866463</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.702891</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>0.793262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-69\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-138\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-138/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-345] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-207\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-276\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-345\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-207] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task3_auth_usr/checkpoint-345 (score: 0.7028909787530476).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task3_auth_usr/checkpoint-345\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2966509163379669, 'eval_accuracy': 0.819672131147541, 'eval_f1': 0.813040293040293, 'eval_precision': 0.795791487326638, 'eval_recall': 0.8916666666666666, 'eval_runtime': 0.7108, 'eval_samples_per_second': 85.819, 'eval_steps_per_second': 5.627, 'epoch': 5.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()\n",
    "run_exp(\"./Models/task3_auth_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f0ed3-3d91-40b3-9fb4-f91711d89210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c985948-e47f-445c-aa70-755816e2feaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
