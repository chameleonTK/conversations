{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee748ab1-ddbf-45ee-ad21-9011cdffb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_task1_conver, get_task2_conver, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7808dd-31f7-422c-9626-47a3d2647577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d2ed16-a74e-403f-b754-c6bc208b65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task1_conver(\"../Task1//annotated_conersations.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=False)\n",
    "# # print(df[0][\"text\"][0])\n",
    "# pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0fdc65-bee0-48df-9ce1-38db99cb8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"closeness\", skips = [], only_user=False)\n",
    "# # print(df[0][\"text\"][0])\n",
    "# pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045bb11d-dcae-4ac1-ad23-7458fcfe2885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"closeness\", skips = [], only_user=False)\n",
    "# # print(df[0][\"text\"][0])\n",
    "# pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0003391-29d3-4de6-a36e-4a3846047fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils import load_jsonl, dump_jsonl, set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a16a7f-c577-4736-b400-73983925692c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb03998c-5faa-42e0-96b5-2d609f1f5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "# import wandb\n",
    "# from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "# from pythainlp.tokenize import word_tokenize\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad407677-6b14-4fc3-86a6-4a5c7da4c90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f343df2c-a90d-405b-ae84-207c74e0a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "import torch.nn as nn\n",
    "import os, shutil\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def run_exp(out_dir, df, report=\"none\", regressor_configs=None):\n",
    "\n",
    "    set_random_seed()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device(\"cpu\")\n",
    "    print(\"START\")\n",
    "    print(\"step 1: load data\")\n",
    "    train, val, test = df\n",
    "    \n",
    "#     train = train.head(100)\n",
    "#     val = val.head(100)\n",
    "#     test = test.head(100)\n",
    "\n",
    "    print(\"step 2: load tokenizer\")\n",
    "    model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    num_added_toks = tokenizer.add_special_tokens({\"additional_special_tokens\": [\"usr\", \"sys\", \"rep\"]})\n",
    "\n",
    "    print(\"step 3: init data\")\n",
    "    ds = DatasetDict()\n",
    "    ds['train'] = Dataset.from_pandas(train)\n",
    "    ds['val'] = Dataset.from_pandas(val)\n",
    "    ds['test'] = Dataset.from_pandas(test)\n",
    "\n",
    "    if regressor_configs is None:\n",
    "        labels = train[\"label\"].unique()\n",
    "        num_labels = len(labels)\n",
    "        print(labels)\n",
    "    \n",
    "        class_weights = compute_class_weight(\"balanced\", classes=labels, y=train[\"label\"].values)\n",
    "        class_weights = torch.tensor(class_weights).float().to(device)\n",
    "\n",
    "        id2label = {i:l for i, l in enumerate(labels)}\n",
    "        label2id = {l:i for i, l in enumerate(labels)}\n",
    "\n",
    "        def word_tokenize(d, tokenizer=None, label2id=None, max_length=256):\n",
    "            texts = [preprocess(t) for t in d[\"text\"]]\n",
    "    #         print(texts)\n",
    "            tokens = tokenizer(texts, truncation=True, max_length=max_length)\n",
    "            num = [len(t) for t in tokens[\"input_ids\"]]\n",
    "    #         print(num)\n",
    "    #         print(\"AVG\", len(num), sum(num)/len(num))\n",
    "            tokens[\"label\"] = [label2id[label] for label in d[\"label\"]]\n",
    "            return tokens\n",
    "    else:\n",
    "#         labels = train[\"label\"].unique()\n",
    "        num_labels = 1\n",
    "\n",
    "        id2label = {1: regressor_configs[\"label\"]}\n",
    "        label2id = [regressor_configs[\"not_label\"], regressor_configs[\"label\"]]\n",
    "\n",
    "        def word_tokenize(d, tokenizer=None, label2id=None, max_length=256):\n",
    "            texts = [preprocess(t) for t in d[\"text\"]]\n",
    "    #         print(texts)\n",
    "            tokens = tokenizer(texts, truncation=True, max_length=max_length)\n",
    "            num = [len(t) for t in tokens[\"input_ids\"]]\n",
    "    #         print(num)\n",
    "    #         print(\"AVG\", len(num), sum(num)/len(num))\n",
    "            tokens[\"label\"] = [regressor_configs[\"label_fn\"](label) for label in d[\"label\"]]\n",
    "            return tokens\n",
    "        \n",
    "        \n",
    "        \n",
    "    tokenized_ds = ds.map(word_tokenize, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"label2id\": label2id, \"max_length\":max_length})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    print(\"step 4: load model\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id);\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(device)\n",
    "\n",
    "    if regressor_configs is None:\n",
    "        metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            return metrics.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    else:\n",
    "        label_fn = regressor_configs[\"label_fn\"]\n",
    "        def compute_metrics(eval_pred):           \n",
    "            predictions, actual = eval_pred\n",
    "            predictions = predictions.reshape(-1)\n",
    "            \n",
    "            predicted_labels = [label_fn(p) for p in predictions]\n",
    "            actual_labels = [label_fn(p) for p in actual]\n",
    "            p, r, f1, _ = precision_recall_fscore_support(actual_labels, predicted_labels, average='macro')\n",
    "            \n",
    "            return {\n",
    "                \"r2_score\": r2_score(actual, predictions),\n",
    "                \"mean_squared_error\": np.sqrt(mean_squared_error(actual, predictions)),\n",
    "                \"accuracy\": accuracy_score(actual_labels, predicted_labels),\n",
    "                \"f1\": f1,\n",
    "                \"precision\": p,\n",
    "                \"recall\": r,\n",
    "            }\n",
    "\n",
    "\n",
    "    print(\"step 5: fine-tune\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=report,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        run_name=out_dir,\n",
    "    )\n",
    "    \n",
    "    if regressor_configs is None:\n",
    "        class CustomTrainer(Trainer):\n",
    "            def compute_loss(self, model, inputs, return_outputs=False):\n",
    "                labels = inputs.get(\"labels\")\n",
    "                # forward pass\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "                logits = outputs.get(\"logits\")\n",
    "\n",
    "                loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "                loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "                return (loss, outputs) if return_outputs else loss\n",
    "    else:\n",
    "        CustomTrainer = Trainer\n",
    "        \n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"val\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,   \n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    best_ckpt_path = trainer.state.best_model_checkpoint\n",
    "    print(best_ckpt_path)\n",
    "\n",
    "    modle_out_path = out_dir+\"/best_model\"\n",
    "    if os.path.exists(modle_out_path):\n",
    "        shutil.rmtree(modle_out_path)\n",
    "        \n",
    "    os.rename(best_ckpt_path, modle_out_path)\n",
    "    best_ckpt_path = modle_out_path\n",
    "    \n",
    "    print(\"step 6: evaluate\")\n",
    "    e = trainer.evaluate(tokenized_ds[\"test\"])\n",
    "    print(e)\n",
    "\n",
    "    print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6d6ae-c79f-4ec2-b912-8440b1a5ec65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1c3b6-9d95-4a3c-9a77-b35b7d268b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6de0fefe-0d41-4e0e-94e8-d23cb2591f71",
   "metadata": {},
   "source": [
    "## Task1: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c7fb066-1968-43e1-8f28-e5d8f592e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = \"none\"\n",
    "batch_size = 16\n",
    "max_length = 128\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2724398-dbfb-4630-a9cd-84ec8fabc07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# stream = os.popen('nohup python3 run_train_task_classifier.py > train2.out &')\n",
    "# output = stream.read()\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc5524-69ab-4661-9dd8-2ceda81651bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59404733-41e6-4b1d-a275-8622006a5e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1234 records from ../Task1/annotated_conersations.jsonl\n",
      "N 1096 60 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1. Close</th>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. Know each other</th>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3. Don't know each other</th>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text\n",
       "label                         \n",
       "1. Close                   551\n",
       "2. Know each other         230\n",
       "3. Don't know each other   435"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "# df = (df[0].head(), df[1].head(), df[2].head())\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d129a6c4-925e-4ecb-ba32-69a97d06fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[0][\"label\"].value_counts().loc[['1. Close', '2. Know each other', \"3. Don't know each other\"]].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbebd375-7ed0-4ef9-b2c2-0f736a9f3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[1][\"label\"].value_counts().loc[['1. Close', '2. Know each other', \"3. Don't know each other\"]].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10942d27-e92a-4d8d-8f60-1dd9e2c942d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[2][\"label\"].value_counts().loc[['1. Close', '2. Know each other', \"3. Don't know each other\"]].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefac027-73fc-4994-8ec5-5e39566ca94f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c6094e0-3054-4f53-9135-01f0359861a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n",
      "step 3: init data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04045e188d8b496f8a9ebbae50819723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebac52889214647a8b6134c462cf263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a26d27b0c84e7ba24d53bb297b5344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1096\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n",
      "  Number of trainable parameters = 105247489\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:27, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.124605</td>\n",
       "      <td>0.408794</td>\n",
       "      <td>0.352994</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.671802</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.727686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.109696</td>\n",
       "      <td>0.479533</td>\n",
       "      <td>0.331203</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.574450</td>\n",
       "      <td>0.577886</td>\n",
       "      <td>0.573039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.591012</td>\n",
       "      <td>0.293598</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666497</td>\n",
       "      <td>0.666473</td>\n",
       "      <td>0.673510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.091351</td>\n",
       "      <td>0.566574</td>\n",
       "      <td>0.302243</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.564348</td>\n",
       "      <td>0.583403</td>\n",
       "      <td>0.563952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.095630</td>\n",
       "      <td>0.546271</td>\n",
       "      <td>0.309241</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.653266</td>\n",
       "      <td>0.655704</td>\n",
       "      <td>0.664194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.085599</td>\n",
       "      <td>0.593864</td>\n",
       "      <td>0.292573</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.649784</td>\n",
       "      <td>0.654401</td>\n",
       "      <td>0.648378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.097556</td>\n",
       "      <td>0.537132</td>\n",
       "      <td>0.312339</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.608116</td>\n",
       "      <td>0.623380</td>\n",
       "      <td>0.603577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.088962</td>\n",
       "      <td>0.577906</td>\n",
       "      <td>0.298265</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.613480</td>\n",
       "      <td>0.596848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.084994</td>\n",
       "      <td>0.596731</td>\n",
       "      <td>0.291538</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.641753</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.647113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.081809</td>\n",
       "      <td>0.611845</td>\n",
       "      <td>0.286023</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666949</td>\n",
       "      <td>0.674169</td>\n",
       "      <td>0.670922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.074242</td>\n",
       "      <td>0.647746</td>\n",
       "      <td>0.272475</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.645703</td>\n",
       "      <td>0.655821</td>\n",
       "      <td>0.643202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.074988</td>\n",
       "      <td>0.644207</td>\n",
       "      <td>0.273840</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.679863</td>\n",
       "      <td>0.684921</td>\n",
       "      <td>0.682827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.073892</td>\n",
       "      <td>0.649409</td>\n",
       "      <td>0.271831</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.706777</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.706637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>0.074419</td>\n",
       "      <td>0.646906</td>\n",
       "      <td>0.272799</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.681530</td>\n",
       "      <td>0.684303</td>\n",
       "      <td>0.685415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.076599</td>\n",
       "      <td>0.636566</td>\n",
       "      <td>0.276765</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.679863</td>\n",
       "      <td>0.684921</td>\n",
       "      <td>0.682827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.072975</td>\n",
       "      <td>0.653759</td>\n",
       "      <td>0.270139</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.660195</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.657695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.072821</td>\n",
       "      <td>0.654490</td>\n",
       "      <td>0.269854</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.695022</td>\n",
       "      <td>0.696609</td>\n",
       "      <td>0.697320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.071991</td>\n",
       "      <td>0.658430</td>\n",
       "      <td>0.268311</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>0.695847</td>\n",
       "      <td>0.699908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.072163</td>\n",
       "      <td>0.657614</td>\n",
       "      <td>0.268631</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.708931</td>\n",
       "      <td>0.710783</td>\n",
       "      <td>0.709225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.072640</td>\n",
       "      <td>0.655349</td>\n",
       "      <td>0.269518</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.725146</td>\n",
       "      <td>0.727041</td>\n",
       "      <td>0.723718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-69\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-138\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-138/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-1380] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-207\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-276\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-207] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-345\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-414\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-414/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-414/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-414/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-414/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-345] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-483\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-483/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-483/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-483/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-483/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-414] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-552\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-552/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-552/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-483] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-621\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-621/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-621/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-621/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-621/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-552] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-690\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-690/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-690/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-690/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-690/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-621] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-759\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-759/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-759/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-759/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-759/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-690] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-828\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-828/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-897\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-897/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-897/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-897/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-897/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-759] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-966\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-966/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-966/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-966/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-966/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-828] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-1035\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-1035/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-1035/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-1035/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-1035/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-966] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-1104\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-1104/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-1035] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-1173\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-1173/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-1173/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-1173/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-1173/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-1104] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-1242\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-1242/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-1242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-1242/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-1242/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-1173] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-1311\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-1311/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-1311/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-897] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_clse_usr/checkpoint-1380\n",
      "Configuration saved in ./Regressors/task1_clse_usr/checkpoint-1380/config.json\n",
      "Model weights saved in ./Regressors/task1_clse_usr/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_clse_usr/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_clse_usr/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_clse_usr/checkpoint-1242] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Regressors/task1_clse_usr/checkpoint-1380 (score: 0.7251461988304094).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Regressors/task1_clse_usr/checkpoint-1380\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06106751784682274, 'eval_r2_score': 0.7007920664827132, 'eval_mean_squared_error': 0.24711842834949493, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.7307692307692308, 'eval_precision': 0.7509803921568627, 'eval_recall': 0.7229390681003585, 'eval_runtime': 0.6888, 'eval_samples_per_second': 87.11, 'eval_steps_per_second': 5.807, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def closeness_label_fn(label):\n",
    "    if label == '1. Close':\n",
    "        return 1\n",
    "    elif label =='2. Know each other':\n",
    "        return 0.5\n",
    "    elif label == \"3. Don't know each other\":\n",
    "        return 0\n",
    "    elif type(label)==str:\n",
    "        assert(False)\n",
    "    \n",
    "    # [0, 0.33) =>\n",
    "    # [0.33, 0.66) =>\n",
    "    # [0.66, 1] =>\n",
    "    \n",
    "    if label > 0.66:\n",
    "        return '1. Close'\n",
    "    elif label > 0.33:\n",
    "        return '2. Know each other'\n",
    "    else:\n",
    "        return \"3. Don't know each other\"\n",
    "    \n",
    "run_exp(\"./Regressors/task1_clse_usr\", df, report=report, regressor_configs={\n",
    "    \"label\": \"close\",\n",
    "    \"not_label\": \"not_close\",\n",
    "    \"label_fn\": closeness_label_fn,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2522b5e-0d0a-4515-a821-3dbb003248c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "['1. Close' \"3. Don't know each other\" '2. Know each other']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3391a33e19504304929504e8a1278a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380e32c5b7b54232ac65a75e3f793dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005c8d27ce614fb9a74d377be2105084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"1. Close\",\n",
      "    \"1\": \"3. Don't know each other\",\n",
      "    \"2\": \"2. Know each other\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"1. Close\": 0,\n",
      "    \"2. Know each other\": 2,\n",
      "    \"3. Don't know each other\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1096\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:23, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.858159</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.529920</td>\n",
       "      <td>0.495370</td>\n",
       "      <td>0.572981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.765787</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.610147</td>\n",
       "      <td>0.612001</td>\n",
       "      <td>0.611341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.744573</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.661561</td>\n",
       "      <td>0.668779</td>\n",
       "      <td>0.657695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.760221</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.642628</td>\n",
       "      <td>0.664021</td>\n",
       "      <td>0.662928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.688059</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.724808</td>\n",
       "      <td>0.723765</td>\n",
       "      <td>0.726305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.786535</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.623859</td>\n",
       "      <td>0.631944</td>\n",
       "      <td>0.636416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.731113</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.681634</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>0.696055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>0.803274</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.654447</td>\n",
       "      <td>0.680693</td>\n",
       "      <td>0.677421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>0.842892</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.682650</td>\n",
       "      <td>0.695609</td>\n",
       "      <td>0.698643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>1.013666</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.679365</td>\n",
       "      <td>0.706539</td>\n",
       "      <td>0.706694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>1.126198</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.678499</td>\n",
       "      <td>0.680776</td>\n",
       "      <td>0.682827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>1.430417</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.659259</td>\n",
       "      <td>0.669823</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>1.304702</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.676264</td>\n",
       "      <td>0.680159</td>\n",
       "      <td>0.680239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.625600</td>\n",
       "      <td>1.361302</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.676264</td>\n",
       "      <td>0.680159</td>\n",
       "      <td>0.680239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>1.401944</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.663566</td>\n",
       "      <td>0.669848</td>\n",
       "      <td>0.668334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>1.557832</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.702767</td>\n",
       "      <td>0.705556</td>\n",
       "      <td>0.704049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>1.501654</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.689297</td>\n",
       "      <td>0.691902</td>\n",
       "      <td>0.692144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>1.509193</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.693557</td>\n",
       "      <td>0.701244</td>\n",
       "      <td>0.705372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>1.571566</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.689297</td>\n",
       "      <td>0.691902</td>\n",
       "      <td>0.692144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>1.534580</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.676264</td>\n",
       "      <td>0.680159</td>\n",
       "      <td>0.680239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-69\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-138\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-138/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-1380] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-207\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-276\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-345\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-207] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-414\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-414/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-414/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-414/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-414/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-483\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-483/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-483/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-483/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-483/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-414] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-552\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-552/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-552/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-483] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-621\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-621/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-621/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-621/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-621/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-552] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-690\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-690/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-690/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-690/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-690/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-621] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-759\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-759/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-759/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-759/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-759/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-690] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-828\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-828/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-759] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-897\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-897/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-897/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-897/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-897/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-828] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-966\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-966/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-966/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-966/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-966/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-897] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-1035\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-1035/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-1035/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-1035/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-1035/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-966] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-1104\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-1104/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-1035] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-1173\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-1173/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-1173/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-1173/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-1173/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-1104] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-1242\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-1242/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-1242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-1242/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-1242/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-1173] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-1311\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-1311/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-1311/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-1242] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_clse_usr/checkpoint-1380\n",
      "Configuration saved in ./Models/task1_clse_usr/checkpoint-1380/config.json\n",
      "Model weights saved in ./Models/task1_clse_usr/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_clse_usr/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_clse_usr/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_clse_usr/checkpoint-1311] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task1_clse_usr/checkpoint-345 (score: 0.7248083673615588).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task1_clse_usr/checkpoint-345\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6723384857177734, 'eval_accuracy': 0.75, 'eval_f1': 0.6599640125955916, 'eval_precision': 0.6772058823529411, 'eval_recall': 0.6514336917562724, 'eval_runtime': 0.7019, 'eval_samples_per_second': 85.486, 'eval_steps_per_second': 5.699, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "run_exp(\"./Models/task1_clse_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd88ae-64b2-4aff-a3b9-ce0ec483f346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf6a91-aca3-4e82-b7b4-85093fc3af53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e987fb4-dddc-4f9c-8dc5-5745d7c8fb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d026221-27c1-48f4-b01e-a9c59ef53ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1234 records from ../Task1/annotated_conersations.jsonl\n",
      "N 1098 61 61\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0. Very respect</th>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1. Respect</th>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. Normal</th>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text\n",
       "label                \n",
       "0. Very respect   248\n",
       "1. Respect        289\n",
       "2. Normal         683"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task1/annotated_conersations.jsonl\", \"authority\", skips = [\"3. Not respect\"], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aa0dd0a-99a3-481e-8ea4-4ced21a3d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a082f2c9b149d7956ed7e8fbd8795f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8b256688eb46918ac67934b2c3d074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd3e0603edb44029b05696f613d118e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"1\": \"respect\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": [\n",
      "    \"not_respect\",\n",
      "    \"respect\"\n",
      "  ],\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1098\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n",
      "  Number of trainable parameters = 105247489\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:08, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.140429</td>\n",
       "      <td>0.099854</td>\n",
       "      <td>0.374738</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.327916</td>\n",
       "      <td>0.299550</td>\n",
       "      <td>0.368627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.137052</td>\n",
       "      <td>0.121501</td>\n",
       "      <td>0.370205</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.387336</td>\n",
       "      <td>0.662142</td>\n",
       "      <td>0.426471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.139538</td>\n",
       "      <td>0.105565</td>\n",
       "      <td>0.373547</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.378151</td>\n",
       "      <td>0.346405</td>\n",
       "      <td>0.435294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.153806</td>\n",
       "      <td>0.014104</td>\n",
       "      <td>0.392181</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>0.374275</td>\n",
       "      <td>0.561111</td>\n",
       "      <td>0.428431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.143608</td>\n",
       "      <td>0.079474</td>\n",
       "      <td>0.378957</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.468989</td>\n",
       "      <td>0.597222</td>\n",
       "      <td>0.517647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.132371</td>\n",
       "      <td>0.151505</td>\n",
       "      <td>0.363828</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.261672</td>\n",
       "      <td>0.229483</td>\n",
       "      <td>0.306536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.122363</td>\n",
       "      <td>0.215651</td>\n",
       "      <td>0.349805</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.473151</td>\n",
       "      <td>0.648268</td>\n",
       "      <td>0.456536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.131772</td>\n",
       "      <td>0.155340</td>\n",
       "      <td>0.363005</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.424558</td>\n",
       "      <td>0.677019</td>\n",
       "      <td>0.442810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.119984</td>\n",
       "      <td>0.230905</td>\n",
       "      <td>0.346387</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.557239</td>\n",
       "      <td>0.585317</td>\n",
       "      <td>0.551961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.128107</td>\n",
       "      <td>0.178832</td>\n",
       "      <td>0.357921</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.493243</td>\n",
       "      <td>0.636765</td>\n",
       "      <td>0.464706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.113166</td>\n",
       "      <td>0.274610</td>\n",
       "      <td>0.336401</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.574769</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.564379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.125371</td>\n",
       "      <td>0.196375</td>\n",
       "      <td>0.354077</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.503268</td>\n",
       "      <td>0.582633</td>\n",
       "      <td>0.475490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.139510</td>\n",
       "      <td>0.105740</td>\n",
       "      <td>0.373511</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.568135</td>\n",
       "      <td>0.641414</td>\n",
       "      <td>0.551961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.121430</td>\n",
       "      <td>0.221634</td>\n",
       "      <td>0.348468</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.633726</td>\n",
       "      <td>0.711640</td>\n",
       "      <td>0.640850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.123180</td>\n",
       "      <td>0.210420</td>\n",
       "      <td>0.350969</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.585504</td>\n",
       "      <td>0.658213</td>\n",
       "      <td>0.574183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.132908</td>\n",
       "      <td>0.148062</td>\n",
       "      <td>0.364565</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.453898</td>\n",
       "      <td>0.544444</td>\n",
       "      <td>0.432680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.120517</td>\n",
       "      <td>0.227490</td>\n",
       "      <td>0.347155</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.596347</td>\n",
       "      <td>0.675485</td>\n",
       "      <td>0.599020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.126103</td>\n",
       "      <td>0.191681</td>\n",
       "      <td>0.355110</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.538249</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>0.521569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.124885</td>\n",
       "      <td>0.199487</td>\n",
       "      <td>0.353391</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.618799</td>\n",
       "      <td>0.685673</td>\n",
       "      <td>0.603595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.124449</td>\n",
       "      <td>0.202283</td>\n",
       "      <td>0.352773</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.643232</td>\n",
       "      <td>0.697646</td>\n",
       "      <td>0.633987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-69\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-138\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-138/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-1380] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-207\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-276\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-207] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-345\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-414\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-414/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-414/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-414/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-414/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-483\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-483/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-483/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-483/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-483/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-345] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-552\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-552/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-552/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-414] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-621\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-621/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-621/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-621/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-621/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-483] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-690\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-690/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-690/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-690/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-690/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-552] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-759\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-759/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-759/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-759/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-759/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-621] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-828\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-828/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-690] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-897\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-897/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-897/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-897/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-897/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-828] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-966\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-966/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-966/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-966/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-966/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-759] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-1035\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-1035/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-1035/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-1035/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-1035/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-897] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-1104\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-1104/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-1035] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-1173\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-1173/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-1173/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-1173/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-1173/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-1104] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-1242\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-1242/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-1242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-1242/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-1242/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-1173] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-1311\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-1311/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-1311/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-1242] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task1_auth_usr/checkpoint-1380\n",
      "Configuration saved in ./Regressors/task1_auth_usr/checkpoint-1380/config.json\n",
      "Model weights saved in ./Regressors/task1_auth_usr/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task1_auth_usr/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task1_auth_usr/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task1_auth_usr/checkpoint-966] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Regressors/task1_auth_usr/checkpoint-1380 (score: 0.6432317484949064).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Regressors/task1_auth_usr/checkpoint-1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16568411886692047, 'eval_r2_score': 0.04342795844278746, 'eval_mean_squared_error': 0.4070431590080261, 'eval_accuracy': 0.5081967213114754, 'eval_f1': 0.4935711631363806, 'eval_precision': 0.5576190476190477, 'eval_recall': 0.4875, 'eval_runtime': 0.7216, 'eval_samples_per_second': 84.537, 'eval_steps_per_second': 5.543, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def authority_label_fn(label):\n",
    "    if label == '0. Very respect':\n",
    "        return 1\n",
    "    elif label =='1. Respect':\n",
    "        return 0.5\n",
    "    elif label == \"2. Normal\":\n",
    "        return 0\n",
    "    elif type(label)==str:\n",
    "        assert(False)\n",
    "    \n",
    "    # [0, 0.33) =>\n",
    "    # [0.33, 0.66) =>\n",
    "    # [0.66, 1] =>\n",
    "    \n",
    "    if label > 0.66:\n",
    "        return '0. Very respect'\n",
    "    elif label > 0.33:\n",
    "        return '1. Respect'\n",
    "    else:\n",
    "        return \"2. Normal\"\n",
    "    \n",
    "run_exp(\"./Regressors/task1_auth_usr\", df, report=report, regressor_configs={\n",
    "    \"label\": \"respect\",\n",
    "    \"not_label\": \"not_respect\",\n",
    "    \"label_fn\": authority_label_fn,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "215bff50-4304-4106-bbf1-4633c5e6e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "['1. Respect' '2. Normal' '0. Very respect']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fa0d708dfa42e0bd547871f492994e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde4418898c54086b652d0e17aff79b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcd6d894abf4bfdb98eaedfc5c53723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"1. Respect\",\n",
      "    \"1\": \"2. Normal\",\n",
      "    \"2\": \"0. Very respect\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"0. Very respect\": 2,\n",
      "    \"1. Respect\": 0,\n",
      "    \"2. Normal\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1098\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:12, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.069297</td>\n",
       "      <td>0.311475</td>\n",
       "      <td>0.220588</td>\n",
       "      <td>0.365828</td>\n",
       "      <td>0.360131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.033785</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.460894</td>\n",
       "      <td>0.481611</td>\n",
       "      <td>0.463399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.000865</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.396011</td>\n",
       "      <td>0.353030</td>\n",
       "      <td>0.450980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.897825</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.512021</td>\n",
       "      <td>0.517874</td>\n",
       "      <td>0.538889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.873287</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.559140</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.589216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.928451</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.536448</td>\n",
       "      <td>0.551012</td>\n",
       "      <td>0.529739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.029924</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.489947</td>\n",
       "      <td>0.508718</td>\n",
       "      <td>0.505556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>0.983697</td>\n",
       "      <td>0.573770</td>\n",
       "      <td>0.538847</td>\n",
       "      <td>0.556999</td>\n",
       "      <td>0.544771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>1.087047</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.517507</td>\n",
       "      <td>0.534872</td>\n",
       "      <td>0.537582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>1.443468</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.545513</td>\n",
       "      <td>0.663702</td>\n",
       "      <td>0.530392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>1.642501</td>\n",
       "      <td>0.639344</td>\n",
       "      <td>0.532404</td>\n",
       "      <td>0.597884</td>\n",
       "      <td>0.516340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>1.556672</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.482595</td>\n",
       "      <td>0.505025</td>\n",
       "      <td>0.480719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>1.604725</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.502347</td>\n",
       "      <td>0.528139</td>\n",
       "      <td>0.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.880100</td>\n",
       "      <td>1.862086</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.511179</td>\n",
       "      <td>0.571869</td>\n",
       "      <td>0.519608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>2.122396</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.549271</td>\n",
       "      <td>0.634691</td>\n",
       "      <td>0.531373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>1.963285</td>\n",
       "      <td>0.540984</td>\n",
       "      <td>0.496616</td>\n",
       "      <td>0.530438</td>\n",
       "      <td>0.494771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>1.909165</td>\n",
       "      <td>0.491803</td>\n",
       "      <td>0.460562</td>\n",
       "      <td>0.484674</td>\n",
       "      <td>0.465359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>2.172906</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.496078</td>\n",
       "      <td>0.514964</td>\n",
       "      <td>0.492157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>2.032528</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.461742</td>\n",
       "      <td>0.474313</td>\n",
       "      <td>0.462745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>2.145939</td>\n",
       "      <td>0.524590</td>\n",
       "      <td>0.482606</td>\n",
       "      <td>0.502933</td>\n",
       "      <td>0.484967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-69\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-138\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-138/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-1380] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-207\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-276\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-345\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-207] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-414\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-414/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-414/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-414/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-414/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-483\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-483/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-483/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-483/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-483/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-414] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-552\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-552/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-552/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-483] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-621\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-621/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-621/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-621/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-621/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-552] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-690\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-690/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-690/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-690/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-690/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-621] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-759\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-759/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-759/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-759/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-759/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-690] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-828\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-828/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-759] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-897\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-897/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-897/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-897/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-897/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-828] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-966\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-966/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-966/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-966/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-966/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-897] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-1035\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-1035/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-1035/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-1035/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-1035/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-966] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-1104\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-1104/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-1035] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-1173\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-1173/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-1173/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-1173/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-1173/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-1104] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-1242\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-1242/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-1242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-1242/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-1242/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-1173] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-1311\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-1311/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-1311/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-1242] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task1_auth_usr/checkpoint-1380\n",
      "Configuration saved in ./Models/task1_auth_usr/checkpoint-1380/config.json\n",
      "Model weights saved in ./Models/task1_auth_usr/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task1_auth_usr/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task1_auth_usr/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task1_auth_usr/checkpoint-1311] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task1_auth_usr/checkpoint-345 (score: 0.5591397849462366).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task1_auth_usr/checkpoint-345\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0375056266784668, 'eval_accuracy': 0.47540983606557374, 'eval_f1': 0.4509031198686371, 'eval_precision': 0.4423076923076923, 'eval_recall': 0.47212301587301586, 'eval_runtime': 0.7083, 'eval_samples_per_second': 86.125, 'eval_steps_per_second': 5.648, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "run_exp(\"./Models/task1_auth_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe5ee6-da8d-446e-a14f-85ff9c617546",
   "metadata": {},
   "source": [
    "## Task2: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e2228a1-c5f6-4220-92aa-406e3f3108d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2486 records from ../Task2/annotated/annotated.jsonl\n",
      "N 1495 186 186\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1. Close</th>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. Know each other</th>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3. Don't know each other</th>\n",
       "      <td>1487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text\n",
       "label                         \n",
       "1. Close                   222\n",
       "2. Know each other         158\n",
       "3. Don't know each other  1487"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c64d0c5f-c012-481f-b316-1c6b30130b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fd3b96adf14accbdec937f3034b71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3e2136aa284eb586cecd4b70a21c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e8ebd0ff0243639a87df2df2225613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"1\": \"close\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": [\n",
      "    \"not_close\",\n",
      "    \"close\"\n",
      "  ],\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1495\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1880\n",
      "  Number of trainable parameters = 105247489\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1880' max='1880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1880/1880 14:01, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.133882</td>\n",
       "      <td>-0.063556</td>\n",
       "      <td>0.365899</td>\n",
       "      <td>0.747312</td>\n",
       "      <td>0.312443</td>\n",
       "      <td>0.294697</td>\n",
       "      <td>0.342908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.111323</td>\n",
       "      <td>0.115654</td>\n",
       "      <td>0.333651</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.313614</td>\n",
       "      <td>0.615624</td>\n",
       "      <td>0.313806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.109697</td>\n",
       "      <td>0.128568</td>\n",
       "      <td>0.331206</td>\n",
       "      <td>0.715054</td>\n",
       "      <td>0.440014</td>\n",
       "      <td>0.677027</td>\n",
       "      <td>0.447447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.105042</td>\n",
       "      <td>0.165552</td>\n",
       "      <td>0.324101</td>\n",
       "      <td>0.731183</td>\n",
       "      <td>0.428345</td>\n",
       "      <td>0.667756</td>\n",
       "      <td>0.425934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.109765</td>\n",
       "      <td>0.128033</td>\n",
       "      <td>0.331307</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.430898</td>\n",
       "      <td>0.584087</td>\n",
       "      <td>0.409693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>0.097504</td>\n",
       "      <td>0.225434</td>\n",
       "      <td>0.312256</td>\n",
       "      <td>0.704301</td>\n",
       "      <td>0.471415</td>\n",
       "      <td>0.560479</td>\n",
       "      <td>0.478960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>0.103255</td>\n",
       "      <td>0.179748</td>\n",
       "      <td>0.321333</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.572759</td>\n",
       "      <td>0.474657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>0.112339</td>\n",
       "      <td>0.107585</td>\n",
       "      <td>0.335170</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.464167</td>\n",
       "      <td>0.581491</td>\n",
       "      <td>0.445390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>0.099276</td>\n",
       "      <td>0.211352</td>\n",
       "      <td>0.315081</td>\n",
       "      <td>0.736559</td>\n",
       "      <td>0.464612</td>\n",
       "      <td>0.578886</td>\n",
       "      <td>0.450236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>0.108264</td>\n",
       "      <td>0.139956</td>\n",
       "      <td>0.329035</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.467582</td>\n",
       "      <td>0.631337</td>\n",
       "      <td>0.459149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.094272</td>\n",
       "      <td>0.251109</td>\n",
       "      <td>0.307037</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.551792</td>\n",
       "      <td>0.646343</td>\n",
       "      <td>0.553144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.090422</td>\n",
       "      <td>0.281694</td>\n",
       "      <td>0.300702</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.578348</td>\n",
       "      <td>0.668718</td>\n",
       "      <td>0.561631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.085949</td>\n",
       "      <td>0.317224</td>\n",
       "      <td>0.293171</td>\n",
       "      <td>0.768817</td>\n",
       "      <td>0.563280</td>\n",
       "      <td>0.633544</td>\n",
       "      <td>0.554539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.090298</td>\n",
       "      <td>0.282678</td>\n",
       "      <td>0.300496</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.574020</td>\n",
       "      <td>0.695200</td>\n",
       "      <td>0.564965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.088454</td>\n",
       "      <td>0.297323</td>\n",
       "      <td>0.297412</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.528349</td>\n",
       "      <td>0.663513</td>\n",
       "      <td>0.515934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.091025</td>\n",
       "      <td>0.276902</td>\n",
       "      <td>0.301703</td>\n",
       "      <td>0.779570</td>\n",
       "      <td>0.548450</td>\n",
       "      <td>0.681913</td>\n",
       "      <td>0.537329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.088594</td>\n",
       "      <td>0.296215</td>\n",
       "      <td>0.297647</td>\n",
       "      <td>0.779570</td>\n",
       "      <td>0.544986</td>\n",
       "      <td>0.674836</td>\n",
       "      <td>0.519693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.089582</td>\n",
       "      <td>0.288366</td>\n",
       "      <td>0.299302</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.552487</td>\n",
       "      <td>0.686502</td>\n",
       "      <td>0.539693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.090296</td>\n",
       "      <td>0.282693</td>\n",
       "      <td>0.300493</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.567640</td>\n",
       "      <td>0.692139</td>\n",
       "      <td>0.550662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.294577</td>\n",
       "      <td>0.297993</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.558671</td>\n",
       "      <td>0.686328</td>\n",
       "      <td>0.536359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-94\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-94/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-94/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-94/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-94/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-188\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-188/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-188/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-188/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-188/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-1880] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-282\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-282/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-282/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-282/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-282/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-94] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-376\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-376/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-376/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-376/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-376/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-188] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-470\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-470/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-470/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-470/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-470/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-376] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-564\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-564/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-564/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-564/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-564/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-282] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-658\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-658/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-658/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-658/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-658/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-470] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-752\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-752/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-752/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-752/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-752/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-564] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-846\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-846/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-846/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-846/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-846/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-752] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-940\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-940/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-940/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-940/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-940/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-846] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1034\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1034/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1034/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1034/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1034/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-658] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1128\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1128/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1128/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1128/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1128/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-940] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1222\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1222/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1222/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1222/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1222/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-1034] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1316\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1316/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1316/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1316/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1316/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-1222] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1410\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1410/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1410/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1410/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1410/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-1316] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1504\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1504/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1504/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1504/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1504/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-1410] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1598\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1598/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1598/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1598/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1598/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-1504] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1692\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1692/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1692/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1692/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1692/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-1598] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1786\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1786/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1786/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1786/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1786/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-1692] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_clse_usr/checkpoint-1880\n",
      "Configuration saved in ./Regressors/task2_clse_usr/checkpoint-1880/config.json\n",
      "Model weights saved in ./Regressors/task2_clse_usr/checkpoint-1880/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_clse_usr/checkpoint-1880/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_clse_usr/checkpoint-1880/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_clse_usr/checkpoint-1786] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Regressors/task2_clse_usr/checkpoint-1128 (score: 0.578348381037758).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Regressors/task2_clse_usr/checkpoint-1128\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1304672509431839, 'eval_r2_score': -0.03636878056182069, 'eval_mean_squared_error': 0.3612024784088135, 'eval_accuracy': 0.7741935483870968, 'eval_f1': 0.42439560439560436, 'eval_precision': 0.47797578472425095, 'eval_recall': 0.420612944102877, 'eval_runtime': 1.8704, 'eval_samples_per_second': 99.444, 'eval_steps_per_second': 6.416, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "run_exp(\"./Regressors/task2_clse_usr\", df, report=report, regressor_configs={\n",
    "    \"label\": \"close\",\n",
    "    \"not_label\": \"not_close\",\n",
    "    \"label_fn\": closeness_label_fn,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13018546-bee3-4d66-af36-8c4c58753157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "['2. Know each other' \"3. Don't know each other\" '1. Close']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01489570ef494e19948524706d7b6cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b477a8a4184fe2b095f69148c62952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155dab299f6449b3b95a42d6e3fdcd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"2. Know each other\",\n",
      "    \"1\": \"3. Don't know each other\",\n",
      "    \"2\": \"1. Close\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"1. Close\": 2,\n",
      "    \"2. Know each other\": 0,\n",
      "    \"3. Don't know each other\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1495\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1880\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1880' max='1880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1880/1880 13:56, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.017085</td>\n",
       "      <td>0.381720</td>\n",
       "      <td>0.333442</td>\n",
       "      <td>0.356964</td>\n",
       "      <td>0.434870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.078606</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.359277</td>\n",
       "      <td>0.382293</td>\n",
       "      <td>0.415485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.038162</td>\n",
       "      <td>0.747312</td>\n",
       "      <td>0.494355</td>\n",
       "      <td>0.652848</td>\n",
       "      <td>0.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.042971</td>\n",
       "      <td>0.505376</td>\n",
       "      <td>0.401805</td>\n",
       "      <td>0.410867</td>\n",
       "      <td>0.468274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.114519</td>\n",
       "      <td>0.473118</td>\n",
       "      <td>0.334397</td>\n",
       "      <td>0.397588</td>\n",
       "      <td>0.430213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.020500</td>\n",
       "      <td>1.127517</td>\n",
       "      <td>0.424731</td>\n",
       "      <td>0.326730</td>\n",
       "      <td>0.411377</td>\n",
       "      <td>0.434208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.020500</td>\n",
       "      <td>0.980263</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.503602</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>0.571253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.020500</td>\n",
       "      <td>1.004971</td>\n",
       "      <td>0.521505</td>\n",
       "      <td>0.433274</td>\n",
       "      <td>0.449022</td>\n",
       "      <td>0.529243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.020500</td>\n",
       "      <td>1.025720</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.539372</td>\n",
       "      <td>0.544749</td>\n",
       "      <td>0.536052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.020500</td>\n",
       "      <td>0.855173</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.507720</td>\n",
       "      <td>0.502529</td>\n",
       "      <td>0.585130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.874694</td>\n",
       "      <td>0.569892</td>\n",
       "      <td>0.479442</td>\n",
       "      <td>0.486941</td>\n",
       "      <td>0.583428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.935560</td>\n",
       "      <td>0.629032</td>\n",
       "      <td>0.501116</td>\n",
       "      <td>0.517641</td>\n",
       "      <td>0.557920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.829672</td>\n",
       "      <td>0.704301</td>\n",
       "      <td>0.575903</td>\n",
       "      <td>0.552485</td>\n",
       "      <td>0.639196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.907390</td>\n",
       "      <td>0.752688</td>\n",
       "      <td>0.614997</td>\n",
       "      <td>0.589376</td>\n",
       "      <td>0.660473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>0.979843</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.607530</td>\n",
       "      <td>0.615588</td>\n",
       "      <td>0.601206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>1.088531</td>\n",
       "      <td>0.747312</td>\n",
       "      <td>0.554632</td>\n",
       "      <td>0.552381</td>\n",
       "      <td>0.557021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>1.072031</td>\n",
       "      <td>0.779570</td>\n",
       "      <td>0.608706</td>\n",
       "      <td>0.601401</td>\n",
       "      <td>0.618416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>1.121409</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.574488</td>\n",
       "      <td>0.567532</td>\n",
       "      <td>0.583688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>1.098378</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.588834</td>\n",
       "      <td>0.576174</td>\n",
       "      <td>0.607991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.581100</td>\n",
       "      <td>1.127116</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.585941</td>\n",
       "      <td>0.578744</td>\n",
       "      <td>0.597021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-94\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-94/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-94/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-94/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-94/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-188\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-188/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-188/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-188/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-188/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-1880] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-282\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-282/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-282/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-282/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-282/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-94] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-376\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-376/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-376/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-376/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-376/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-188] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-470\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-470/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-470/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-470/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-470/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-376] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-564\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-564/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-564/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-564/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-564/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-470] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-658\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-658/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-658/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-658/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-658/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-282] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-752\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-752/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-752/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-752/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-752/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-564] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-846\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-846/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-846/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-846/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-846/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-658] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-940\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-940/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-940/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-940/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-940/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-752] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1034\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1034/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1034/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1034/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1034/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-940] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1128\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1128/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1128/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1128/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1128/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-1034] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1222\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1222/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1222/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1222/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1222/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-846] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1316\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1316/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1316/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1316/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1316/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-1128] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1410\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1410/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1410/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1410/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1410/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-1222] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1504\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1504/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1504/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1504/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1504/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-1410] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1598\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1598/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1598/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1598/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1598/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-1504] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1692\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1692/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1692/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1692/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1692/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-1598] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1786\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1786/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1786/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1786/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1786/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-1692] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_clse_usr/checkpoint-1880\n",
      "Configuration saved in ./Models/task2_clse_usr/checkpoint-1880/config.json\n",
      "Model weights saved in ./Models/task2_clse_usr/checkpoint-1880/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_clse_usr/checkpoint-1880/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_clse_usr/checkpoint-1880/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_clse_usr/checkpoint-1786] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task2_clse_usr/checkpoint-1316 (score: 0.6149966155184085).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 186\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task2_clse_usr/checkpoint-1316\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8854197859764099, 'eval_accuracy': 0.7473118279569892, 'eval_f1': 0.54585326953748, 'eval_precision': 0.5300235036543354, 'eval_recall': 0.5819761893587396, 'eval_runtime': 1.8457, 'eval_samples_per_second': 100.772, 'eval_steps_per_second': 6.501, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "run_exp(\"./Models/task2_clse_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd0df34-b578-4f83-989a-1d09d96c9e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1981804-f588-4959-aa36-f849f011cec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4aec330-7005-4c77-b427-1489374cc63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2486 records from ../Task2/annotated/annotated.jsonl\n",
      "N 1876 234 234\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1. Respect</th>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. Normal</th>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3. Not respect</th>\n",
       "      <td>364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text\n",
       "label               \n",
       "1. Respect       319\n",
       "2. Normal       1661\n",
       "3. Not respect   364"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_task2_conver(\"../Task2/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "132ac526-14cb-4357-851a-478903395257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70802788acd342b89504c77fa5fba9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2692b9cc98449c8ee18671ca5e15f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68300ff21e3d4f0f923b3995ba43fbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"1\": \"respect\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": [\n",
      "    \"not_respect\",\n",
      "    \"respect\"\n",
      "  ],\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1876\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2360\n",
      "  Number of trainable parameters = 105247489\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2360' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2360/2360 17:30, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.057401</td>\n",
       "      <td>0.278662</td>\n",
       "      <td>0.239585</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.601399</td>\n",
       "      <td>0.591984</td>\n",
       "      <td>0.635712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.048853</td>\n",
       "      <td>0.386083</td>\n",
       "      <td>0.221027</td>\n",
       "      <td>0.705128</td>\n",
       "      <td>0.633363</td>\n",
       "      <td>0.629526</td>\n",
       "      <td>0.683342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.041869</td>\n",
       "      <td>0.473841</td>\n",
       "      <td>0.204620</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.702352</td>\n",
       "      <td>0.679728</td>\n",
       "      <td>0.737997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.041747</td>\n",
       "      <td>0.475382</td>\n",
       "      <td>0.204321</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.702102</td>\n",
       "      <td>0.679753</td>\n",
       "      <td>0.743327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.041258</td>\n",
       "      <td>0.481532</td>\n",
       "      <td>0.203119</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.751808</td>\n",
       "      <td>0.718938</td>\n",
       "      <td>0.805342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.039422</td>\n",
       "      <td>0.504600</td>\n",
       "      <td>0.198549</td>\n",
       "      <td>0.764957</td>\n",
       "      <td>0.750210</td>\n",
       "      <td>0.739035</td>\n",
       "      <td>0.786540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.042149</td>\n",
       "      <td>0.470325</td>\n",
       "      <td>0.205303</td>\n",
       "      <td>0.747863</td>\n",
       "      <td>0.677238</td>\n",
       "      <td>0.676239</td>\n",
       "      <td>0.682663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.063887</td>\n",
       "      <td>0.197155</td>\n",
       "      <td>0.252759</td>\n",
       "      <td>0.619658</td>\n",
       "      <td>0.551315</td>\n",
       "      <td>0.628860</td>\n",
       "      <td>0.605142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.039107</td>\n",
       "      <td>0.508550</td>\n",
       "      <td>0.197756</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.747337</td>\n",
       "      <td>0.732637</td>\n",
       "      <td>0.767282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.040081</td>\n",
       "      <td>0.496322</td>\n",
       "      <td>0.200201</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.679220</td>\n",
       "      <td>0.679959</td>\n",
       "      <td>0.727189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.038565</td>\n",
       "      <td>0.515361</td>\n",
       "      <td>0.196381</td>\n",
       "      <td>0.782051</td>\n",
       "      <td>0.731596</td>\n",
       "      <td>0.715982</td>\n",
       "      <td>0.755337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.047504</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.217954</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.683483</td>\n",
       "      <td>0.705300</td>\n",
       "      <td>0.735509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.036250</td>\n",
       "      <td>0.544459</td>\n",
       "      <td>0.190394</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.738272</td>\n",
       "      <td>0.728048</td>\n",
       "      <td>0.756539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.036165</td>\n",
       "      <td>0.545529</td>\n",
       "      <td>0.190171</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.760189</td>\n",
       "      <td>0.767021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.034066</td>\n",
       "      <td>0.571909</td>\n",
       "      <td>0.184569</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.748752</td>\n",
       "      <td>0.730714</td>\n",
       "      <td>0.770841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.036770</td>\n",
       "      <td>0.537923</td>\n",
       "      <td>0.191755</td>\n",
       "      <td>0.799145</td>\n",
       "      <td>0.750303</td>\n",
       "      <td>0.737336</td>\n",
       "      <td>0.774708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.034407</td>\n",
       "      <td>0.567618</td>\n",
       "      <td>0.185492</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.738019</td>\n",
       "      <td>0.724183</td>\n",
       "      <td>0.754769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.035017</td>\n",
       "      <td>0.559955</td>\n",
       "      <td>0.187128</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.742872</td>\n",
       "      <td>0.726305</td>\n",
       "      <td>0.767850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.538764</td>\n",
       "      <td>0.191581</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.742237</td>\n",
       "      <td>0.734557</td>\n",
       "      <td>0.764291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.035947</td>\n",
       "      <td>0.548263</td>\n",
       "      <td>0.189598</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.742964</td>\n",
       "      <td>0.733469</td>\n",
       "      <td>0.764291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-118\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-118/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-118/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-118/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-118/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-1652] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-236\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-236/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-236/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-236/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-236/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-1770] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-354\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-354/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-354/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-354/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-354/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-118] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-472\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-472/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-472/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-472/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-472/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-236] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-590\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-590/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-590/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-590/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-590/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-354] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-708\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-708/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-708/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-708/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-708/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-472] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-826\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-826/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-826/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-826/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-826/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-708] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-944\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-944/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-944/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-944/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-944/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-826] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-1062\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-1062/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-1062/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-1062/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-1062/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-944] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-1180\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-1180/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-1180/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-1180/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-1180/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-1062] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-1298\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-1298/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-1298/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-1298/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-1298/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-1180] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-1416\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-1416/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-1416/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-1416/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-1416/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-1298] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-1534\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-1534/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-1534/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-1534/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-1534/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-1416] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-1652\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-1652/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-1652/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-1652/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-1652/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-590] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-1770\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-1770/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-1770/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-1770/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-1770/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-1534] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-1888\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-1888/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-1888/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-1888/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-1888/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-1770] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-2006\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-2006/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-2006/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-2006/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-2006/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-1888] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-2124\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-2124/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-2124/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-2124/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-2124/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-2006] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-2242\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-2242/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-2242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-2242/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-2242/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-2124] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task2_auth_usr/checkpoint-2360\n",
      "Configuration saved in ./Regressors/task2_auth_usr/checkpoint-2360/config.json\n",
      "Model weights saved in ./Regressors/task2_auth_usr/checkpoint-2360/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task2_auth_usr/checkpoint-2360/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task2_auth_usr/checkpoint-2360/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task2_auth_usr/checkpoint-2242] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Regressors/task2_auth_usr/checkpoint-1652 (score: 0.7605630465358039).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Regressors/task2_auth_usr/checkpoint-1652\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03201077878475189, 'eval_r2_score': 0.5525477856687152, 'eval_mean_squared_error': 0.17891556024551392, 'eval_accuracy': 0.7905982905982906, 'eval_f1': 0.7337138345303059, 'eval_precision': 0.6999205876513798, 'eval_recall': 0.7869849586541203, 'eval_runtime': 2.3902, 'eval_samples_per_second': 97.9, 'eval_steps_per_second': 6.276, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "def authority2_label_fn(label):\n",
    "    if label == '1. Respect':\n",
    "        return 1\n",
    "    elif label =='2. Normal':\n",
    "        return 0.5\n",
    "    elif label == \"3. Not respect\":\n",
    "        return 0\n",
    "    elif type(label)==str:\n",
    "        assert(False)\n",
    "    \n",
    "    if label > 0.66:\n",
    "        return '1. Respect'\n",
    "    elif label > 0.33:\n",
    "        return '2. Normal'\n",
    "    else:\n",
    "        return \"3. Not respect\"\n",
    "\n",
    "run_exp(\"./Regressors/task2_auth_usr\", df, report=report, regressor_configs={\n",
    "    \"label\": \"respect\",\n",
    "    \"not_label\": \"not_respect\",\n",
    "    \"label_fn\": authority2_label_fn,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48bb4fb1-6c65-4272-aee6-82c45ff31f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "['2. Normal' '3. Not respect' '1. Respect']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2da009008a44897acabd24c7ca1bac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095dae4333e74518ada41dd0a9f9412a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76952ccb51c045f6b5759b2fb7c634ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"2. Normal\",\n",
      "    \"1\": \"3. Not respect\",\n",
      "    \"2\": \"1. Respect\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"1. Respect\": 2,\n",
      "    \"2. Normal\": 0,\n",
      "    \"3. Not respect\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1876\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2360\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2360' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2360/2360 17:29, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.587866</td>\n",
       "      <td>0.773504</td>\n",
       "      <td>0.723465</td>\n",
       "      <td>0.705228</td>\n",
       "      <td>0.754135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596551</td>\n",
       "      <td>0.790598</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.727321</td>\n",
       "      <td>0.770515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595517</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.773542</td>\n",
       "      <td>0.764795</td>\n",
       "      <td>0.783420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.662912</td>\n",
       "      <td>0.811966</td>\n",
       "      <td>0.777886</td>\n",
       "      <td>0.747720</td>\n",
       "      <td>0.825915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>1.589213</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.760353</td>\n",
       "      <td>0.839275</td>\n",
       "      <td>0.724265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>1.182574</td>\n",
       "      <td>0.816239</td>\n",
       "      <td>0.775388</td>\n",
       "      <td>0.759325</td>\n",
       "      <td>0.794731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>1.413458</td>\n",
       "      <td>0.824786</td>\n",
       "      <td>0.770465</td>\n",
       "      <td>0.784708</td>\n",
       "      <td>0.764990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>1.693525</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.771700</td>\n",
       "      <td>0.814347</td>\n",
       "      <td>0.749552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.364664</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>0.802448</td>\n",
       "      <td>0.798160</td>\n",
       "      <td>0.807309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.689437</td>\n",
       "      <td>0.829060</td>\n",
       "      <td>0.768159</td>\n",
       "      <td>0.801699</td>\n",
       "      <td>0.755776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.543553</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.786675</td>\n",
       "      <td>0.795579</td>\n",
       "      <td>0.780494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>1.862349</td>\n",
       "      <td>0.837607</td>\n",
       "      <td>0.781918</td>\n",
       "      <td>0.812883</td>\n",
       "      <td>0.765624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>1.933496</td>\n",
       "      <td>0.837607</td>\n",
       "      <td>0.778977</td>\n",
       "      <td>0.818438</td>\n",
       "      <td>0.757304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>1.891892</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>0.787751</td>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.773376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>1.914310</td>\n",
       "      <td>0.837607</td>\n",
       "      <td>0.780640</td>\n",
       "      <td>0.813674</td>\n",
       "      <td>0.765624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>1.857700</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>0.787751</td>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.773376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>1.898002</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>0.786084</td>\n",
       "      <td>0.822384</td>\n",
       "      <td>0.765056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>1.876536</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>0.788985</td>\n",
       "      <td>0.817066</td>\n",
       "      <td>0.773376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>1.848439</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>0.787751</td>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.773376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>1.857163</td>\n",
       "      <td>0.841880</td>\n",
       "      <td>0.787751</td>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.773376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-118\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-118/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-118/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-118/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-118/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-236\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-236/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-236/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-236/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-236/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-515] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-354\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-354/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-354/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-354/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-354/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-118] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-472\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-472/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-472/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-472/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-472/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-236] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-590\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-590/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-590/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-590/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-590/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-354] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-708\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-708/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-708/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-708/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-708/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-590] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-826\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-826/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-826/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-826/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-826/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-708] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-944\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-944/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-944/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-944/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-944/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-826] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-1062\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-1062/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-1062/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-1062/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-1062/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-472] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-1180\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-1180/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-1180/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-1180/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-1180/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-944] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-1298\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-1298/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-1298/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-1298/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-1298/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-1180] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-1416\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-1416/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-1416/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-1416/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-1416/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-1298] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-1534\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-1534/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-1534/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-1534/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-1534/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-1416] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-1652\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-1652/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-1652/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-1652/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-1652/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-1534] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-1770\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-1770/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-1770/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-1770/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-1770/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-1652] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-1888\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-1888/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-1888/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-1888/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-1888/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-1770] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-2006\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-2006/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-2006/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-2006/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-2006/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-1888] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-2124\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-2124/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-2124/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-2124/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-2124/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-2006] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-2242\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-2242/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-2242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-2242/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-2242/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-2124] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task2_auth_usr/checkpoint-2360\n",
      "Configuration saved in ./Models/task2_auth_usr/checkpoint-2360/config.json\n",
      "Model weights saved in ./Models/task2_auth_usr/checkpoint-2360/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task2_auth_usr/checkpoint-2360/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task2_auth_usr/checkpoint-2360/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task2_auth_usr/checkpoint-2242] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task2_auth_usr/checkpoint-1062 (score: 0.8024483094069289).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 234\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task2_auth_usr/checkpoint-1062\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2533892393112183, 'eval_accuracy': 0.8247863247863247, 'eval_f1': 0.7676337930756904, 'eval_precision': 0.7615776669130327, 'eval_recall': 0.7803696179070431, 'eval_runtime': 2.3852, 'eval_samples_per_second': 98.104, 'eval_steps_per_second': 6.289, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "run_exp(\"./Models/task2_auth_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbe21a-0a9e-4b20-86b9-72067c928ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89a668-3680-4d2d-82ac-4b705dac628a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccd70a91-a596-4118-834c-ed6567ac8c53",
   "metadata": {},
   "source": [
    "## Task3: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c1a0dbb-4004-44de-ac45-5085086dbd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ../Task3/annotated/annotated.jsonl\n",
      "N 1090 60 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1. Close</th>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. Know each other</th>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3. Don't know each other</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          text\n",
       "label                         \n",
       "1. Close                   462\n",
       "2. Know each other         696\n",
       "3. Don't know each other    52"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"closeness\", skips = [\"4. Don't like each other\"], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edb5a5c8-9b01-452d-863b-66be4c056b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706a5da220394baf99761a8f218658d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73dd7d13cb74c28a89d57dbaab52fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f270019ce5410bb1c1cfe5237ab2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"1\": \"close\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": [\n",
      "    \"not_close\",\n",
      "    \"close\"\n",
      "  ],\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1090\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n",
      "  Number of trainable parameters = 105247489\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:19, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.150253</td>\n",
       "      <td>-0.809065</td>\n",
       "      <td>0.387625</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.294074</td>\n",
       "      <td>0.442593</td>\n",
       "      <td>0.310833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.062383</td>\n",
       "      <td>0.248898</td>\n",
       "      <td>0.249766</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.505530</td>\n",
       "      <td>0.519231</td>\n",
       "      <td>0.498750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.083970</td>\n",
       "      <td>-0.011008</td>\n",
       "      <td>0.289776</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.498506</td>\n",
       "      <td>0.496296</td>\n",
       "      <td>0.510694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.114989</td>\n",
       "      <td>-0.384482</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.086265</td>\n",
       "      <td>-0.038646</td>\n",
       "      <td>0.293710</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.218615</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.093363</td>\n",
       "      <td>-0.124098</td>\n",
       "      <td>0.305553</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.078330</td>\n",
       "      <td>0.056896</td>\n",
       "      <td>0.279875</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.084721</td>\n",
       "      <td>-0.020058</td>\n",
       "      <td>0.291069</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.083792</td>\n",
       "      <td>-0.008863</td>\n",
       "      <td>0.289468</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.088217</td>\n",
       "      <td>-0.062139</td>\n",
       "      <td>0.297013</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.116768</td>\n",
       "      <td>-0.405908</td>\n",
       "      <td>0.341714</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.083456</td>\n",
       "      <td>-0.004819</td>\n",
       "      <td>0.288887</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.085320</td>\n",
       "      <td>-0.027265</td>\n",
       "      <td>0.292096</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.086730</td>\n",
       "      <td>-0.044243</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.088695</td>\n",
       "      <td>-0.067899</td>\n",
       "      <td>0.297817</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.084640</td>\n",
       "      <td>-0.019079</td>\n",
       "      <td>0.290930</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.087589</td>\n",
       "      <td>-0.054584</td>\n",
       "      <td>0.295955</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.085518</td>\n",
       "      <td>-0.029649</td>\n",
       "      <td>0.292435</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.084246</td>\n",
       "      <td>-0.014339</td>\n",
       "      <td>0.290252</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.088800</td>\n",
       "      <td>0.085072</td>\n",
       "      <td>-0.024275</td>\n",
       "      <td>0.291671</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.231884</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-69\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-138\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-138/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-207\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-276\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-207] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-345\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-414\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-414/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-414/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-414/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-414/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-345] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-483\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-483/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-483/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-483/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-483/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-414] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-552\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-552/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-552/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-483] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-621\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-621/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-621/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-621/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-621/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-552] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-690\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-690/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-690/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-690/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-690/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-621] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-759\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-759/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-759/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-759/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-759/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-690] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-828\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-828/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-759] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-897\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-897/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-897/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-897/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-897/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-828] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-966\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-966/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-966/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-966/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-966/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-897] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-1035\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-1035/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-1035/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-1035/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-1035/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-966] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-1104\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-1104/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-1035] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-1173\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-1173/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-1173/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-1173/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-1173/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-1104] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-1242\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-1242/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-1242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-1242/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-1242/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-1173] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-1311\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-1311/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-1311/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-1242] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "/home/imtk/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./Regressors/task3_clse_usr/checkpoint-1380\n",
      "Configuration saved in ./Regressors/task3_clse_usr/checkpoint-1380/config.json\n",
      "Model weights saved in ./Regressors/task3_clse_usr/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_clse_usr/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_clse_usr/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_clse_usr/checkpoint-1311] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Regressors/task3_clse_usr/checkpoint-138 (score: 0.5055302537410541).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Regressors/task3_clse_usr/checkpoint-138\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.042262692004442215, 'eval_r2_score': 0.44217882703627065, 'eval_mean_squared_error': 0.20557895302772522, 'eval_accuracy': 0.7, 'eval_f1': 0.5786243386243387, 'eval_precision': 0.5782051282051283, 'eval_recall': 0.6183183183183183, 'eval_runtime': 0.6676, 'eval_samples_per_second': 89.875, 'eval_steps_per_second': 5.992, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "run_exp(\"./Regressors/task3_clse_usr\", df, report=report, regressor_configs={\n",
    "    \"label\": \"close\",\n",
    "    \"not_label\": \"not_close\",\n",
    "    \"label_fn\": closeness_label_fn,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "680181f6-7abf-426c-9f1c-8795b4494894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "['2. Know each other' \"3. Don't know each other\" '1. Close']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b8b23028a344c58e71cd49f1c4deb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d955fe6667482b9dfd6e5f1fd36eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bf16758d264b7e8cad2ec3b2145fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"2. Know each other\",\n",
      "    \"1\": \"3. Don't know each other\",\n",
      "    \"2\": \"1. Close\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"1. Close\": 2,\n",
      "    \"2. Know each other\": 0,\n",
      "    \"3. Don't know each other\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n",
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1090\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:18, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.754529</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.551553</td>\n",
       "      <td>0.555973</td>\n",
       "      <td>0.555417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.531545</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.745177</td>\n",
       "      <td>0.729249</td>\n",
       "      <td>0.770139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.585955</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.770333</td>\n",
       "      <td>0.776768</td>\n",
       "      <td>0.767222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.709013</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.678169</td>\n",
       "      <td>0.709150</td>\n",
       "      <td>0.659028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.730013</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.782828</td>\n",
       "      <td>0.786587</td>\n",
       "      <td>0.780556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.726652</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.795683</td>\n",
       "      <td>0.794882</td>\n",
       "      <td>0.799722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.026091</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.729515</td>\n",
       "      <td>0.666528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>0.843036</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.795116</td>\n",
       "      <td>0.796717</td>\n",
       "      <td>0.793889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>1.051181</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.770333</td>\n",
       "      <td>0.776768</td>\n",
       "      <td>0.767222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>1.253408</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.665879</td>\n",
       "      <td>0.699172</td>\n",
       "      <td>0.645694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>1.332875</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.678169</td>\n",
       "      <td>0.709150</td>\n",
       "      <td>0.659028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>1.304343</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.771254</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.770139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>1.350816</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.783472</td>\n",
       "      <td>0.783472</td>\n",
       "      <td>0.783472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>1.514754</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.689718</td>\n",
       "      <td>0.723188</td>\n",
       "      <td>0.669444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>1.612986</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.689718</td>\n",
       "      <td>0.723188</td>\n",
       "      <td>0.669444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>1.740827</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.690256</td>\n",
       "      <td>0.719394</td>\n",
       "      <td>0.672361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>1.658556</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.690256</td>\n",
       "      <td>0.719394</td>\n",
       "      <td>0.672361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>1.668173</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.690256</td>\n",
       "      <td>0.719394</td>\n",
       "      <td>0.672361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>1.702099</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.690256</td>\n",
       "      <td>0.719394</td>\n",
       "      <td>0.672361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>1.706340</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.690256</td>\n",
       "      <td>0.719394</td>\n",
       "      <td>0.672361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-69\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-138\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-138/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-207\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-276\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-345\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-207] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-414\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-414/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-414/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-414/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-414/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-483\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-483/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-483/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-483/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-483/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-345] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-552\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-552/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-552/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-483] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-621\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-621/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-621/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-621/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-621/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-552] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-690\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-690/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-690/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-690/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-690/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-621] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-759\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-759/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-759/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-759/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-759/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-690] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-828\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-828/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-759] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-897\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-897/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-897/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-897/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-897/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-828] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-966\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-966/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-966/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-966/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-966/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-897] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-1035\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-1035/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-1035/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-1035/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-1035/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-966] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-1104\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-1104/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-1035] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-1173\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-1173/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-1173/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-1173/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-1173/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-1104] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-1242\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-1242/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-1242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-1242/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-1242/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-1173] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-1311\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-1311/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-1311/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-1242] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_clse_usr/checkpoint-1380\n",
      "Configuration saved in ./Models/task3_clse_usr/checkpoint-1380/config.json\n",
      "Model weights saved in ./Models/task3_clse_usr/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_clse_usr/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_clse_usr/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_clse_usr/checkpoint-1311] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task3_clse_usr/checkpoint-414 (score: 0.7956834037873319).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task3_clse_usr/checkpoint-414\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4971728324890137, 'eval_accuracy': 0.8333333333333334, 'eval_f1': 0.5735430157261795, 'eval_precision': 0.5594135802469136, 'eval_recall': 0.6036036036036037, 'eval_runtime': 0.676, 'eval_samples_per_second': 88.762, 'eval_steps_per_second': 5.917, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "run_exp(\"./Models/task3_clse_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808bccbc-f452-4de2-9156-3635717dc5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "675c4a41-36fd-406e-bca1-826c580d6c0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ../Task3/annotated/annotated.jsonl\n",
      "N 1099 61 61\n",
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n",
      "['2. Normal' '1. Respect' '3. Not respect']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947d6b8d17694cc09a2ea47d1d4c539b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264528deb4a24e4eae1a81fd1bda1c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703b44148f5443a5a9ad127eaa01e469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"2. Normal\",\n",
      "    \"1\": \"1. Respect\",\n",
      "    \"2\": \"3. Not respect\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"1. Respect\": 1,\n",
      "    \"2. Normal\": 0,\n",
      "    \"3. Not respect\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1099\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n",
      "  Number of trainable parameters = 105249027\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:14, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.795773</td>\n",
       "      <td>0.885246</td>\n",
       "      <td>0.709330</td>\n",
       "      <td>0.726424</td>\n",
       "      <td>0.702614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.774079</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.665107</td>\n",
       "      <td>0.629293</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698042</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.591258</td>\n",
       "      <td>0.615079</td>\n",
       "      <td>0.643791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.013291</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.611772</td>\n",
       "      <td>0.627778</td>\n",
       "      <td>0.620915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.863391</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.619062</td>\n",
       "      <td>0.599473</td>\n",
       "      <td>0.669935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.393473</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.541474</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.544118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.427778</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.579060</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.557190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>2.085276</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.434119</td>\n",
       "      <td>0.413462</td>\n",
       "      <td>0.460784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>2.234012</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.434119</td>\n",
       "      <td>0.413462</td>\n",
       "      <td>0.460784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>2.136647</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.579060</td>\n",
       "      <td>0.628931</td>\n",
       "      <td>0.557190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>2.956556</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.432165</td>\n",
       "      <td>0.467320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>2.973042</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.432165</td>\n",
       "      <td>0.467320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>2.473794</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.420915</td>\n",
       "      <td>0.398693</td>\n",
       "      <td>0.454248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.445000</td>\n",
       "      <td>2.877105</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.397647</td>\n",
       "      <td>0.376623</td>\n",
       "      <td>0.441176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>3.296050</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.465079</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.473856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>3.156649</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.465079</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.473856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>3.070102</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.432165</td>\n",
       "      <td>0.467320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>3.205451</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.465079</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.473856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>3.202195</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.448718</td>\n",
       "      <td>0.432165</td>\n",
       "      <td>0.467320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>3.241958</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.465079</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.473856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-69\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-138\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-138/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-207\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-276\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-207] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-345\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-414\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-414/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-414/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-414/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-414/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-345] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-483\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-483/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-483/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-483/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-483/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-414] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-552\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-552/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-552/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-483] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-621\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-621/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-621/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-621/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-621/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-552] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-690\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-690/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-690/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-690/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-690/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-621] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-759\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-759/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-759/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-759/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-759/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-690] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-828\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-828/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-759] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-897\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-897/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-897/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-897/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-897/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-828] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-966\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-966/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-966/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-966/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-966/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-897] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-1035\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-1035/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-1035/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-1035/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-1035/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-966] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-1104\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-1104/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-1035] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-1173\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-1173/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-1173/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-1173/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-1173/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-1104] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-1242\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-1242/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-1242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-1242/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-1242/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-1173] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-1311\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-1311/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-1311/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-1242] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Models/task3_auth_usr/checkpoint-1380\n",
      "Configuration saved in ./Models/task3_auth_usr/checkpoint-1380/config.json\n",
      "Model weights saved in ./Models/task3_auth_usr/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./Models/task3_auth_usr/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./Models/task3_auth_usr/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [Models/task3_auth_usr/checkpoint-1311] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Models/task3_auth_usr/checkpoint-69 (score: 0.7093298858004741).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Models/task3_auth_usr/checkpoint-69\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.584971010684967, 'eval_accuracy': 0.8852459016393442, 'eval_f1': 0.7664720600500416, 'eval_precision': 0.7757575757575758, 'eval_recall': 0.7592592592592592, 'eval_runtime': 0.7069, 'eval_samples_per_second': 86.292, 'eval_steps_per_second': 5.658, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "df = get_task1_conver(\"../Task3/annotated/annotated.jsonl\", \"authority\", skips = [], only_user=True)\n",
    "# print(df[0][\"text\"][0])\n",
    "pd.concat(df).groupby(\"label\").count()\n",
    "run_exp(\"./Models/task3_auth_usr\", df, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b9708a6-d184-457b-8845-e40f34e7c749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START\n",
      "step 1: load data\n",
      "step 2: load tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "Assigning ['usr', 'sys', 'rep'] to the additional_special_tokens key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3: init data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6efe58220b41a6b02a6c5432059c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5e737f0d0f42489ac6777654bc0af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f214c6e9274515b39bcbe81631f2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/config.json\n",
      "Model config CamembertConfig {\n",
      "  \"_name_or_path\": \"airesearch/wangchanberta-base-att-spm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"1\": \"respect\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": [\n",
      "    \"not_respect\",\n",
      "    \"respect\"\n",
      "  ],\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"camembert\",\n",
      "  \"num_attention_head\": 12,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 25005\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/imtk/.cache/huggingface/hub/models--airesearch--wangchanberta-base-att-spm-uncased/snapshots/b81d38df6b4755dbedec0bfea863c9956cbb963e/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4: load model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased were not used when initializing CamembertForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/imtk/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1099\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1380\n",
      "  Number of trainable parameters = 105247489\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5: fine-tune\n",
      "['labels']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1380' max='1380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1380/1380 11:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>Mean Squared Error</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.085439</td>\n",
       "      <td>-1.098467</td>\n",
       "      <td>0.292299</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.385193</td>\n",
       "      <td>0.539367</td>\n",
       "      <td>0.521242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.047561</td>\n",
       "      <td>-0.168145</td>\n",
       "      <td>0.218084</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.522751</td>\n",
       "      <td>0.505280</td>\n",
       "      <td>0.552288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.046304</td>\n",
       "      <td>-0.137272</td>\n",
       "      <td>0.215183</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.569164</td>\n",
       "      <td>0.610806</td>\n",
       "      <td>0.557190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.051438</td>\n",
       "      <td>-0.263368</td>\n",
       "      <td>0.226799</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>0.624143</td>\n",
       "      <td>0.616190</td>\n",
       "      <td>0.633987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.079456</td>\n",
       "      <td>-0.951516</td>\n",
       "      <td>0.281879</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.415429</td>\n",
       "      <td>0.393333</td>\n",
       "      <td>0.454248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.060507</td>\n",
       "      <td>-0.486114</td>\n",
       "      <td>0.245982</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.651512</td>\n",
       "      <td>0.672269</td>\n",
       "      <td>0.640523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.044141</td>\n",
       "      <td>-0.084143</td>\n",
       "      <td>0.210097</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.529444</td>\n",
       "      <td>0.565986</td>\n",
       "      <td>0.537582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.054235</td>\n",
       "      <td>-0.332080</td>\n",
       "      <td>0.232885</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.498161</td>\n",
       "      <td>0.540691</td>\n",
       "      <td>0.517974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.059654</td>\n",
       "      <td>-0.465172</td>\n",
       "      <td>0.244242</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.541474</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.544118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.045618</td>\n",
       "      <td>-0.120429</td>\n",
       "      <td>0.213584</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.600640</td>\n",
       "      <td>0.618096</td>\n",
       "      <td>0.614379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.055376</td>\n",
       "      <td>-0.360091</td>\n",
       "      <td>0.235321</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.481514</td>\n",
       "      <td>0.536842</td>\n",
       "      <td>0.540850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.045972</td>\n",
       "      <td>-0.129130</td>\n",
       "      <td>0.214412</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.498161</td>\n",
       "      <td>0.540691</td>\n",
       "      <td>0.517974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.060078</td>\n",
       "      <td>-0.475580</td>\n",
       "      <td>0.245108</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.520934</td>\n",
       "      <td>0.514706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.057249</td>\n",
       "      <td>-0.406085</td>\n",
       "      <td>0.239267</td>\n",
       "      <td>0.622951</td>\n",
       "      <td>0.514550</td>\n",
       "      <td>0.567139</td>\n",
       "      <td>0.549020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.050667</td>\n",
       "      <td>-0.244429</td>\n",
       "      <td>0.225093</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.534392</td>\n",
       "      <td>0.511438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.043979</td>\n",
       "      <td>-0.080180</td>\n",
       "      <td>0.209713</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.611772</td>\n",
       "      <td>0.627778</td>\n",
       "      <td>0.620915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.052098</td>\n",
       "      <td>-0.279591</td>\n",
       "      <td>0.228251</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.498161</td>\n",
       "      <td>0.540691</td>\n",
       "      <td>0.517974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.048676</td>\n",
       "      <td>-0.195541</td>\n",
       "      <td>0.220627</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.507937</td>\n",
       "      <td>0.547872</td>\n",
       "      <td>0.524510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.045892</td>\n",
       "      <td>-0.127155</td>\n",
       "      <td>0.214224</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.590247</td>\n",
       "      <td>0.609903</td>\n",
       "      <td>0.607843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.047031</td>\n",
       "      <td>-0.155130</td>\n",
       "      <td>0.216866</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.590247</td>\n",
       "      <td>0.609903</td>\n",
       "      <td>0.607843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-69\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-69/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-138\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-138/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-138/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-207\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-207/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-207/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-69] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-276\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-276/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-276/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-276/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-276/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-138] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-345\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-345/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-345/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-345/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-345/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-207] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-414\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-414/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-414/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-414/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-414/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-276] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-483\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-483/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-483/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-483/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-483/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-345] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-552\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-552/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-552/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-552/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-552/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-483] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-621\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-621/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-621/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-621/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-621/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-552] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-690\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-690/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-690/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-690/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-690/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-621] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-759\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-759/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-759/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-759/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-759/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-690] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-828\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-828/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-828/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-828/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-828/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-759] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-897\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-897/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-897/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-897/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-897/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-828] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-966\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-966/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-966/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-966/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-966/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-897] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-1035\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-1035/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-1035/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-1035/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-1035/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-966] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-1104\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-1104/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-1104/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-1104/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-1104/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-1035] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-1173\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-1173/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-1173/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-1173/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-1173/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-1104] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-1242\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-1242/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-1242/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-1242/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-1242/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-1173] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-1311\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-1311/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-1311/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-1242] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./Regressors/task3_auth_usr/checkpoint-1380\n",
      "Configuration saved in ./Regressors/task3_auth_usr/checkpoint-1380/config.json\n",
      "Model weights saved in ./Regressors/task3_auth_usr/checkpoint-1380/pytorch_model.bin\n",
      "tokenizer config file saved in ./Regressors/task3_auth_usr/checkpoint-1380/tokenizer_config.json\n",
      "Special tokens file saved in ./Regressors/task3_auth_usr/checkpoint-1380/special_tokens_map.json\n",
      "Deleting older checkpoint [Regressors/task3_auth_usr/checkpoint-1311] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./Regressors/task3_auth_usr/checkpoint-414 (score: 0.6515118868060045).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 61\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Regressors/task3_auth_usr/checkpoint-414\n",
      "step 6: evaluate\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.026724383234977722, 'eval_r2_score': 0.010533113879875766, 'eval_mean_squared_error': 0.16347593069076538, 'eval_accuracy': 0.9016393442622951, 'eval_f1': 0.8148148148148149, 'eval_precision': 0.8148148148148149, 'eval_recall': 0.8148148148148149, 'eval_runtime': 0.715, 'eval_samples_per_second': 85.32, 'eval_steps_per_second': 5.595, 'epoch': 20.0}\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "run_exp(\"./Regressors/task3_auth_usr\", df, report=report, regressor_configs={\n",
    "    \"label\": \"respect\",\n",
    "    \"not_label\": \"not_respect\",\n",
    "    \"label_fn\": authority2_label_fn,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d687341-a269-4413-aedb-ea2ac53c81a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd757fd8-3c63-4cdf-b054-7d3138fd757b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DOOOOOM'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"DOOOOOM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24f0ed3-3d91-40b3-9fb4-f91711d89210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c985948-e47f-445c-aa70-755816e2feaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
