{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1877f9b4",
   "metadata": {},
   "source": [
    "## Load Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "93deeee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "import pandas as pd\n",
    "from utils import load_jsonl, dump_jsonl\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "79f99623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subject(data):\n",
    "    random.shuffle(data)\n",
    "    n = int(len(data)*0.5)\n",
    "\n",
    "    p1 = pd.DataFrame(data[0:n])\n",
    "    p2 = pd.DataFrame(data[n:])\n",
    "\n",
    "    p1[\"label\"] = p1[\"label_1\"]\n",
    "    p1[\"subject\"] = 0\n",
    "\n",
    "    p2[\"label\"] = p2[\"label_2\"]\n",
    "    p2[\"subject\"] = 1\n",
    "\n",
    "    df = pd.concat([p1, p2]).sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def get_task1_conver(in_dir, col_labels, skips=[]):\n",
    "    conversations = load_jsonl(f\"{in_dir}\")\n",
    "    \n",
    "    \n",
    "    def to_message_str(messages, users):\n",
    "        s = []\n",
    "        u = []\n",
    "        for m in messages:\n",
    "            if users[m['user_id']] == \"SYS\":\n",
    "                s.append(m['text'])\n",
    "            else:\n",
    "                u.append(m['text'])\n",
    "                \n",
    "        return s, u\n",
    "        \n",
    "    newdata = []\n",
    "    for row in conversations:\n",
    "        row[\"messages\"].sort(key=lambda x: x[\"date_created\"], reverse=False)\n",
    "        \n",
    "        users = {}\n",
    "        for m in row[\"messages\"]:\n",
    "            if m[\"user_id\"] not in users:\n",
    "#                 username = \"USR\"+str(len(users)+1) if len(users.keys())==0 else \"SYS\"\n",
    "                username = \"USR\" if len(users.keys())==0 else \"SYS\"\n",
    "                users[m[\"user_id\"]] = username\n",
    "                \n",
    "#         if len(users)>2:\n",
    "#             print(\"More than 1 users\", len(users))\n",
    "        \n",
    "        \n",
    "        messages = row[\"messages\"]\n",
    "        chunk_size = 100\n",
    "        for i in range(0, len(messages), chunk_size):\n",
    "            sub_messages = messages[i:i+chunk_size]\n",
    "            s, u = to_message_str(sub_messages, users)\n",
    "            \n",
    "            \n",
    "            o = {\n",
    "                \"user\": u,\n",
    "                \"sys\": s,\n",
    "                \"nturn\": len(messages)\n",
    "            }\n",
    "            \n",
    "            skip = False\n",
    "            for i, col in enumerate(col_labels):\n",
    "                l = row[col]\n",
    "                if pd.isna(l) or (l in skips):\n",
    "                    skip = True\n",
    "                    break\n",
    "                \n",
    "                o[\"label_\"+str(i+1)] = l\n",
    "                \n",
    "\n",
    "            if skip:\n",
    "                continue\n",
    "\n",
    "            newdata.append(o)\n",
    "\n",
    "    \n",
    "    if len(col_labels)==1:\n",
    "        df = pd.DataFrame(newdata)\n",
    "        df[\"label\"] = df[\"label_1\"]\n",
    "        df[\"subject\"] = 0\n",
    "    else:\n",
    "        df = add_subject(newdata)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "92feb049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ./Task3/annotated/annotated.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>sys</th>\n",
       "      <th>nturn</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[มาแล้วคะ, ชอบออกกำลังกายด้วยเหรอ, ปุ้ยนี่ชอบม...</td>\n",
       "      <td>[ค่ะ, ปกติออกกำลังกายไหมคะ, ไม่ค่อยได้ออกกำบัง...</td>\n",
       "      <td>35</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[วันนี้ฝนตกมั้ยคะ, ที่นี่ร้อนมากเลยค่ะ, ดีจังค...</td>\n",
       "      <td>[ไม่ค่ะ, เช่นกันค่ะ, แต่แถวบ้านยังไม่ร้อนมากค่...</td>\n",
       "      <td>38</td>\n",
       "      <td>1. Respect</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>1. Respect</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[สวัสดีค่ะ, วันนี้ก็พบกับรายการของเราอีกเช่นเค...</td>\n",
       "      <td>[เกินไปๆ, ก็เคนะ แปลกใหม่ดี, ปกติเป็นพิธีกร พู...</td>\n",
       "      <td>35</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>1. Respect</td>\n",
       "      <td>1. Respect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[เทอยื่นภาษีไปยังงงงงง, แล้วเทอได้ทวิ 50 ของออ...</td>\n",
       "      <td>[ยังๆ, แล้วมันต้องใช้อะไรบ้าง, เค้าไม่เคยยื่น,...</td>\n",
       "      <td>39</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[เราอยู่ในช่วง, เพื่อนไม่มี, 5555555555, วันนี...</td>\n",
       "      <td>[สปอนเซอร์ไม่มา, ถถถถถถถถถถถถ, เรามีแผน, เอาละ...</td>\n",
       "      <td>35</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>3. Not respect</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                user  \\\n",
       "0  [มาแล้วคะ, ชอบออกกำลังกายด้วยเหรอ, ปุ้ยนี่ชอบม...   \n",
       "1  [วันนี้ฝนตกมั้ยคะ, ที่นี่ร้อนมากเลยค่ะ, ดีจังค...   \n",
       "2  [สวัสดีค่ะ, วันนี้ก็พบกับรายการของเราอีกเช่นเค...   \n",
       "3  [เทอยื่นภาษีไปยังงงงงง, แล้วเทอได้ทวิ 50 ของออ...   \n",
       "4  [เราอยู่ในช่วง, เพื่อนไม่มี, 5555555555, วันนี...   \n",
       "\n",
       "                                                 sys  nturn     label_1  \\\n",
       "0  [ค่ะ, ปกติออกกำลังกายไหมคะ, ไม่ค่อยได้ออกกำบัง...     35   2. Normal   \n",
       "1  [ไม่ค่ะ, เช่นกันค่ะ, แต่แถวบ้านยังไม่ร้อนมากค่...     38  1. Respect   \n",
       "2  [เกินไปๆ, ก็เคนะ แปลกใหม่ดี, ปกติเป็นพิธีกร พู...     35   2. Normal   \n",
       "3  [ยังๆ, แล้วมันต้องใช้อะไรบ้าง, เค้าไม่เคยยื่น,...     39   2. Normal   \n",
       "4  [สปอนเซอร์ไม่มา, ถถถถถถถถถถถถ, เรามีแผน, เอาละ...     35   2. Normal   \n",
       "\n",
       "          label_2       label  subject  \n",
       "0       2. Normal   2. Normal        0  \n",
       "1       2. Normal  1. Respect        0  \n",
       "2      1. Respect  1. Respect        1  \n",
       "3       2. Normal   2. Normal        1  \n",
       "4  3. Not respect   2. Normal        0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_task1_conver(\"./Task3/annotated/annotated.jsonl\", [\"authority_1\", \"authority_3\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "554617d6-e691-4d93-acd3-bd65c7e93cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>sys</th>\n",
       "      <th>nturn</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1. Respect</th>\n",
       "      <td>273</td>\n",
       "      <td>273</td>\n",
       "      <td>273</td>\n",
       "      <td>273</td>\n",
       "      <td>273</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. Normal</th>\n",
       "      <td>822</td>\n",
       "      <td>822</td>\n",
       "      <td>822</td>\n",
       "      <td>822</td>\n",
       "      <td>822</td>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3. Not respect</th>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                user  sys  nturn  label_1  label_2  subject\n",
       "label                                                      \n",
       "1. Respect       273  273    273      273      273      273\n",
       "2. Normal        822  822    822      822      822      822\n",
       "3. Not respect   114  114    114      114      114      114"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b648fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(\"label\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fc25201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_task2_conver(in_dir, col_labels, skips=[]):\n",
    "    conversations = load_jsonl(f\"{in_dir}\")\n",
    "    \n",
    "    n=0\n",
    "    newdata = []\n",
    "    for row in conversations:\n",
    "        row[\"messages\"].sort(key=lambda x: x[\"created_at\"], reverse=False)\n",
    "        \n",
    "        users = {}\n",
    "        for m in row[\"messages\"]:\n",
    "            if m[\"user_id\"] not in users:\n",
    "#                 username = \"USR\"+str(len(users)+1) if len(users.keys())==0 else \"SYS\"\n",
    "                username = \"USR\" if len(users.keys())!=0 else \"SYS\"\n",
    "                users[m[\"user_id\"]] = username\n",
    "                \n",
    "        if len(users)>2:\n",
    "            print(\"More than 1 users\", len(users))\n",
    "        \n",
    "        \n",
    "        \n",
    "        messages = row[\"messages\"]\n",
    "        s = []\n",
    "        u = []\n",
    "        for m in messages:\n",
    "            text = m['text'].replace(\"[USR]\", \"\").replace(\"[URL]\", \"URL\")\n",
    "            if users[m['user_id']] == \"SYS\":\n",
    "                s.append(text)\n",
    "            else:\n",
    "                u.append(text)\n",
    "    \n",
    "        labels = [row[col] for col in col_labels]\n",
    "        \n",
    "\n",
    "        skip = False\n",
    "        for l in labels:\n",
    "            # if (l in skips):\n",
    "            if pd.isna(l) or (l in skips):\n",
    "                skip = True\n",
    "                break\n",
    "        \n",
    "        \n",
    "        if skip:\n",
    "            n+=1\n",
    "            continue\n",
    "            \n",
    "        o = {\n",
    "            \"user\": u,\n",
    "            \"sys\": s,\n",
    "            \"nturn\": len(messages)\n",
    "        }\n",
    "        \n",
    "        for i, l in enumerate(labels):\n",
    "            o[\"label_\"+str(i+1)] = l\n",
    "            \n",
    "        newdata.append(o)\n",
    "    \n",
    "    if len(col_labels)==1:\n",
    "        df = pd.DataFrame(newdata)\n",
    "        df[\"label\"] = df[\"label_1\"]\n",
    "        df[\"subject\"] = 0\n",
    "        return df\n",
    "    else:\n",
    "        df = add_subject(newdata)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# df = get_task2_conver(\"./Task2/annotated/annotated.jsonl\", [\"authority_1\", \"authority_3\"])\n",
    "# # df.groupby(\"label\").count()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d06ba5-d541-415f-bfd3-b1b24bb52de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "50daa8d9-75ff-4bd3-b3e4-613541eef8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ./Task2/annotated/annotated.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>sys</th>\n",
       "      <th>nturn</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Bio jelly night 🌙🌃\\nไม่ใช่ยานอนหลับแต่เป็นอา...</td>\n",
       "      <td>[ตื่นมาก็งงเลอะ #คลับเฮ้าส์toxic URL]</td>\n",
       "      <td>3</td>\n",
       "      <td>1. Respect</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ ขอซองสีชมพูนะคะ😆😍😍💌]</td>\n",
       "      <td>[ว้อยยยยย!!! ไม่ได้แจกการ์ด จะแจก #ยาดมน้องแบม...</td>\n",
       "      <td>3</td>\n",
       "      <td>1. Respect</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ อยากดูคะ]</td>\n",
       "      <td>[ฟังแล้วหดหู่​คนที่อยากทำงานเพื่อปชช.ต้องเจออะ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ ถ้ามีการระบาดเกิดขึ้น มันจะต้องโทษว่าเป็นเพร...</td>\n",
       "      <td>[แบน 8 ชาติจากอาฟริกา แต่โอไมครอนดันมาจากสเปน ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>3. Not respect</td>\n",
       "      <td>3. Not respect</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ บ้านเรา  มีแต่คนนอกพื้นที่ไปทำบุญ 🤣เราว่าพอก...</td>\n",
       "      <td>[โอเค  👍🏻 ความคิดใครความคิดมัน  อย่ามองคน ที่ภ...</td>\n",
       "      <td>10</td>\n",
       "      <td>3. Not respect</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>2. Normal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                user  \\\n",
       "0  [ Bio jelly night 🌙🌃\\nไม่ใช่ยานอนหลับแต่เป็นอา...   \n",
       "1                             [ ขอซองสีชมพูนะคะ😆😍😍💌]   \n",
       "2                                        [ อยากดูคะ]   \n",
       "3  [ ถ้ามีการระบาดเกิดขึ้น มันจะต้องโทษว่าเป็นเพร...   \n",
       "4  [ บ้านเรา  มีแต่คนนอกพื้นที่ไปทำบุญ 🤣เราว่าพอก...   \n",
       "\n",
       "                                                 sys  nturn         label_1  \\\n",
       "0              [ตื่นมาก็งงเลอะ #คลับเฮ้าส์toxic URL]      3      1. Respect   \n",
       "1  [ว้อยยยยย!!! ไม่ได้แจกการ์ด จะแจก #ยาดมน้องแบม...      3      1. Respect   \n",
       "2  [ฟังแล้วหดหู่​คนที่อยากทำงานเพื่อปชช.ต้องเจออะ...      3       2. Normal   \n",
       "3  [แบน 8 ชาติจากอาฟริกา แต่โอไมครอนดันมาจากสเปน ...      3       2. Normal   \n",
       "4  [โอเค  👍🏻 ความคิดใครความคิดมัน  อย่ามองคน ที่ภ...     10  3. Not respect   \n",
       "\n",
       "          label_2           label  subject  \n",
       "0       2. Normal       2. Normal        1  \n",
       "1       2. Normal       2. Normal        1  \n",
       "2       2. Normal       2. Normal        0  \n",
       "3  3. Not respect  3. Not respect        1  \n",
       "4       2. Normal       2. Normal        1  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_task2_conver(\"./Task2/annotated/annotated.jsonl\", [\"authority_1\",  \"authority_2\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "27de11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27dfc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a566de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8d78a46",
   "metadata": {},
   "source": [
    "## Load Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "caafaef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "755a8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./PrivateSpace/thai-dictionary/RoyalInstituteDictionary/words.json\", encoding=\"utf-8\") as fin:\n",
    "    raw = json.load(fin)\n",
    "    thaidict_royal = set()\n",
    "    for k in raw:\n",
    "        thaidict_royal.update(raw[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63a63c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25573 records from lexicons.jsonl\n"
     ]
    }
   ],
   "source": [
    "lexicons_arr = load_jsonl(\"lexicons.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1bbccb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "tags = set()\n",
    "lexicons = {}\n",
    "lexicons_keys = defaultdict(list)\n",
    "\n",
    "for key, values  in lexicons_arr:\n",
    "    if len(key) <= 1:\n",
    "        continue\n",
    "        \n",
    "    key = key.lower()\n",
    "    if key.endswith(\"rep\"):\n",
    "        key = key.replace(\"rep\", \"\")\n",
    "        \n",
    "    w = word_tokenize(key)\n",
    "    \n",
    "    lexicons_keys[w[0]].append(key)\n",
    "    \n",
    "    tag = [t for t in values[\"tags\"] if not t.startswith(\"cat:\")]\n",
    "    lexicons[key] = tag\n",
    "    tags.update(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "320f0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pythainlp.util import countthai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12380160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "388acee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import groupby\n",
    "import emoji\n",
    "from pythainlp.util import countthai\n",
    "from tqdm import tqdm\n",
    "\n",
    "def rm_reptitive(text):\n",
    "    s = \"\"\n",
    "    groups = groupby(text)\n",
    "    for label, group in groups:\n",
    "        g = list(group)\n",
    "        if len(g) >= 3:\n",
    "            s += f\"{label*3} rep \"\n",
    "        else:\n",
    "            s += \"\".join(g)\n",
    "    return s\n",
    "\n",
    "def remove_space(sent):\n",
    "    newwords = []\n",
    "    for w in sent:\n",
    "        if len(w.strip())==0:\n",
    "            continue\n",
    "        newwords.append(w)\n",
    "    return newwords\n",
    "    \n",
    "def analyse_conv_per_person(texts):\n",
    "    \n",
    "    # Word Statistic\n",
    "    texts = [t.lower() for t in texts]\n",
    "    texts = [rm_reptitive(t) for t in texts]\n",
    "    words = [word_tokenize(t) for t in texts]\n",
    "    words = [remove_space(w) for w in words]\n",
    "    \n",
    "    nlongword = 0\n",
    "    ndict = 0\n",
    "    for sent in words:\n",
    "        ndict += sum([1 if w in thaidict_royal else 0 for w in sent])\n",
    "        nlongword += sum([1 if len(w) > 7 else 0 for w in sent])\n",
    "        nthai = sum([1 if countthai(w) > 50 else 0 for w in sent])\n",
    "        \n",
    "        \n",
    "    \n",
    "    uwords = set()\n",
    "    for sent in words:\n",
    "        uwords.update(sent)\n",
    "    \n",
    "    # Lexicon \n",
    "    lex = []\n",
    "    for sidx, sent in enumerate(words):\n",
    "        for widx, w in enumerate(sent):\n",
    "            if w not in lexicons_keys:\n",
    "                continue \n",
    "            \n",
    "            s = \"\".join(sent[widx:])\n",
    "            for l in lexicons_keys[w]:\n",
    "                if not s.startswith(l):\n",
    "                    continue\n",
    "\n",
    "                lex.extend(lexicons[l])\n",
    "#                 print(\">>\", w, l, lexicons[l])\n",
    "\n",
    "    lexcat = {}\n",
    "    for l in lex:\n",
    "        if l not in lexcat:\n",
    "            lexcat[l] = 0\n",
    "        lexcat[l] += 1\n",
    "    \n",
    "    # Stylistic words\n",
    "    nrepeat = 0\n",
    "    for sidx, sent in enumerate(words):\n",
    "        nrepeat += sum([1 if w==\"rep\" else 0 for w in sent])\n",
    "    \n",
    "    s = \" \".join(texts)\n",
    "    nemoji = emoji.emoji_count(s)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"nsent\": len(texts),\n",
    "        \"nword\": sum([len(w) for w in words]),\n",
    "        \"ndict\": ndict,\n",
    "        \"nunique\": len(uwords),\n",
    "        \"nlongword\": nlongword,\n",
    "        \"nrepeat\": nrepeat,\n",
    "        \"nthai\": nthai,\n",
    "        \"nemoji\": nemoji,\n",
    "        **lexcat\n",
    "    }\n",
    "  \n",
    "\n",
    "\n",
    "def analyse_conversation(df):\n",
    "    metrics = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        ms = analyse_conv_per_person(row[\"sys\"])\n",
    "        mu = analyse_conv_per_person(row[\"user\"])\n",
    "        metrics.append((ms, mu))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "analyse_conv_per_person([\"เมิงงงงงงงมันโง่เหมือนควายยยยยยยยย\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28f22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9e63519",
   "metadata": {},
   "source": [
    "## Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "14df6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_obj_values(filepath, obj):\n",
    "  with open(filepath, 'wb') as fin:\n",
    "    pickle.dump(obj, fin)\n",
    "    \n",
    "def load_obj_values(filepath):\n",
    "  with open(filepath, 'rb') as fin:\n",
    "    obj = pickle.load(fin)\n",
    "  return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7b65e400-6b85-4bc2-a379-59e1518bf57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_values = {\n",
    "    \"public\": {\n",
    "        \"closeness\": [],\n",
    "        \"authority\": [],\n",
    "    },\n",
    "    \"private\": {\n",
    "        \"closeness\": [],\n",
    "        \"authority\": [],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "28132c43-1e9c-4528-850e-479f92676feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ./Task2/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1714/1714 [00:04<00:00, 409.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ./Task3/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1219/1219 [00:05<00:00, 233.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ./Task2/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2399/2399 [00:05<00:00, 465.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ./Task3/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1218/1218 [00:05<00:00, 223.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ./Task2/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1718/1718 [00:03<00:00, 445.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ./Task3/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1220/1220 [00:05<00:00, 237.17it/s]\n"
     ]
    }
   ],
   "source": [
    "annotation_pairs = [\n",
    "    (\"closeness_1\", \"closeness_2\"),\n",
    "    (\"closeness_1\", \"closeness_3\"),\n",
    "    (\"closeness_2\", \"closeness_3\"),\n",
    "]\n",
    "for col1, col2 in annotation_pairs:\n",
    "    df = get_task2_conver(\"./Task2/annotated/annotated.jsonl\", [col1, col2], skips = [])\n",
    "    metrics = analyse_conversation(df)\n",
    "    \n",
    "    analysis_values[\"public\"][\"closeness\"].append({\n",
    "        \"values\": metrics,\n",
    "        \"labels\": df[\"label\"].values,\n",
    "        \"subject\": df[\"subject\"].values\n",
    "    })\n",
    "    \n",
    "    \n",
    "    df = get_task1_conver(\"./Task3/annotated/annotated.jsonl\", [col1, col2], skips = [])\n",
    "    metrics = analyse_conversation(df)\n",
    "    \n",
    "    analysis_values[\"private\"][\"closeness\"].append({\n",
    "        \"values\": metrics,\n",
    "        \"labels\": df[\"label\"].values,\n",
    "        \"subject\": df[\"subject\"].values\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3d8fabbd-f9da-4583-a928-77d177daa9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ./Task2/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 966/966 [00:02<00:00, 480.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ./Task3/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1209/1209 [00:05<00:00, 234.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ./Task2/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 175/175 [00:00<00:00, 490.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ./Task3/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1209/1209 [00:04<00:00, 246.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2463 records from ./Task2/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1638/1638 [00:03<00:00, 494.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1221 records from ./Task3/annotated/annotated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1221/1221 [00:04<00:00, 247.24it/s]\n"
     ]
    }
   ],
   "source": [
    "annotation_pairs = [\n",
    "    (\"authority_1\", \"authority_2\"),\n",
    "    (\"authority_1\", \"authority_3\"),\n",
    "    (\"authority_2\", \"authority_3\"),\n",
    "]\n",
    "for col1, col2 in annotation_pairs:\n",
    "    df = get_task2_conver(\"./Task2/annotated/annotated.jsonl\", [col1, col2], skips = [])\n",
    "    metrics = analyse_conversation(df)\n",
    "    \n",
    "    analysis_values[\"public\"][\"authority\"].append({\n",
    "        \"values\": metrics,\n",
    "        \"labels\": df[\"label\"].values,\n",
    "        \"subject\": df[\"subject\"].values\n",
    "    })\n",
    "    \n",
    "    \n",
    "    df = get_task1_conver(\"./Task3/annotated/annotated.jsonl\", [col1, col2], skips = [])\n",
    "    metrics = analyse_conversation(df)\n",
    "    \n",
    "    analysis_values[\"private\"][\"authority\"].append({\n",
    "        \"values\": metrics,\n",
    "        \"labels\": df[\"label\"].values,\n",
    "        \"subject\": df[\"subject\"].values\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "78faf9eb-be20-44ac-a4b5-80bc4c1191f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj_values(\"Results/analysis_values_sub.pkl\", analysis_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd013858-c593-4edf-88ec-c8b648532ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "439d8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_values = load_obj_values(\"Results/analysis_values_sub.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bced4ac-545d-42a2-b179-39885475ad03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad66f10",
   "metadata": {},
   "source": [
    "## Appendix: Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8a47c508-9aa5-4010-bb88-6e5a9ebfbc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = {\n",
    "    \"Word Statistics\" : {\n",
    "        \"nsent\": \"Number of utterance\",\n",
    "        \"nword\": \"Number of word\",\n",
    "    },\n",
    "    \"Lexicon Diversity\" : {\n",
    "        \"nunique\": \"Vocabulary size\",\n",
    "        \"nthai\": \"Thai words\",\n",
    "        \"nlongword\": \"Long words\",\n",
    "        \"ndict\": \"Dictionary words\"\n",
    "    },\n",
    "    \n",
    "    \"Function Words\": {\n",
    "\n",
    "        \"pronoun\": \"Pronoun\",\n",
    "        \"pronoun_1st\": \">> 1st person pronoun\",\n",
    "        \"pronoun_2nd\": \">> 2nd person pronoun\",\n",
    "        \"pronoun_3rd\": \">> 3rd person pronoun\",\n",
    "        \"pronoun_misspelling\": \">> Pronoun in non-standard spelling\",\n",
    "\n",
    "        \"particles\": \"Particles\",\n",
    "        \"particles_SARP\": \">> Socially-related particles\",\n",
    "        \"particles_notSARP\": \">> Non-socially-related particles\",\n",
    "        \"particles_misspelling\": \">> Particle in non-standard spelling\",\n",
    "    },\n",
    "    \n",
    "    \"Sentiment-related\": {\n",
    "    \n",
    "        \"sentiment\": \"Sentiment words\",\n",
    "        \"sentiment_positive\": \">> Positive words\",\n",
    "        \"sentiment_negative\": \">> Negative words\",\n",
    "    },\n",
    "    \n",
    "    \"Internet Lexicons\": {\n",
    "        \"misspelling\": \"Spelling variation\",\n",
    "        \"misspelling_common\": \">> Common misspelt words\",\n",
    "        \"misspelling_intention\": \">> Sematic variation\",\n",
    "        \"misspelling_shorten\": \">> Simplified variation\",\n",
    "        \"nrepeat\": \">> Repeated characters\",\n",
    "\n",
    "        \"abbr\": \"Abbreviation\",\n",
    "        # \"slang\": \"Slang\",\n",
    "        \"swear\": \"Swear words\",\n",
    "        \"transliterated\": \"Transliteration\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "cc6aefec-6878-4549-a5c8-884592d3b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clse_print_labels = ['1. Close', '2. Know each other', \"3. Don't know each other\", \"4. Don't like each other\"]\n",
    "auth_print_labels = ['0. Very respect', '1. Respect', '2. Normal', '3. Not respect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f55aa4b-9185-4ba6-bca4-984338077ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "65d94022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def authority_to_vec1(label, subject):\n",
    "    if label == '0. Very respect':\n",
    "        return {\"x1\": 1, \"x2\": 0, \"x3\": subject, \"x4\": subject, \"x5\": 0}\n",
    "    elif label == '1. Respect': \n",
    "        return {\"x1\": 0, \"x2\": 0, \"x3\": subject, \"x4\": 0, \"x5\": 0}\n",
    "    elif label == '2. Normal':\n",
    "        return {\"x1\": 0, \"x2\": 1, \"x3\": subject, \"x4\": 0, \"x5\": subject}\n",
    "    elif label == '3. Not respect':\n",
    "        return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def closeness_to_vec1(label, subject):\n",
    "    if label == \"1. Close\":\n",
    "        return {\"x1\": 1, \"x2\": 0, \"x3\": subject, \"x4\": subject, \"x5\": 0}\n",
    "    elif label == \"2. Know each other\":\n",
    "        return {\"x1\": 0, \"x2\": 0, \"x3\": subject, \"x4\": 0, \"x5\": 0}\n",
    "    elif label == \"3. Don't know each other\": \n",
    "        return {\"x1\": 0, \"x2\": 1, \"x3\": subject, \"x4\": 0, \"x5\": subject}\n",
    "    elif label == \"4. Don't like each other\":\n",
    "        return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08665b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "85b93ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def to_features(analysis_values, setting, relation, idx, to_vec_func, skips=[]):\n",
    "    exp = analysis_values[setting][relation][idx]\n",
    "    values = exp[\"values\"]\n",
    "    labels = exp[\"labels\"]\n",
    "    subject = exp[\"subject\"]\n",
    "\n",
    "    assert(len(values)==len(labels))\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    for (ms, mu), l, s in zip(values, labels, subject):\n",
    "        if l in skips:\n",
    "            continue\n",
    "\n",
    "        x = to_vec_func(l, s)\n",
    "        if x is None:\n",
    "            continue\n",
    "\n",
    "        for m in mu:\n",
    "            if m in [\"nsent\", \"nword\"]:\n",
    "                v = mu[m]\n",
    "            else:\n",
    "                v = mu[m]*100/mu[\"nword\"]\n",
    "\n",
    "\n",
    "            rows.append({\n",
    "                \"metric\": m,\n",
    "                \"value\": v,\n",
    "                **x\n",
    "            })\n",
    "\n",
    "        if \"particles_SARP\" in mu:\n",
    "            particles_notSARP = mu[\"particles\"] - mu[\"particles_SARP\"]\n",
    "            rows.append({\n",
    "                \"metric\": \"particles_notSARP\",\n",
    "                \"value\": particles_notSARP,\n",
    "                **x\n",
    "            })\n",
    "            \n",
    "    feats = pd.DataFrame(rows)  \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fceb9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "def print_anova_test(feats, n=3):\n",
    "    feat_names = feats[\"metric\"].unique()\n",
    "    \n",
    "    feat_important = []\n",
    "    summary = []\n",
    "    for f in feat_names:\n",
    "        if \"cat:\" in f:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            d = feats[feats[\"metric\"]==f]\n",
    "        #     print(f, len(d))\n",
    "            x_columns = [f\"x{i+1}\" for i in range(n)]\n",
    "            X = d[x_columns]\n",
    "            X = sm.add_constant(X)\n",
    "            Y = d[\"value\"]\n",
    "            model = sm.OLS(Y,X)\n",
    "            results = model.fit()\n",
    "            \n",
    "            ncoef = n + 1\n",
    "            \n",
    "            t_test = results.t_test(np.identity(ncoef))\n",
    "            f_test = results.f_test(np.identity(ncoef))\n",
    "            o = {\"feat\": f, \"f_value\": f_test.pvalue}\n",
    "            for i in range(ncoef):\n",
    "                o[f\"coef{i}\"] = t_test.effect[i]\n",
    "                \n",
    "            for i in range(ncoef):\n",
    "                o[f\"pval{i}\"] = t_test.pvalue[i]\n",
    "                \n",
    "            feat_important.append(o)\n",
    "            summary.append(results)\n",
    "        except Exception as e:\n",
    "            print(\"error\", f, e)\n",
    "\n",
    "    outputs = pd.DataFrame(feat_important)\n",
    "    return outputs, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860eae70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e208f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "574751f3-cfd7-49fe-9bbd-bdfc44bf37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weights(outputs):\n",
    "    \n",
    "    coefs = {}\n",
    "    coef_labels = [\"b\", \"w1\", \"w2\", \"w3\", \"w4\", \"w5\"]\n",
    "    for i, label in enumerate(coef_labels):\n",
    "        for output in outputs:\n",
    "            for _, row in output.iterrows():\n",
    "                key = (label, row[\"feat\"])\n",
    "                if key not in coefs:\n",
    "                    coefs[key] = (row[f\"coef{i}\"]/len(outputs), row[f\"pval{i}\"]/len(outputs))\n",
    "                else: \n",
    "                    ## Average the coefs across experiments\n",
    "                    _coef, _pval = coefs[key]\n",
    "                    _coef += row[f\"coef{i}\"]/len(outputs)\n",
    "                    _pval += row[f\"pval{i}\"]/len(outputs)\n",
    "                    coefs[key] = (_coef, _pval)\n",
    "\n",
    "#     print(coefs)\n",
    "\n",
    "    printed_text = \"\"\n",
    "    for g in metric_names:\n",
    "        # print(\"\\multicolumn{4}{l}{\\\\textit{\"+g+\"}} \\\\\\\\\")\n",
    "        printed_text += \"\\multicolumn{7}{l}{\\\\textit{\"+g+\"}} \\\\\\\\\"+\"\\n\"\n",
    "        for m in metric_names[g]:\n",
    "            s = f\"{metric_names[g][m]} \"\n",
    "            for l in coef_labels:                    \n",
    "                val, pval = coefs[(l, m)]\n",
    "                if l==\"b\":\n",
    "                    s += f\"& {val:.2f} \"\n",
    "                elif pval < 0.05:\n",
    "                    s += \"& \\cellcolor{gray!25} \"+f\"{val:.2f} \"\n",
    "                else:\n",
    "                    s += f\"& {val:.2f} \"\n",
    "            s += \"\\\\\\\\\"\n",
    "            # print(s)\n",
    "            printed_text += s+\"\\n\"\n",
    "        printed_text += \"&  & &\\\\\\\\\" + \"\\n\"\n",
    "        printed_text += \"\\hline\" + \"\\n\"\n",
    "        # print(\"&  & &\\\\\\\\\")\n",
    "        # print(\"\\hline\")\n",
    "    return printed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "123595e1-f763-455a-87a6-62bc1fc253fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printed_text1 = print_weights([outputs])\n",
    "# print(printed_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f82a11-1c8c-45b6-8b02-1a05e1563a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11badd25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "e0102f26-6fb0-4f46-9e65-37d856c86cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error particles_?? wrong shape for coefs\n",
      "error particles_?? r_matrix performs f_test for using dimensions that are asymptotically non-normal\n",
      "error particles_?? wrong shape for coefs\n",
      "error swear r_matrix performs f_test for using dimensions that are asymptotically non-normal\n",
      "error particles_?? wrong shape for coefs\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "outputs = []\n",
    "for i in range(3):\n",
    "    feats = to_features(analysis_values, \"public\", \"closeness\", i, closeness_to_vec1, skips=[])\n",
    "    output, summary = print_anova_test(feats, n=5)\n",
    "    outputs.append(output)\n",
    "printed_text1 = print_weights(outputs)\n",
    "\n",
    "outputs = []\n",
    "for i in range(3):\n",
    "    feats = to_features(analysis_values, \"public\", \"authority\", i, authority_to_vec1, skips=[\"3. Not respect\"])\n",
    "    output, summary = print_anova_test(feats, n=5)\n",
    "    outputs.append(output)\n",
    "printed_text2 = print_weights(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "50750d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366bdad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a18a5b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closeness_to_vec2(label, subject):\n",
    "    if label == \"1. Close\":\n",
    "        return {\"x1\": 1, \"x2\": 0, \"x3\": subject, \"x4\": subject, \"x5\": 0}\n",
    "    elif label == \"2. Know each other\": \n",
    "        return {\"x1\": 0, \"x2\": 0, \"x3\": subject, \"x4\": 0, \"x5\": 0}\n",
    "    elif label == \"3. Don't know each other\": ## Base category\n",
    "        return {\"x1\": 0, \"x2\": 1, \"x3\": subject, \"x4\": 0, \"x5\": subject}\n",
    "    elif label == \"4. Don't like each other\":\n",
    "        return None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def authority_to_vec2(label, subject):\n",
    "    if label == '0. Very respect':\n",
    "        return None\n",
    "    elif label == '1. Respect': \n",
    "        return {\"x1\": 1, \"x2\": 0, \"x3\": subject, \"x4\": subject, \"x5\": 0}\n",
    "    elif label == '2. Normal': ## Base category\n",
    "        return {\"x1\": 0, \"x2\": 0, \"x3\": subject, \"x4\": 0, \"x5\": 0}\n",
    "    elif label == '3. Not respect':\n",
    "        return {\"x1\": 0, \"x2\": 1, \"x3\": subject, \"x4\": 0, \"x5\": subject}\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1d045e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "550f18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for i in range(3):\n",
    "    feats = to_features(analysis_values, \"private\", \"closeness\", i, closeness_to_vec2, skips=[])\n",
    "    output, summary = print_anova_test(feats, n=5)\n",
    "    outputs.append(output)\n",
    "printed_text3 = print_weights(outputs)\n",
    "\n",
    "outputs = []\n",
    "for i in range(3):\n",
    "    feats = to_features(analysis_values, \"private\", \"authority\", i, authority_to_vec2, skips=[])\n",
    "    output, summary = print_anova_test(feats, n=5)\n",
    "    outputs.append(output)\n",
    "printed_text4 = print_weights(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9cedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b1bd0f30-6750-4e6d-a1fc-60717f9b2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\subsection{Setting 2: Public Conversations with Labels from 3rd Party }\n",
      "\n",
      "\\subsubsection{Closeness}\n",
      "\\begin{longtable}[h]{\n",
      "    p{\\dimexpr 0.40\\linewidth-2\\tabcolsep}|c|c|c|c|c|c|\n",
      "}\n",
      "    \\hline\n",
      "\n",
      "    Lexical Features & b & w1 & w2 & w3 & w4 & w5 \\\\\n",
      "    \\hline\n",
      "    \\endfirsthead\n",
      "\n",
      "    \\endhead\n",
      "    \n",
      "    \\multicolumn{7}{l}{\\textit{Word Statistics}} \\\\\n",
      "    Number of utterance & 1.52 & -0.06 & 0.01 & 0.05 & -0.01 & -0.10 \\\\\n",
      "    Number of word & 37.78 & \\cellcolor{gray!25} -13.95 & -0.48 & 3.76 & -3.18 & -5.50 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Lexicon Diversity}} \\\\\n",
      "    Vocabulary size & 87.83 & \\cellcolor{gray!25} 4.10 & -0.42 & -0.21 & -2.05 & 0.19 \\\\\n",
      "    Thai words & 65.00 & -1.06 & -1.48 & 0.38 & -3.87 & 1.35 \\\\\n",
      "    Long words & 4.07 & -1.05 & 0.51 & 0.09 & 1.10 & -0.32 \\\\\n",
      "    Dictionary words & 71.71 & -4.85 & 0.45 & 1.36 & -3.20 & -1.06 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Function Words}} \\\\\n",
      "    Pronoun & 9.42 & -0.18 & \\cellcolor{gray!25} -2.66 & -0.84 & 1.22 & 0.69 \\\\\n",
      "    >> 1st person pronoun & 7.16 & 0.16 & -2.53 & -1.23 & 1.18 & 1.12 \\\\\n",
      "    >> 2nd person pronoun & 8.48 & -1.93 & \\cellcolor{gray!25} -2.96 & -1.76 & 2.58 & 1.36 \\\\\n",
      "    >> 3rd person pronoun & 6.07 & 1.44 & -1.13 & -0.77 & 1.00 & 0.95 \\\\\n",
      "    >> Pronoun in non-standard spelling & 4.71 & 2.62 & -1.00 & -0.92 & 1.79 & 1.17 \\\\\n",
      "    Particles & 9.06 & 1.29 & 0.01 & -0.33 & 0.34 & 0.94 \\\\\n",
      "    >> Socially-related particles & 6.17 & 1.74 & 0.90 & -0.15 & -0.11 & 0.48 \\\\\n",
      "    >> Non-socially-related particles & 1.30 & -0.01 & -0.03 & 0.19 & -0.10 & -0.16 \\\\\n",
      "    >> Particle in non-standard spelling & 4.74 & 0.87 & 0.67 & 0.16 & -0.09 & 0.10 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Sentiment-related}} \\\\\n",
      "    Sentiment words & 9.64 & 1.92 & -0.24 & -0.55 & 0.89 & 0.54 \\\\\n",
      "    >> Positive words & 7.17 & 1.77 & -0.62 & -0.37 & 2.19 & 0.17 \\\\\n",
      "    >> Negative words & 5.55 & 3.45 & 0.73 & -0.02 & -0.81 & 0.30 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Internet Lexicons}} \\\\\n",
      "    Spelling variation & 23.29 & 2.84 & -2.31 & 0.48 & -1.97 & -0.71 \\\\\n",
      "    >> Common misspelt words & 4.09 & 1.15 & -0.61 & -1.52 & 1.82 & 2.08 \\\\\n",
      "    >> Sematic variation & 15.43 & 2.35 & -2.89 & -0.10 & -0.18 & 0.02 \\\\\n",
      "    >> Simplified variation & 13.12 & 1.94 & -0.28 & 0.23 & -0.95 & -0.38 \\\\\n",
      "    >> Repeated characters & 3.07 & 0.41 & -1.52 & -0.78 & 1.25 & 0.82 \\\\\n",
      "    Abbreviation & 3.53 & 2.96 & 0.49 & 2.14 & -0.08 & -1.46 \\\\\n",
      "    Swear words & 5.77 & 2.05 & -2.92 & -3.95 & 2.38 & 4.49 \\\\\n",
      "    Transliteration & 3.56 & 1.85 & 1.45 & -0.66 & 2.16 & 0.92 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \n",
      "\\end{longtable}\n",
      "\n",
      "\\clearpage\n",
      "\\subsubsection{Respect}\n",
      "\\begin{longtable}[h]{\n",
      "    p{\\dimexpr 0.40\\linewidth-2\\tabcolsep}|c|c|c|c|c|c|\n",
      "}\n",
      "    \\hline\n",
      "\n",
      "    Lexical Features & b & w1 & w2 & w3 & w4 & w5 \\\\\n",
      "    \\hline\n",
      "    \\endfirsthead\n",
      "\n",
      "    \\endhead\n",
      "    \n",
      "    \\multicolumn{7}{l}{\\textit{Word Statistics}} \\\\\n",
      "    Number of utterance & 1.53 & -0.00 & 0.01 & -0.03 & 0.00 & 0.01 \\\\\n",
      "    Number of word & 47.67 & -0.00 & -14.96 & -13.06 & 0.00 & 10.69 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Lexicon Diversity}} \\\\\n",
      "    Vocabulary size & 83.47 & -0.00 & 4.50 & 2.91 & 0.00 & -1.63 \\\\\n",
      "    Thai words & 62.64 & -0.00 & 1.25 & -0.99 & 0.00 & 0.69 \\\\\n",
      "    Long words & 4.16 & -0.00 & -0.13 & 0.74 & 0.00 & -0.32 \\\\\n",
      "    Dictionary words & 66.62 & -0.00 & 4.83 & -3.19 & 0.00 & 3.53 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Function Words}} \\\\\n",
      "    Pronoun & 6.10 & \\cellcolor{gray!25} -0.00 & 1.39 & 2.83 & 0.00 & -3.13 \\\\\n",
      "    >> 1st person pronoun & 4.69 & 0.00 & 0.60 & 0.42 & 0.00 & -0.35 \\\\\n",
      "    >> 2nd person pronoun & 5.78 & 0.00 & -0.25 & 3.17 & 0.00 & -2.51 \\\\\n",
      "    >> 3rd person pronoun & 3.77 & 0.00 & 2.23 & 2.88 & 0.00 & -3.68 \\\\\n",
      "    >> Pronoun in non-standard spelling & 3.67 & 0.00 & 1.10 & -0.69 & 0.00 & 0.04 \\\\\n",
      "    Particles & 11.35 & -0.00 & -2.36 & -1.40 & 0.00 & 1.85 \\\\\n",
      "    >> Socially-related particles & 8.33 & 0.00 & -0.78 & -1.25 & 0.00 & 1.20 \\\\\n",
      "    >> Non-socially-related particles & 1.35 & -0.00 & -0.22 & 0.05 & 0.00 & 0.05 \\\\\n",
      "    >> Particle in non-standard spelling & 3.90 & 0.00 & 0.38 & 1.22 & 0.00 & 3.19 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Sentiment-related}} \\\\\n",
      "    Sentiment words & 10.77 & 0.00 & -1.89 & -0.33 & 0.00 & 0.68 \\\\\n",
      "    >> Positive words & 9.34 & 0.00 & \\cellcolor{gray!25} -3.32 & -0.42 & 0.00 & 0.45 \\\\\n",
      "    >> Negative words & 4.96 & 0.00 & 1.50 & -0.17 & 0.00 & 0.18 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Internet Lexicons}} \\\\\n",
      "    Spelling variation & 24.63 & \\cellcolor{gray!25} -0.00 & -2.53 & -3.04 & 0.00 & 3.45 \\\\\n",
      "    >> Common misspelt words & 3.46 & -0.00 & -0.57 & 0.34 & 0.00 & 0.03 \\\\\n",
      "    >> Sematic variation & 17.94 & -0.00 & -3.91 & -2.71 & 0.00 & 2.74 \\\\\n",
      "    >> Simplified variation & 11.42 & -0.00 & 1.59 & -0.29 & 0.00 & 0.75 \\\\\n",
      "    >> Repeated characters & 2.32 & -0.00 & 0.19 & 0.20 & 0.00 & -0.45 \\\\\n",
      "    Abbreviation & 10.95 & -0.00 & -3.60 & -5.60 & 0.00 & 2.47 \\\\\n",
      "    Swear words & 1.87 & -0.00 & 0.37 & -1.14 & 0.00 & 0.52 \\\\\n",
      "    Transliteration & 3.11 & -0.00 & 1.54 & -1.16 & 0.00 & 2.90 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \n",
      "\\end{longtable}\n",
      "\n",
      "\\clearpage\n",
      "\n",
      "\\subsection{Setting 3: Private Conversations with Labels from 3rd Party }\n",
      "\n",
      "\\subsubsection{Closeness}\n",
      "\\begin{longtable}[h]{\n",
      "    p{\\dimexpr 0.40\\linewidth-2\\tabcolsep}|c|c|c|c|c|c|\n",
      "}\n",
      "    \\hline\n",
      "\n",
      "    Lexical Features & b & w1 & w2 & w3 & w4 & w5 \\\\\n",
      "    \\hline\n",
      "    \\endfirsthead\n",
      "\n",
      "    \\endhead\n",
      "    \n",
      "    \\multicolumn{7}{l}{\\textit{Word Statistics}} \\\\\n",
      "    Number of utterance & 18.34 & 0.55 & \\cellcolor{gray!25} -8.70 & 0.72 & -0.52 & \\cellcolor{gray!25} -5.94 \\\\\n",
      "    Number of word & 99.50 & -2.92 & -42.85 & 5.55 & -7.89 & \\cellcolor{gray!25} -42.43 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Lexicon Diversity}} \\\\\n",
      "    Vocabulary size & 69.05 & \\cellcolor{gray!25} 2.57 & 10.44 & 0.07 & 0.25 & \\cellcolor{gray!25} 10.42 \\\\\n",
      "    Thai words & 10.91 & -4.68 & \\cellcolor{gray!25} 31.76 & -3.92 & 4.27 & \\cellcolor{gray!25} 22.32 \\\\\n",
      "    Long words & 4.04 & \\cellcolor{gray!25} -0.57 & -0.11 & -0.00 & -0.26 & -1.27 \\\\\n",
      "    Dictionary words & 79.32 & -1.92 & 6.59 & -0.30 & 0.82 & 2.97 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Function Words}} \\\\\n",
      "    Pronoun & 4.22 & \\cellcolor{gray!25} 1.84 & 1.01 & 0.62 & 0.21 & 0.89 \\\\\n",
      "    >> 1st person pronoun & 2.73 & \\cellcolor{gray!25} 0.58 & 1.09 & 0.25 & 0.17 & 0.56 \\\\\n",
      "    >> 2nd person pronoun & 3.50 & 0.67 & 0.25 & 0.25 & -0.04 & 1.26 \\\\\n",
      "    >> 3rd person pronoun & 1.92 & \\cellcolor{gray!25} 0.96 & 0.53 & 0.30 & -0.24 & \\cellcolor{gray!25} 5.12 \\\\\n",
      "    >> Pronoun in non-standard spelling & 1.39 & \\cellcolor{gray!25} 0.95 & 0.16 & 0.37 & -0.16 & 1.02 \\\\\n",
      "    Particles & 11.95 & \\cellcolor{gray!25} -4.54 & \\cellcolor{gray!25} 15.63 & \\cellcolor{gray!25} -2.34 & 2.27 & \\cellcolor{gray!25} 9.33 \\\\\n",
      "    >> Socially-related particles & 7.89 & \\cellcolor{gray!25} -4.38 & \\cellcolor{gray!25} 16.94 & \\cellcolor{gray!25} -2.13 & 1.90 & \\cellcolor{gray!25} 8.29 \\\\\n",
      "    >> Non-socially-related particles & 5.15 & 0.41 & -2.31 & 0.39 & -0.47 & -1.78 \\\\\n",
      "    >> Particle in non-standard spelling & 2.05 & -0.15 & 1.44 & -0.24 & 0.10 & 1.90 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Sentiment-related}} \\\\\n",
      "    Sentiment words & 7.69 & \\cellcolor{gray!25} -1.48 & 0.03 & -0.91 & 1.01 & \\cellcolor{gray!25} 11.65 \\\\\n",
      "    >> Positive words & 5.79 & \\cellcolor{gray!25} -1.98 & 1.59 & -0.98 & 0.93 & \\cellcolor{gray!25} 13.15 \\\\\n",
      "    >> Negative words & 2.86 & 0.40 & -0.99 & -0.06 & 0.17 & 1.92 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Internet Lexicons}} \\\\\n",
      "    Spelling variation & 23.21 & \\cellcolor{gray!25} 1.52 & \\cellcolor{gray!25} -3.74 & 0.34 & -0.23 & 2.61 \\\\\n",
      "    >> Common misspelt words & 1.49 & 0.34 & -0.55 & 0.21 & -0.36 & 0.15 \\\\\n",
      "    >> Sematic variation & 14.51 & 0.48 & \\cellcolor{gray!25} -0.80 & -0.27 & 0.02 & 5.92 \\\\\n",
      "    >> Simplified variation & 11.63 & 0.73 & -3.18 & 0.06 & 0.61 & 3.01 \\\\\n",
      "    >> Repeated characters & 2.07 & 0.19 & \\cellcolor{gray!25} -1.60 & -0.11 & 0.21 & 0.40 \\\\\n",
      "    Abbreviation & 1.28 & 0.60 & 3.35 & 0.10 & 0.21 & -0.54 \\\\\n",
      "    Swear words & 2.11 & -0.06 & -0.16 & -0.01 & 0.11 & 0.15 \\\\\n",
      "    Transliteration & 1.42 & -0.00 & -0.28 & 0.02 & 0.12 & 0.19 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \n",
      "\\end{longtable}\n",
      "\n",
      "\\clearpage\n",
      "\\subsubsection{Respect}\n",
      "\\begin{longtable}[h]{\n",
      "    p{\\dimexpr 0.40\\linewidth-2\\tabcolsep}|c|c|c|c|c|c|\n",
      "}\n",
      "    \\hline\n",
      "\n",
      "    Lexical Features & b & w1 & w2 & w3 & w4 & w5 \\\\\n",
      "    \\hline\n",
      "    \\endfirsthead\n",
      "\n",
      "    \\endhead\n",
      "    \n",
      "    \\multicolumn{7}{l}{\\textit{Word Statistics}} \\\\\n",
      "    Number of utterance & 18.88 & \\cellcolor{gray!25} -2.12 & 0.47 & 0.30 & -0.46 & -0.60 \\\\\n",
      "    Number of word & 103.88 & -11.84 & -13.83 & -3.09 & 0.84 & 11.19 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Lexicon Diversity}} \\\\\n",
      "    Vocabulary size & 69.79 & 0.86 & 2.92 & 0.80 & -0.76 & -1.81 \\\\\n",
      "    Thai words & 7.44 & \\cellcolor{gray!25} 6.17 & -0.83 & -0.59 & 0.90 & -0.92 \\\\\n",
      "    Long words & 3.93 & -0.11 & \\cellcolor{gray!25} -1.12 & -0.13 & 0.32 & 0.45 \\\\\n",
      "    Dictionary words & 78.97 & 1.13 & \\cellcolor{gray!25} -2.01 & -1.05 & 0.80 & 2.23 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Function Words}} \\\\\n",
      "    Pronoun & 4.80 & 0.64 & \\cellcolor{gray!25} 3.09 & 0.58 & -1.18 & -0.49 \\\\\n",
      "    >> 1st person pronoun & 2.90 & 0.51 & \\cellcolor{gray!25} 0.98 & 0.17 & -0.52 & -0.08 \\\\\n",
      "    >> 2nd person pronoun & 3.69 & 0.40 & 0.90 & 0.26 & -0.45 & -0.41 \\\\\n",
      "    >> 3rd person pronoun & 2.34 & 0.09 & \\cellcolor{gray!25} 1.23 & 0.20 & -0.53 & -0.92 \\\\\n",
      "    >> Pronoun in non-standard spelling & 1.79 & 0.03 & \\cellcolor{gray!25} 1.80 & 0.23 & -0.46 & -1.26 \\\\\n",
      "    Particles & 9.18 & \\cellcolor{gray!25} 4.03 & -2.35 & -0.64 & 0.52 & 0.25 \\\\\n",
      "    >> Socially-related particles & 5.66 & \\cellcolor{gray!25} 3.89 & -2.69 & -0.87 & 1.15 & 0.07 \\\\\n",
      "    >> Non-socially-related particles & 5.40 & -0.00 & -0.43 & 0.27 & -1.12 & 0.26 \\\\\n",
      "    >> Particle in non-standard spelling & 1.83 & 0.35 & 0.13 & 0.24 & -0.68 & -0.65 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Sentiment-related}} \\\\\n",
      "    Sentiment words & 6.56 & 1.13 & -0.06 & 0.08 & -0.14 & 0.03 \\\\\n",
      "    >> Positive words & 4.46 & 1.52 & -0.95 & -0.01 & -0.15 & -0.18 \\\\\n",
      "    >> Negative words & 2.95 & -0.23 & \\cellcolor{gray!25} 0.77 & 0.15 & -0.10 & -0.11 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \\multicolumn{7}{l}{\\textit{Internet Lexicons}} \\\\\n",
      "    Spelling variation & 23.70 & 0.13 & \\cellcolor{gray!25} 1.81 & 0.41 & -1.35 & -1.04 \\\\\n",
      "    >> Common misspelt words & 1.60 & 0.31 & 0.24 & 0.20 & -0.45 & -0.29 \\\\\n",
      "    >> Sematic variation & 14.21 & 1.16 & 1.14 & 0.39 & -1.03 & -1.96 \\\\\n",
      "    >> Simplified variation & 12.03 & -0.98 & \\cellcolor{gray!25} 1.36 & 0.14 & -0.31 & 0.84 \\\\\n",
      "    >> Repeated characters & 2.01 & -0.42 & \\cellcolor{gray!25} 0.83 & 0.23 & -0.02 & -0.75 \\\\\n",
      "    Abbreviation & 1.55 & 0.16 & 2.00 & -0.09 & -0.30 & -2.10 \\\\\n",
      "    Swear words & 2.06 & -0.19 & 0.08 & -0.09 & 1.08 & 0.07 \\\\\n",
      "    Transliteration & 1.43 & -0.31 & 0.17 & -0.07 & 0.58 & -0.10 \\\\\n",
      "    &  & &\\\\\n",
      "    \\hline\n",
      "    \n",
      "\\end{longtable}\n",
      "\n",
      "\\clearpage\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sections = [\n",
    "    \"Setting 2: Public Conversations with Labels from 3rd Party \",\n",
    "    \"Setting 3: Private Conversations with Labels from 3rd Party \",\n",
    "]\n",
    "\n",
    "table_contents = [\n",
    "    (printed_text1, printed_text2),\n",
    "    (printed_text3, printed_text4),\n",
    "]\n",
    "\n",
    "printed_text = \"\"\n",
    "for section, (t1, t2) in zip(sections, table_contents):\n",
    "    \n",
    "    printed_text += \"\\subsection{\"+section+\"}\"+\"\\n\\n\"\n",
    "    \n",
    "    printed_text += '''\\subsubsection{Closeness}\n",
    "\\\\begin{longtable}[h]{\n",
    "    p{\\dimexpr 0.40\\linewidth-2\\\\tabcolsep}|c|c|c|c|c|c|\n",
    "}\n",
    "    \\hline\n",
    "\n",
    "    Lexical Features & b & w1 & w2 & w3 & w4 & w5 \\\\\\\\\n",
    "    \\hline\n",
    "    \\endfirsthead\n",
    "\n",
    "    \\endhead\n",
    "    '''\n",
    "    \n",
    "    printed_text += (\"\\n\"+t1).replace(\"\\n\", \"\\n    \")\n",
    "    printed_text += \"\\n\"\n",
    "    printed_text += \"\\end{longtable}\"\n",
    "    printed_text += \"\\n\\n\"\n",
    "\n",
    "    printed_text += '''\\clearpage\n",
    "\\subsubsection{Respect}\n",
    "\\\\begin{longtable}[h]{\n",
    "    p{\\dimexpr 0.40\\linewidth-2\\\\tabcolsep}|c|c|c|c|c|c|\n",
    "}\n",
    "    \\hline\n",
    "\n",
    "    Lexical Features & b & w1 & w2 & w3 & w4 & w5 \\\\\\\\\n",
    "    \\hline\n",
    "    \\endfirsthead\n",
    "\n",
    "    \\endhead\n",
    "    '''\n",
    "    \n",
    "    printed_text += (\"\\n\"+t2).replace(\"\\n\", \"\\n    \")\n",
    "    printed_text += \"\\n\"\n",
    "    printed_text += \"\\end{longtable}\"\n",
    "    printed_text += \"\\n\\n\"\n",
    "    \n",
    "    printed_text += \"\\clearpage\"\n",
    "    printed_text += \"\\n\\n\"\n",
    "    # break\n",
    "    \n",
    "print(printed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d391969-05f1-43d4-ba90-8eac82bc0c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
